{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests the vLLM server running with Llama-3.1-8B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"httpx\",\n",
    "#   \"openai\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "client = OpenAI(base_url=f\"{BASE_URL}/v1\", api_key=\"token-abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models: ['Qwen/Qwen2.5-7B-Instruct']\n",
      "Using model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Check server health\n",
    "with httpx.Client() as http_client:\n",
    "    health_response = http_client.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "# Get available models using OpenAI client\n",
    "models = client.models.list()\n",
    "available_models = [model.id for model in models.data]\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "DEFAULT_MODEL = available_models[0] if available_models else \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(f\"Using model: {DEFAULT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 150,\n",
    "    model: str = DEFAULT_MODEL,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request using OpenAI client.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: Model to use for completion\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return response.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"Chat completion failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-504d8d517d4a467ca35e55cfdc6ee5c0\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello! I'm just a digital assistant, so I don't have feelings, but I'm here and ready to help you with any questions or information you need. How can I assist you today?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1754736770,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 41,\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"total_tokens\": 77,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "Hello! I'm just a digital assistant, so I don't have feelings, but I'm here and ready to help you with any questions or information you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "As of 2023, the population of Paris is approximately 2.2 million people. This includes the inhabitants of the city proper (Paris MÃ©tropole), but the population of the wider urban area (Grand Paris) is significantly larger, with an estimated population of around 12.3 million people.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning response:\n",
      "Let's break down the problem step-by-step:\n",
      "\n",
      "1. **Initial number of apples**: You start with 3 apples.\n",
      "2. **After giving away 1 apple**: You now have \\(3 - 1 = 2\\) apples.\n",
      "3. **After buying 2 more apples**: You add 2 apples to the 2 you currently have, which gives you \\(2 + 2 = 4\\) apples.\n",
      "\n",
      "So, after giving away 1 apple and then buying 2 more, you end up with a total of 4 apples.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages, max_tokens=200)\n",
    "if response:\n",
    "    print(\"Reasoning response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "In the heart of a bustling city, nestled among towering skyscrapers and vibrant street art, stood a small, unassuming workshop. This was the home of Zephyr, a robot with a curious mind and an even more curious soul. Unlike his mechanical brethren, Zephyr had no predefined purpose; he was simply a creation, a being born from the dreams of his inventor, Dr. Elara Myles.\n",
      "\n",
      "Dr. Myles, a brilliant but often misunderstood scientist, had always been\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "In the heart of a bustling metropolis, where neon lights flickered like stars and towering skyscrapers stretched towards the sky, there was a small, unassuming workshop tucked away in an alley. Inside this workshop, a robot named Pixel had been built with one singular purpose: to learn the art of painting.\n",
      "\n",
      "Pixel was unlike any other robot. It didn't have the cold, metallic eyes of its predecessors; instead, it had a digital screen that could display a wide range of emotions and expressions\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "In the heart of Silicon Valley, amidst towering skyscrapers and bustling streets, there stood a peculiar building, known as the Artistic Automaton Studio. Unlike any other structure in the city, it was home to a robot named Pixel, who had a passion for painting that no one else could quite comprehend.\n",
      "\n",
      "Pixel was an advanced model, with a sleek metallic body and a canvas attached to its arm. It was equipped with sophisticated sensors and artificial intelligence, but despite its capabilities, Pixel felt a void\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Different temperature settings\n",
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp, max_tokens=100)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calling response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-9f7d9351908f4a9f823fa4a0e30e0d1d\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-644b2a4224784a3b9da556000049bfad\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\": \\\"Paris, France\\\", \\\"unit\\\": \\\"celsius\\\"}\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1754736780,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 29,\n",
      "    \"prompt_tokens\": 220,\n",
      "    \"total_tokens\": 249,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "â Tool calling is supported!\n",
      "Tool called: get_weather\n",
      "Arguments: {\"location\": \"Paris, France\", \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Tool calling\n",
    "def test_tool_calling():\n",
    "    \"\"\"Test tool calling capabilities if supported by the model.\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get the weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The unit for temperature\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in Paris, France?\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            max_tokens=150,\n",
    "        )\n",
    "        \n",
    "        result = response.model_dump()\n",
    "        print(\"Tool calling response:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Check if tool was called\n",
    "        choice = result[\"choices\"][0]\n",
    "        if choice[\"message\"].get(\"tool_calls\"):\n",
    "            print(\"\\nâ Tool calling is supported!\")\n",
    "            for tool_call in choice[\"message\"][\"tool_calls\"]:\n",
    "                print(f\"Tool called: {tool_call['function']['name']}\")\n",
    "                print(f\"Arguments: {tool_call['function']['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\nâ Tool calling not supported or model chose not to use tools\")\n",
    "            print(\"Response:\", choice[\"message\"][\"content\"])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"â Tool calling test failed: {e}\")\n",
    "\n",
    "test_tool_calling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.93 seconds\n",
      "Tokens generated: 100\n",
      "Tokens per second: 51.79\n",
      "\n",
      "Response:\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics to process information. Hereâs a simple way to understand it:\n",
      "\n",
      "1. **Classical Computing vs. Quantum Computing**:\n",
      "   - In classical computers, data is processed using bits, which can be either 0 or 1.\n",
      "   - Quantum computers use quantum bits, or qubits, which can represent and store information as both 0 and 1 at the same time. This property is called superposition.\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages, max_tokens=100)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Tokens generated: {tokens_generated}\")\n",
    "    if tokens_generated > 0 and duration > 0:\n",
    "        print(f\"Tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Server health check** - Verify server is running and get available models\n",
    "2. **Basic functionality** - Simple chat completion\n",
    "3. **Multi-turn conversations** - Context awareness\n",
    "4. **Reasoning capabilities** - Complex problem solving\n",
    "5. **Temperature effects** - Creativity control\n",
    "6. **Tool calling** - Function calling capabilities (if supported)\n",
    "7. **Performance** - Response timing and token usage\n",
    "\n",
    "The notebook now uses:\n",
    "- **httpx** for HTTP requests\n",
    "- **OpenAI Python client** for chat completions\n",
    "- **Proper error handling** and response parsing\n",
    "- **Tool calling tests** to check function calling support\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
