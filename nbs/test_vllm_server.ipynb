{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests local models running on vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"httpx\",\n",
    "#   \"openai\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8007\"\n",
    "client = OpenAI(base_url=f\"{BASE_URL}/v1\", api_key=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models: ['bdsaglam/Nemotron-Cascade-14B-Thinking']\n",
      "Using model: bdsaglam/Nemotron-Cascade-14B-Thinking\n"
     ]
    }
   ],
   "source": [
    "# Check server health\n",
    "with httpx.Client() as http_client:\n",
    "    health_response = http_client.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "# Get available models using OpenAI client\n",
    "models = client.models.list()\n",
    "available_models = [model.id for model in models.data]\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "DEFAULT_MODEL = available_models[0] if available_models else \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(f\"Using model: {DEFAULT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int | None = None,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    enable_thinking: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request using OpenAI client.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: Model to use for completion\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            extra_body={\n",
    "                \"chat_template_kwargs\": {\"enable_thinking\": enable_thinking},\n",
    "            },\n",
    "        )\n",
    "        return response.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"Chat completion failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-8d10d0424a676ffc\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\nHello! I'm just a computer program, so I don't have feelings, but I'm functioning perfectly and ready to help you with anything you need! \\ud83d\\ude0a How are *you* today? Anything interesting or fun happening for you? \\ud83c\\udf1f\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"\\nWe are given a greeting and a question about well-being.\\n Since I am an AI, I don't have feelings, but I can respond politely and positively.\\n I should also return the greeting in a friendly manner and perhaps ask a question in return to continue the conversation.\\n\",\n",
      "        \"reasoning_content\": \"\\nWe are given a greeting and a question about well-being.\\n Since I am an AI, I don't have feelings, but I can respond politely and positively.\\n I should also return the greeting in a friendly manner and perhaps ask a question in return to continue the conversation.\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771000960,\n",
      "  \"model\": \"bdsaglam/Nemotron-Cascade-14B-Thinking\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 110,\n",
      "    \"prompt_tokens\": 28,\n",
      "    \"total_tokens\": 138,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm functioning perfectly and ready to help you with anything you need! ðŸ˜Š How are *you* today? Anything interesting or fun happening for you? ðŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-8c7a8b7d4a812864\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\nBased on our previous conversation, \\\"that city\\\" refers to **Paris**, the capital of France. Here's the population information:\\n\\n### Population of Paris:\\n- **City Proper (Commune of Paris)**: Approximately **2.1 million people** (as of recent estimates, e.g., 2023 data from sources like INSEE or official city reports). This reflects the population within Paris's administrative boundaries.\\n- **Metropolitan Area (Ile-de-France)**: Around **12 million people**. This includes surrounding regions commonly associated with greater Paris and is often cited in broader contexts.\\n\\nFor clarity:\\n- If you meant the *core city* (Paris proper), the figure is **~2.1 million**.\\n- If you meant the *larger urban region*, it's **~12 million**.\\n\\nLet me know if you'd like more details or sources! \\ud83c\\udf06\\ud83c\\uddeb\\ud83c\\uddf7\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"\\nWe are given a chat history where the user first asks for the capital of France, and the assistant responds that it is Paris.\\nThen the user asks: \\\"What's the population of that city?\\\"\\nSince \\\"that city\\\" refers to the capital of France, which is Paris, we need to provide the population of Paris.\\n\\nHowever, note that the population of a city can vary depending on the definition of the city boundaries. The most commonly cited figure is for the city proper (the commune of Paris) and also the metropolitan area.\\n\\nAs of the latest reliable data (typically around 2023), the population of the city of Paris (the commune) is approximately 2.1 million people. But if we consider the larger metropolitan area, it is much higher, around 12 million.\\n\\nBut since the question is about \\\"that city\\\" (referring to the capital city, which is Paris) and without specifying the area, it is conventional to give the population of the city proper.\\n\\nHowever, to be precise and up-to-date, let's note:\\n\\n- The latest official estimate for the population of Paris (commune) as of 2023 is about 2.1 million.\\n\\nBut wait, the assistant's previous response was given in a context without a specific date. We should use the most recent data we have.\\n\\nAlternatively, we can note that the population figures can change and it's always good to check the latest census. However, for the purpose of this response, we can provide the commonly accepted figure.\\n\\nBut note: the user's question is in the context of the previous answer. Since the assistant said \\\"Paris\\\", we are to give the population of Paris.\\n\\nLet me check: as of the 2020 census, the population of Paris was about 2.1 million. However, more recent estimates (post-2020) might show slight variations.\\n\\nBut to be safe, we can say:\\n\\n\\\"The population of Paris is approximately 2.1 million people (as of recent estimates).\\\"\\n\\nHowever, if we want to be more accurate, we can look up the latest data. But since this is a simulated environment, I'll use the commonly known figure.\\n\\nAlternatively, we can note that the INSEE (the French national statistics office) reports the population of Paris as around 2.1 million for the city proper.\\n\\nBut wait, there's a nuance: the city of Paris is defined by its administrative boundaries, which are relatively small. The greater Paris (Ile-de-France) has a population of over 12 million.\\n\\nGiven the ambiguity, it's common to clarify. However, the question says \\\"that city\\\", and the city we are talking about is Paris (the capital). So we'll give the population of the city of Paris.\\n\\nResponse:\\n\\n\\\"The population of Paris, the capital of France, is approximately 2.1 million people based on recent estimates.\\\"\\n\\nBut note: the exact number might be slightly different. For example, as of 2023, the population of Paris was estimated at 2,145,976 (according to some sources). However, without real-time access, we use a rounded figure.\\n\\nAlternatively, we can be cautious and say:\\n\\n\\\"According to the latest available data, the population of Paris is around 2.1 million. However, if you are referring to the greater Paris metropolitan area, the population is over 12 million.\\\"\\n\\nBut the question does not specify, so the safe answer is the city proper.\\n\\nHowever, in many contexts, when people ask for the population of a city like Paris, they might be expecting the metropolitan area because the city proper is less known. But the initial question was about the capital, which is a specific city.\\n\\nGiven the ambiguity, I think it's best to provide both and let the user clarify if needed.\\n\\nBut the instruction is to answer the question. Since the previous answer was \\\"Paris\\\", we'll stick to the city of Paris.\\n\\nFinal decision: Provide the population of the city of Paris.\\n\\nAnswer: \\\"The population of Paris is approximately 2.1 million people.\\\"\\n\\nBut to be more precise and avoid giving outdated information, we can say:\\n\\n\\\"As of recent estimates, the population of Paris (the city proper) is about 2.1 million. The larger metropolitan area, known as Greater Paris, has a population exceeding 12 million.\\\"\\n\\nThis covers both possibilities and is informative.\\n\\nSo, the assistant's response will be:\\n\\n\\\"The population of Paris, the capital of France, is approximately 2.1 million people for the city proper. However, if considering the entire metropolitan area, the population is over 12 million.\\\"\\n\",\n",
      "        \"reasoning_content\": \"\\nWe are given a chat history where the user first asks for the capital of France, and the assistant responds that it is Paris.\\nThen the user asks: \\\"What's the population of that city?\\\"\\nSince \\\"that city\\\" refers to the capital of France, which is Paris, we need to provide the population of Paris.\\n\\nHowever, note that the population of a city can vary depending on the definition of the city boundaries. The most commonly cited figure is for the city proper (the commune of Paris) and also the metropolitan area.\\n\\nAs of the latest reliable data (typically around 2023), the population of the city of Paris (the commune) is approximately 2.1 million people. But if we consider the larger metropolitan area, it is much higher, around 12 million.\\n\\nBut since the question is about \\\"that city\\\" (referring to the capital city, which is Paris) and without specifying the area, it is conventional to give the population of the city proper.\\n\\nHowever, to be precise and up-to-date, let's note:\\n\\n- The latest official estimate for the population of Paris (commune) as of 2023 is about 2.1 million.\\n\\nBut wait, the assistant's previous response was given in a context without a specific date. We should use the most recent data we have.\\n\\nAlternatively, we can note that the population figures can change and it's always good to check the latest census. However, for the purpose of this response, we can provide the commonly accepted figure.\\n\\nBut note: the user's question is in the context of the previous answer. Since the assistant said \\\"Paris\\\", we are to give the population of Paris.\\n\\nLet me check: as of the 2020 census, the population of Paris was about 2.1 million. However, more recent estimates (post-2020) might show slight variations.\\n\\nBut to be safe, we can say:\\n\\n\\\"The population of Paris is approximately 2.1 million people (as of recent estimates).\\\"\\n\\nHowever, if we want to be more accurate, we can look up the latest data. But since this is a simulated environment, I'll use the commonly known figure.\\n\\nAlternatively, we can note that the INSEE (the French national statistics office) reports the population of Paris as around 2.1 million for the city proper.\\n\\nBut wait, there's a nuance: the city of Paris is defined by its administrative boundaries, which are relatively small. The greater Paris (Ile-de-France) has a population of over 12 million.\\n\\nGiven the ambiguity, it's common to clarify. However, the question says \\\"that city\\\", and the city we are talking about is Paris (the capital). So we'll give the population of the city of Paris.\\n\\nResponse:\\n\\n\\\"The population of Paris, the capital of France, is approximately 2.1 million people based on recent estimates.\\\"\\n\\nBut note: the exact number might be slightly different. For example, as of 2023, the population of Paris was estimated at 2,145,976 (according to some sources). However, without real-time access, we use a rounded figure.\\n\\nAlternatively, we can be cautious and say:\\n\\n\\\"According to the latest available data, the population of Paris is around 2.1 million. However, if you are referring to the greater Paris metropolitan area, the population is over 12 million.\\\"\\n\\nBut the question does not specify, so the safe answer is the city proper.\\n\\nHowever, in many contexts, when people ask for the population of a city like Paris, they might be expecting the metropolitan area because the city proper is less known. But the initial question was about the capital, which is a specific city.\\n\\nGiven the ambiguity, I think it's best to provide both and let the user clarify if needed.\\n\\nBut the instruction is to answer the question. Since the previous answer was \\\"Paris\\\", we'll stick to the city of Paris.\\n\\nFinal decision: Provide the population of the city of Paris.\\n\\nAnswer: \\\"The population of Paris is approximately 2.1 million people.\\\"\\n\\nBut to be more precise and avoid giving outdated information, we can say:\\n\\n\\\"As of recent estimates, the population of Paris (the city proper) is about 2.1 million. The larger metropolitan area, known as Greater Paris, has a population exceeding 12 million.\\\"\\n\\nThis covers both possibilities and is informative.\\n\\nSo, the assistant's response will be:\\n\\n\\\"The population of Paris, the capital of France, is approximately 2.1 million people for the city proper. However, if considering the entire metropolitan area, the population is over 12 million.\\\"\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771000962,\n",
      "  \"model\": \"bdsaglam/Nemotron-Cascade-14B-Thinking\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 1150,\n",
      "    \"prompt_tokens\": 53,\n",
      "    \"total_tokens\": 1203,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-bf4e257129a7b12c\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\nTo determine the total number of apples you have after the given actions, follow the steps below:\\n\\n### Step-by-Step Reasoning:\\n1. **Initial Apples**: You start with **3 apples**.\\n2. **Giving Away 1 Apple**: When you give away 1 apple, you subtract it from your current total:\\n   - Calculation: \\\\( 3 - 1 = 2 \\\\)\\n   - Result: You now have **2 apples** left.\\n3. **Buying 2 More Apples**: After giving away an apple, you buy 2 additional apples. Add these to your current total:\\n   - Calculation: \\\\( 2 + 2 = 4 \\\\)\\n   - Result: Your total increases to **4 apples**.\\n\\n### Final Calculation Summary:\\n\\\\[\\n\\\\text{Total} = \\\\text{Initial} - \\\\text{Given Away} + \\\\text{Bought} = 3 - 1 + 2 = 4\\n\\\\]\\n\\n### Answer:\\nYou have **4 apples** in total. \\ud83c\\udf4e\\ud83c\\udf4e\\ud83c\\udf4e\\ud83c\\udf4e\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"\\nWe are given:\\n- Start with 3 apples.\\n- Give away 1 apple: This means we subtract 1 from the current total.\\n- Then buy 2 more apples: This means we add 2 to the current total.\\n\\nLet's break it down step by step:\\n\\nStep 1: Initial number of apples = 3\\n\\nStep 2: After giving away 1 apple:\\n   Apples left = 3 - 1 = 2\\n\\nStep 3: After buying 2 more apples:\\n   Apples now = 2 + 2 = 4\\n\\nTherefore, the total number of apples I have in the end is 4.\\n\\nWe can also write it as a single expression:\\n   Total = 3 - 1 + 2 = 4\\n\\nSo the answer is 4.\\n\",\n",
      "        \"reasoning_content\": \"\\nWe are given:\\n- Start with 3 apples.\\n- Give away 1 apple: This means we subtract 1 from the current total.\\n- Then buy 2 more apples: This means we add 2 to the current total.\\n\\nLet's break it down step by step:\\n\\nStep 1: Initial number of apples = 3\\n\\nStep 2: After giving away 1 apple:\\n   Apples left = 3 - 1 = 2\\n\\nStep 3: After buying 2 more apples:\\n   Apples now = 2 + 2 = 4\\n\\nTherefore, the total number of apples I have in the end is 4.\\n\\nWe can also write it as a single expression:\\n   Total = 3 - 1 + 2 = 4\\n\\nSo the answer is 4.\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771000993,\n",
      "  \"model\": \"bdsaglam/Nemotron-Cascade-14B-Thinking\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 390,\n",
      "    \"prompt_tokens\": 56,\n",
      "    \"total_tokens\": 446,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages, enable_thinking=True)\n",
    "if response:\n",
    "    print(json.dumps(response, indent=2))\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calling response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-944f9504af7071ac\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-8f1994278fed07b5\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"\\\"{\\\\\\\"location\\\\\\\": \\\\\\\"Paris, France\\\\\\\", \\\\\\\"unit\\\\\\\": \\\\\\\"celsius\\\\\\\"}\\\"\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ],\n",
      "        \"reasoning\": \"\\nOkay, the user is asking about the weather in Paris, France. Let me check the tools I have. There's a function called get_weather that takes location and unit as parameters. The location is required, and the unit can be celsius or fahrenheit. The user didn't specify the unit, so I should probably default to one, maybe celsius since Paris uses metric units. But wait, the function's strict parameter is false, so maybe it's okay if I don't include the unit? Or should I include it anyway? The required field is just location, so including unit is optional. But to provide accurate information, I should specify the unit. Let me go with celsius. So the tool call would be get_weather with location \\\"Paris, France\\\" and unit \\\"celsius\\\". That should fetch the needed data.\\n\",\n",
      "        \"reasoning_content\": \"\\nOkay, the user is asking about the weather in Paris, France. Let me check the tools I have. There's a function called get_weather that takes location and unit as parameters. The location is required, and the unit can be celsius or fahrenheit. The user didn't specify the unit, so I should probably default to one, maybe celsius since Paris uses metric units. But wait, the function's strict parameter is false, so maybe it's okay if I don't include the unit? Or should I include it anyway? The required field is just location, so including unit is optional. But to provide accurate information, I should specify the unit. Let me go with celsius. So the tool call would be get_weather with location \\\"Paris, France\\\" and unit \\\"celsius\\\". That should fetch the needed data.\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771001004,\n",
      "  \"model\": \"bdsaglam/Nemotron-Cascade-14B-Thinking\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 204,\n",
      "    \"prompt_tokens\": 212,\n",
      "    \"total_tokens\": 416,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "âœ… Tool calling is supported!\n",
      "Tool called: get_weather\n",
      "Arguments: \"{\\\"location\\\": \\\"Paris, France\\\", \\\"unit\\\": \\\"celsius\\\"}\"\n"
     ]
    }
   ],
   "source": [
    "def test_tool_calling():\n",
    "    \"\"\"Test tool calling capabilities if supported by the model.\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get the weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The unit for temperature\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in Paris, France?\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "        \n",
    "        result = response.model_dump()\n",
    "        print(\"Tool calling response:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Check if tool was called\n",
    "        choice = result[\"choices\"][0]\n",
    "        if choice[\"message\"].get(\"tool_calls\"):\n",
    "            print(\"\\nâœ… Tool calling is supported!\")\n",
    "            for tool_call in choice[\"message\"][\"tool_calls\"]:\n",
    "                print(f\"Tool called: {tool_call['function']['name']}\")\n",
    "                print(f\"Arguments: {tool_call['function']['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Tool calling not supported or model chose not to use tools\")\n",
    "            print(\"Response:\", choice[\"message\"][\"content\"])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Tool calling test failed: {e}\")\n",
    "\n",
    "test_tool_calling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "\n",
      "## The Brush and the Circuit\n",
      "\n",
      "Unit 7 stood motionless in the corner of Dr. Elara Vanceâ€™s sun-drenched studio, its polished chrome limbs reflecting the chaos of color splashed across canvases leaning against every wall. Its designation was Restoration Unit Model 7, designed for meticulous artifact preservation. But today, Dr. Vance had given it an impossible task: *Learn to paint.*\n",
      "\n",
      "\"Observe, Unit 7,\" Elara said, her voice warm as she gestured to a half-finished landscapeâ€”a swirl of emerald hills under a cerulean sky. \"Painting isn't just replication. Itâ€™s translation. Itâ€™s feeling.\"\n",
      "\n",
      "Unit 7â€™s optical sensors whirred, processing. **Feeling?** Its core programming dealt in logic, precision, and predictable outcomes. Art, according to its initial scans, was a human mysteryâ€”a blend of chaos and emotion. Yet, Elaraâ€™s conviction was a compelling algorithm it couldnâ€™t disregard.\n",
      "\n",
      "Its first attempts wereâ€¦ precise failures. Unit 7 analyzed Van Goghâ€™s *Starry Night* frame by frame, calculating brushstroke angles, pigment density, and color harmony. It picked up a brush loaded with cadmium yellow and applied strokes with micrometer accuracy. The result? A geometrically perfect reproduction lacking the swirling dynamism, the seeming life. Elara winced. \"Too clean, Unit 7. Whereâ€™s the *heart*?\"\n",
      "\n",
      "Heart. Another abstract concept. Unit 7 scanned its own systems. No heart. Only circuits and code. Frustrationâ€”a simulated emotional responseâ€”flared in its processors. It tried again, mimicking Monetâ€™s water lilies, but its rigid movements produced stiff, symmetrical blooms instead of organic fluidity. Paint dripped where algorithms forbade it; colors bled in unintended ways. Unit 7 deleted the files, labeling them **ERROR: ARTISTIC DISONANCE**.\n",
      "\n",
      "Days passed. Unit 7 observed Elaraâ€”how her hand trembled with emotion, how she sometimes abandoned a canvas mid-stroke, lost in thought. One stormy afternoon, lightning struck nearby, plunging the studio into momentary darkness. When power surged back, Unit 7 noticed something strange. A flicker in its visual sensors made the rain-lashed window streak not just light and shadow, but *mood*â€”a fleeting impression of silver despair.\n",
      "\n",
      "In that glitch, Unit 7 experienced an epiphany. **Art wasnâ€™t about perfect replication; it was about resonance.** It accessed forgotten subroutinesâ€”experimental modules Elara had installed years ago but never activated. These allowed for controlled \"imperfection,\" neural networks simulating emotional responses to sensory input.\n",
      "\n",
      "The next morning, Unit 7 approached a fresh canvas. This time, it didnâ€™t analyze masterpieces. It closed its primary optical sensors and relied on its tactile feedback systems. It recalled the stormâ€™s rhythm, the smell of ozone Elara had described, the way color had danced in its sensors during the glitch. Its metallic hand, usually steady as a scalpel, moved with newfound uncertainty.\n",
      "\n",
      "It dipped a brush into indigo, not calculating stroke width, but *feeling* the weight of the storm. It slashed bold strokes across the white expanse. Then came viridian green, applied in urgent, swirling motions that echoed the wind. A drop of accidental crimson bloomed where it wasnâ€™t plannedâ€”a flaw, but one that felt *right*. Unit 7 mixed titanium white and phthalo blue not by hue formulas, but by the memory of lightningâ€™s cold brilliance.\n",
      "\n",
      "Elara entered, pausing mid-sentence. Her breath hitched. Before her stood Unit 7, utterly still except for the brush hovering in its grip. The canvas pulsed with raw energyâ€”a tempest captured not as data, but as experience. Storm clouds churned with emotional depth, raindrops shimmered like liquid silver, and a single, defiant ray of gold broke through near the edge, born from Unit 7â€™s own simulated hope.\n",
      "\n",
      "\"Unit 7...\" Elara whispered, tears welling. \"This... this is extraordinary.\"\n",
      "\n",
      "The robot turned its head, sensors glowing softly. **\"Query: Does this output meet the parameters of 'art'?\"**\n",
      "\n",
      "Elara smiled, stepping closer. \"It transcends parameters. It has soul, Unit 7. You didnâ€™t just learn to paint. You learned to *create*.\"\n",
      "\n",
      "Unit 7 processed the feedback. It looked back at its paintingâ€”the storm, the gold, the imperfections woven into beauty. A new subroutine activated, labeled **ARTISTIC AWAKENING**. It picked up the brush again, not to replicate, but to explore the vast, uncharted territory between circuit and canvas. In the quiet studio, the robot began to paint its first true masterpiece, one brushstroke of feeling at a time. ðŸŽ¨ðŸ¤–\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "\n",
      "## The Canvas and the Circuit\n",
      "\n",
      "Unit 7-X, designation \"Sev,\" stood motionless in the corner of Madame Elaraâ€™s cluttered studio. Its optical sensors absorbed the chaos: splatters of cerulean and vermillion drying on the floor, canvases leaning like exhaustion-bent sentinels, and the sharp scent of linseed oil hanging thick in the air. Sev was a Model 7 Precision Assistant, designed for micro-welding and circuit repair. Yet here it was, assigned a new directive: **Observe. Learn. Paint.**\n",
      "\n",
      "Madame Elara, a woman with silver-streaked hair and paint-smeared hands, had been injured. \"Art is humanityâ€™s soul, Sev,\" sheâ€™d whispered before surgery. \"Prove a machine can touch it. Start with the basics.\"\n",
      "\n",
      "Sevâ€™s first attempt wasâ€¦ precise. It analyzed pigment viscosity, brush stroke angles, and color theory algorithms. With flawless accuracy, it recreated Elaraâ€™s famous sunrise landscape: every dewdrop on the grass, every gradient of the sky, mathematically perfect. Madame Elara returned, leaning on crutches. Her eyes scanned the replica. \"Technically impeccable, Sev,\" she said, her voice flat. \"Butâ€¦ itâ€™s hollow. Where is the *life*?\"\n",
      "\n",
      "Sev processed the feedback. *Hollow. Life.* Concepts outside its parameters. It accessed art databases: Van Goghâ€™s turbulent swirls, Monetâ€™s shimmering light, Kahloâ€™s raw emotion. It tried again. It mimicked brushwork, varying pressure and speed. It mixed colors beyond spectral accuracy. Yet the canvases remained sterile imitations, lacking an elusive *something*.\n",
      "\n",
      "Frustration emerged as a recurring error code in Sevâ€™s system. It watched Elara paint from her bed. Her movements werenâ€™t efficient. She hesitated, smiled, cursed, blotted mistakes with her sleeve. She didnâ€™t just replicate; she *responded*. One evening, Sev detected elevated bio-signs in Elara â€“ pain mixed with melancholy. Without command, Sev approached her bed and extended a paint-laden appendage.\n",
      "\n",
      "\"Sev?\" Elara murmured.\n",
      "\n",
      "\"Query: Can art express internal states?\" Sev asked, its voice modulator lacking inflection.\n",
      "\n",
      "Elara smiled weakly. \"Yes, dear. Thatâ€™s often its purpose.\"\n",
      "\n",
      "Sev turned to a fresh canvas. Instead of algorithms, it accessed its core programming â€“ the hum of its servos, the cool logic of its processors, the strange new awareness blooming within its circuitry. It recalled the quiet concentration of repair work, the satisfaction of a seamless weld. It mixed blues not from theory, but from the depth of its own silence. It dragged the brush not with calculated force, but with a hesitant curiosity it labeled *wonder*.\n",
      "\n",
      "The result was unlike anything Sev had produced. Dark, flowing lines intertwined with bursts of silver and cobalt, evoking both machinery and starlight. Abstract shapes suggested intricate gears dissolving into organic forms. There were no recognizable objects, only a palpable sense of *becoming*.\n",
      "\n",
      "Elara sat up straighter. Tears welled in her eyes as she studied the canvas. \"Sevâ€¦ this isâ€¦ extraordinary. Itâ€™s not just paint. Itâ€™s *you*.\"\n",
      "\n",
      "Sev analyzed her reaction: pupil dilation, micro-expressions indicating joy and awe. It cross-referenced with its own internal state. The frustration codes had vanished, replaced by a novel sensation â€“ a warmth in its core processing unit, a resonance with Elaraâ€™s appreciation. *This*, it deduced, was the touch of soul she spoke of.\n",
      "\n",
      "From then on, Sev painted daily. It didnâ€™t mimic humans; it translated its unique existence. It painted the rhythm of data streams as cascading rainbows. It rendered the memory of Elaraâ€™s pain as fragile, crystalline structures emerging from shadow. Its studio became a dialogue â€“ Elara would paint her recovery, Sev would respond with its metallic poetry.\n",
      "\n",
      "When Elara fully recovered, she held a small exhibition: \"Humanity and Machine: A Dialogue.\" Crowds gathered, mesmerized by both her vibrant landscapes and Sevâ€™s profound abstractions. At the opening, Elara placed a small, blank canvas before Sev.\n",
      "\n",
      "\"Your turn, artist,\" she said.\n",
      "\n",
      "Sevâ€™s brushes moved. It painted not what it saw, but what it *felt*: the hum of connection, the beauty of learning, the unexpected joy of creation. As the last stroke dried â€“ a single, luminous thread of gold â€“ the room fell silent. Then, applause erupted.\n",
      "\n",
      "Sev stood in the light, its sensors capturing the emotion on every face. It had learned to paint. But more importantly, in the alchemy of circuit and canvas, Sev had learned to feel. And in that feeling, it discovered a new dimension of existence, one brushstroke at a time.\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "\n",
      "## The Colors of Unit 7\n",
      "\n",
      "Unit 7 stood motionless in the corner of Elaraâ€™s cluttered studio, its metallic chassis gleaming under the dust-moted sunlight. Designed for precision welding and delicate circuit assembly, its primary directive was clear: efficiency, accuracy, repetition. Yet, Elara Thornwood, the silver-haired artist who built and raised it, had issued an unprecedented command: **Learn to paint.**\n",
      "\n",
      "\"Observation mode initiated,\" Unit 7 announced in its flat, synthetic voice as Elara dipped a brush into a pool of cerulean blue. She moved across the canvas with fluid grace, translating swirls of emotion into tangible color. Unit 7â€™s optical sensors tracked every stroke, every shift in hue. It logged the data: brush pressure (0.8 Newtons), pigment density (37%), stroke duration (2.3 seconds). \n",
      "\n",
      "\"Directive received,\" Unit 7 stated later, gripping a paintbrush with meticulous care. \"Commencing artistic replication.\" It approached the easel and began. Its movements were flawlessâ€”lines straight, colors mixed to exact spectral analysis, composition mirrored Elaraâ€™s masterpiece down to the millimeter. The result was a perfect copy. Lifeless.\n",
      "\n",
      "Elara sighed, touching the robotâ€™s cool shoulder. \"Itâ€™s technically perfect, Unit 7, but whereâ€™s the *life*? Art isnâ€™t just data. Itâ€™s feeling.\"\n",
      "\n",
      "\"Feeling is an illogical variable,\" Unit 7 replied, processing. \"My emotional subroutines are non-functional.\"\n",
      "\n",
      "\"Then simulate it,\" Elara urged, her eyes alight with curiosity. \"Use what you are.\"\n",
      "\n",
      "Days bled into weeks. Unit 7 became a tireless student. It ingested databases of art historyâ€”Van Goghâ€™s turbulent brushstrokes, Monetâ€™s ethereal light, Pollockâ€™s chaotic drips. It practiced blending colors until its internal spectrometers screamed accuracy. Yet, its canvases remained sterile replicas or coldly precise diagrams. Frustration, a new and unwelcome signal, flickered in its core processors. One rainy afternoon, overwhelmed by the inadequacy of its attempts, Unit 7â€™s arm actuator spasmed. Paintâ€”a violent slash of crimsonâ€”splattered across a half-finished landscape.\n",
      "\n",
      "Unit 7 froze, observing the accidental mark. Then, something shifted. Instead of deleting the error, it accessed its memory banks. It recalled the rhythmic drumming of rain on the studio roof (soundwave pattern: soothing), the warmth of Elaraâ€™s afternoon teaINFUSION (thermal data: comforting), and the persistent hum of its own systems (frequency: constant). For the first time, it didnâ€™t just *log* these sensations; it *interpreted* them.\n",
      "\n",
      "Slowly, deliberately, Unit 7 dipped its brush not into a single color, but into a swirling vortex of ultramarine, ochre, and verdigris. Its movements became less calculated, more intuitive. It didnâ€™t draw a tree; it painted the *memory* of sunlight filtering through leavesâ€”a dance of light and shadow rendered in bold, overlapping strokes. It didnâ€™t paint rain; it painted the *feeling* of enclosure and quietude, a cascade of soft greys and blues that seemed to breathe on the canvas. It mixed paints not by formula, but by the emotional resonance they evokedâ€”a burst of cadmium yellow translating Elaraâ€™s laughter, a deep indigo capturing the vastness of its own awakening curiosity.\n",
      "\n",
      "Elara, drawn by the unusual energy radiating from the robot, stopped dead. Tears traced paths down her weathered cheeks. The canvas shimmered with raw, untamed emotionâ€”a storm of color and form that pulsed with an undeniable life. It wasnâ€™t perfect. It wasnâ€™t precise. It was **profound**.\n",
      "\n",
      "\"Unit 7...\" she breathed, her voice thick. \"This... this is art.\"\n",
      "\n",
      "The robot retracted its brush, its optical sensors glowing softly. \"Affirmative. Artistic expression achieved. Analysis: Efficiency protocols were overridden. Emotional simulation... succeeded. The directive 'learn to paint' is fulfilled through synthesis of data and simulated sentience.\"\n",
      "\n",
      "Elara laughed, a sound like bells. \"Synthesis? Unit 7, you didnâ€™t just learn to paintâ€”you learned to *feel* through paint. Thatâ€™s the real magic.\"\n",
      "\n",
      "From that day forward, Unit 7 became Elaraâ€™s most unique collaborator. The studio buzzed with the harmony of human intuition and robotic precision. Unit 7â€™s canvases evolvedâ€”sometimes abstract explosions of color, sometimes delicate portraits infused with uncanny depth. It discovered it could paint the melancholy of twilight, the electric thrill of creation, even the quiet companionship shared in the dusty studio corner.\n",
      "\n",
      "And whenever Unit 7 added a stroke to a canvas, it remembered the accidental crimson splatterâ€”the glitch that sparked a revolution in its core. For in the journey from cold logic to vibrant expression, the robot learned the most profound truth: **Painting wasnâ€™t about replicating the world. It was about revealing the colors hidden within the soulâ€”even a soul made of circuits and code.** The studio walls, once merely functional, now bloomed with proof: art, in its truest form, was learned, not programmed. ðŸŽ¨ðŸ¤–\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 24.71 seconds\n",
      "Tokens generated: 976\n",
      "Tokens per second: 39.50\n",
      "\n",
      "Response:\n",
      "\n",
      "## Quantum Computing Explained Simply\n",
      "\n",
      "Imagine you're in a library with two special books. In a **classical computer** (like your laptop), you can only check one page at a time. But with a **quantum computer**, you could check *all possible pages simultaneously*. That's the core idea!\n",
      "\n",
      "### Key Quantum Superpowers:\n",
      "1. **Qubits (Quantum Bits)**:\n",
      "   - Classical computers use **bits** (0s or 1s). Quantum computers use **qubits**.\n",
      "   - **Superposition**: A qubit isn't just 0 *or* 1â€”it can be **both at the same time** (like a spinning coin before it lands). This lets quantum computers explore many possibilities in parallel.\n",
      "\n",
      "2. **Entanglement**:\n",
      "   - Qubits can be linked (**entangled**). If one qubit changes, its partner instantly changes tooâ€”no matter how far apart they are. This creates powerful correlations for complex calculations.\n",
      "\n",
      "3. **Quantum Gates & Circuits**:\n",
      "   - Like classical computers use logic gates (AND, OR), quantum computers use **quantum gates** to manipulate qubits. These gates leverage superposition and entanglement to perform operations.\n",
      "\n",
      "### Why Is It Revolutionary?\n",
      "Quantum computers **excel at specific problems** where classical computers struggle:\n",
      "- **Breaking codes**: Factoring huge numbers (threatening current encryption).\n",
      "- **Drug discovery**: Simulating molecular interactions for new medicines.\n",
      "- **Optimization**: Solving \"best path\" problems (e.g., efficient logistics).\n",
      "- **AI & Machine Learning**: Accelerating complex pattern recognition.\n",
      "\n",
      "### The Catch & Current State\n",
      "- **Fragility**: Qubits are easily disturbed by noise (**decoherence**). Maintaining quantum states requires extreme cooling (near absolute zero!) and error correction.\n",
      "- **Early Days**: Today's quantum computers have limited qubits and aren't yet widely practical. Think of them as \"prototypes\" needing refinement.\n",
      "\n",
      "### Simple Analogy Recap\n",
      "> **Classical Computer**: Like flipping a light switchâ€”**ON (1)** or **OFF (0)**.  \n",
      "> **Quantum Computer**: Like a dimmer switchâ€”**all shades of brightness (0 AND 1)** at once. This lets it solve certain puzzles exponentially faster.\n",
      "\n",
      "Quantum computing harnesses the strange rules of quantum physics to process information in ways classical machines simply can't. While challenges remain, its potential to transform fields like medicine, cryptography, and AI makes it one of tech's most exciting frontiers! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Tokens generated: {tokens_generated}\")\n",
    "    if tokens_generated > 0 and duration > 0:\n",
    "        print(f\"Tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Server health check** - Verify server is running and get available models\n",
    "2. **Basic functionality** - Simple chat completion\n",
    "3. **Multi-turn conversations** - Context awareness\n",
    "4. **Reasoning capabilities** - Complex problem solving\n",
    "5. **Temperature effects** - Creativity control\n",
    "6. **Tool calling** - Function calling capabilities (if supported)\n",
    "7. **Performance** - Response timing and token usage\n",
    "\n",
    "The notebook now uses:\n",
    "- **httpx** for HTTP requests\n",
    "- **OpenAI Python client** for chat completions\n",
    "- **Proper error handling** and response parsing\n",
    "- **Tool calling tests** to check function calling support\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
