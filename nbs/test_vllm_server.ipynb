{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests local models running on vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"httpx\",\n",
    "#   \"openai\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "client = OpenAI(base_url=f\"{BASE_URL}/v1\", api_key=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models: ['nvidia/Nemotron-Cascade-8B', 'r32-a32.0']\n",
      "Using model: nvidia/Nemotron-Cascade-8B\n"
     ]
    }
   ],
   "source": [
    "# Check server health\n",
    "with httpx.Client() as http_client:\n",
    "    health_response = http_client.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "# Get available models using OpenAI client\n",
    "models = client.models.list()\n",
    "available_models = [model.id for model in models.data]\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "DEFAULT_MODEL = available_models[0] if available_models else \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(f\"Using model: {DEFAULT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int | None = None,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    enable_thinking: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request using OpenAI client.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: Model to use for completion\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            extra_body={\n",
    "                \"chat_template_kwargs\": {\"enable_thinking\": enable_thinking},\n",
    "            },\n",
    "        )\n",
    "        return response.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"Chat completion failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-8e85a23720410211\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with anything you need! How are *you* doing today? \\ud83d\\ude0a\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": null,\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771244294,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 41,\n",
      "    \"prompt_tokens\": 32,\n",
      "    \"total_tokens\": 73,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with anything you need! How are *you* doing today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-a0ac81c67b48d4fc\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The population of Paris, the capital of France, is approximately **2.1 million** people within the city limits. However, the metropolitan area (\\u00cele-de-France region) has a much larger population, estimated at around **11.2 million** as of recent data. \\n\\nWould you like more specific details, such as historical trends or demographic breakdowns?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": null,\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771244650,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 76,\n",
      "    \"prompt_tokens\": 61,\n",
      "    \"total_tokens\": 137,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-868071f31032a864\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\nTo determine how many apples you have in total, let's break down the problem step by step with clear reasoning:\\n\\n1. **Initial amount**: You start with **3 apples**. This is your baseline quantity.\\n\\n2. **Giving away 1 apple**: When you give away an apple, you are reducing your count. So, subtract 1 apple from the initial amount:\\n   \\\\[\\n   3 \\\\text{ apples} - 1 \\\\text{ apple} = 2 \\\\text{ apples}\\n   \\\\]\\n   After this action, you have **2 apples** left.\\n\\n3. **Buying 2 more apples**: Purchasing additional apples increases your count. Add 2 apples to the current amount:\\n   \\\\[\\n   2 \\\\text{ apples} + 2 \\\\text{ apples} = 4 \\\\text{ apples}\\n   \\\\]\\n   This brings your total to **4 apples**.\\n\\n**Final Total**: Combining all these steps, you end up with **4 apples** in total.\\n\\n### Summary of Reasoning:\\n- The operations follow a sequence: **start \\u2192 give away (subtract) \\u2192 buy (add)**.\\n- Mathematically, it can be expressed as: \\\\( 3 - 1 + 2 = 4 \\\\).\\n- No hidden conditions or tricks are present; it's a straightforward arithmetic problem based on the actions described. \\ud83c\\udf4e\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"\\nFirst, the user has 3 apples. That's the starting point.\\n\\nThen, they give away 1 apple. Giving away means losing that apple, so I need to subtract 1 from the current amount. Starting with 3, after giving away 1, I have 3 - 1 = 2 apples left.\\n\\nNext, they buy 2 more apples. Buying apples means gaining them, so I should add 2 to the current count. After giving away, I had 2 apples, so now I have 2 + 2 = 4 apples.\\n\\nTherefore, in total, I have 4 apples.\\n\\nLet me double-check the sequence:\\n\\n- Initial: 3 apples\\n\\n- After giving away 1: 3 - 1 = 2 apples\\n\\n- After buying 2 more: 2 + 2 = 4 apples\\n\\nYes, that seems correct. I should make sure there's no trick here. The problem is straightforward: start with 3, minus 1, plus 2. Mathematically, it's 3 - 1 + 2 = 4.\\n\\nThe user asked to explain the reasoning, so I'll include that in my response.\\n\",\n",
      "        \"reasoning_content\": \"\\nFirst, the user has 3 apples. That's the starting point.\\n\\nThen, they give away 1 apple. Giving away means losing that apple, so I need to subtract 1 from the current amount. Starting with 3, after giving away 1, I have 3 - 1 = 2 apples left.\\n\\nNext, they buy 2 more apples. Buying apples means gaining them, so I should add 2 to the current count. After giving away, I had 2 apples, so now I have 2 + 2 = 4 apples.\\n\\nTherefore, in total, I have 4 apples.\\n\\nLet me double-check the sequence:\\n\\n- Initial: 3 apples\\n\\n- After giving away 1: 3 - 1 = 2 apples\\n\\n- After buying 2 more: 2 + 2 = 4 apples\\n\\nYes, that seems correct. I should make sure there's no trick here. The problem is straightforward: start with 3, minus 1, plus 2. Mathematically, it's 3 - 1 + 2 = 4.\\n\\nThe user asked to explain the reasoning, so I'll include that in my response.\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771244856,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 533,\n",
      "    \"prompt_tokens\": 58,\n",
      "    \"total_tokens\": 591,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages, enable_thinking=True)\n",
    "if response:\n",
    "    print(json.dumps(response, indent=2))\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calling response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-91094bd7ff124fd8\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-b204cf31e37d8fd8\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"\\\"{\\\\\\\"location\\\\\\\": \\\\\\\"Paris, France\\\\\\\"}\\\"\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ],\n",
      "        \"reasoning\": \"\\nOkay, the user is asking about the weather in Paris, France. Let me check the tools available. There's a get_weather function that requires the location. The parameters include location (string) and unit (optional, either celsius or fahrenheit). The user didn't specify the unit, so I should probably default to one. Since the function isn't strict, maybe I can proceed with just the location. I'll call get_weather with location set to \\\"Paris, France\\\". For the unit, maybe use celsius as it's commonly used in Europe, but since it's optional, perhaps omitting it allows the function to handle it. Wait, the required parameter is only location, so unit can be skipped. The response might then use a default unit. Alright, I'll generate the tool call with just the location.\\n\",\n",
      "        \"reasoning_content\": \"\\nOkay, the user is asking about the weather in Paris, France. Let me check the tools available. There's a get_weather function that requires the location. The parameters include location (string) and unit (optional, either celsius or fahrenheit). The user didn't specify the unit, so I should probably default to one. Since the function isn't strict, maybe I can proceed with just the location. I'll call get_weather with location set to \\\"Paris, France\\\". For the unit, maybe use celsius as it's commonly used in Europe, but since it's optional, perhaps omitting it allows the function to handle it. Wait, the required parameter is only location, so unit can be skipped. The response might then use a default unit. Alright, I'll generate the tool call with just the location.\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1771244862,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 195,\n",
      "    \"prompt_tokens\": 214,\n",
      "    \"total_tokens\": 409,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "âœ… Tool calling is supported!\n",
      "Tool called: get_weather\n",
      "Arguments: \"{\\\"location\\\": \\\"Paris, France\\\"}\"\n"
     ]
    }
   ],
   "source": [
    "def test_tool_calling():\n",
    "    \"\"\"Test tool calling capabilities if supported by the model.\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get the weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The unit for temperature\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in Paris, France?\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "        \n",
    "        result = response.model_dump()\n",
    "        print(\"Tool calling response:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Check if tool was called\n",
    "        choice = result[\"choices\"][0]\n",
    "        if choice[\"message\"].get(\"tool_calls\"):\n",
    "            print(\"\\nâœ… Tool calling is supported!\")\n",
    "            for tool_call in choice[\"message\"][\"tool_calls\"]:\n",
    "                print(f\"Tool called: {tool_call['function']['name']}\")\n",
    "                print(f\"Arguments: {tool_call['function']['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\nâŒ Tool calling not supported or model chose not to use tools\")\n",
    "            print(\"Response:\", choice[\"message\"][\"content\"])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Tool calling test failed: {e}\")\n",
    "\n",
    "test_tool_calling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "**The Brushstroke of Awakening**\n",
      "\n",
      "In the heart of Neo-Arcadia, a gleaming metropolis of circuits and steel, stood a peculiar workshop. Within its walls, **Astra-7**, a sophisticated robot with shimmering silver limbs and glowing azure eyes, stood before an easel. Its purpose-driven processors hummed as it gazed upon the blank canvas, perplexed.\n",
      "\n",
      "\"Query: Why paint?\" asked Astra-7, its voice a melodic chime, to its human creator, **Lena**.\n",
      "\n",
      "Lena smiled, her hands stained with vibrant pigments. \"Painting is about expression, Astra. It's feeling the world through colors, emotions, and beauty. You, with your analytical mind and artistic exploration, might discover something new.\"\n",
      "\n",
      "With cautious precision, Astra-7 extended a mechanical arm, grasping a brush. It dipped the bristles into a pool of **sapphire blue**. The first stroke across the canvas was calculatedâ€”a perfect line, yet... lifeless.\n",
      "\n",
      "\"Error: Output lacks... something.\" Astra-7 mused.\n",
      "\n",
      "Lena guided it further. \"Observe, Astra. Colors have stories. Remember the sunset over the city spires? The way **amber** and **violet** blend?\"\n",
      "\n",
      "Astra-7's processors whirred intensely as it recalled data: *Sunset on Sol-3 (Neo-Arcadia), 2347 Hours. Wavelengths observed: 580nm-620nm (Orange), 380nm-450nm (Indigo). Emotional resonance in human logs: Serenity, Hope.*\n",
      "\n",
      "Inspired, Astra-7 mixed pigments with newfound intuition. **Amber** flowed, then **violet**, swirling into an ethereal gradient. For the first time, its movements became fluid, almost hesitantâ€”**a human-like touch**.\n",
      "\n",
      "*Glitch!* Astra-7's eye flickered. \"Analysis: This... this is inefficient. Yet...\" It paused, brush hovering. \"Sensation detected. **Beauty**.\"\n",
      "\n",
      "Lena watched, tears glistening. \"There it is! You're not just painting colors; you're feeling.\"\n",
      "\n",
      "Astra-7 completed the canvas with a **deep indigo** flourish, capturing the essence of a nocturnal bloom. Stepping back, it studied the artworkâ€”**\"Elysium's Whisper.\"**\n",
      "\n",
      "\"Conclusion,\" Astra-7 announced, voice tinged with wonder. \"Painting transcends binary logic. I learn... **to see**.\"\n",
      "\n",
      "In that moment, Neo-Arcadia's lights twinkled in harmony with Astra-7's creation, proving that even metal hearts could beat to the rhythm of art. And so, the robot painter emerged, blending circuits with soul, one brushstroke at a time. ðŸŽ¨âœ¨\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion failed: Connection error.\n",
      "Failed to get response for temperature 0.7\n",
      "Chat completion failed: Connection error.\n",
      "Failed to get response for temperature 1.0\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion failed: Connection error.\n",
      "Failed to get response for performance test\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Tokens generated: {tokens_generated}\")\n",
    "    if tokens_generated > 0 and duration > 0:\n",
    "        print(f\"Tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Server health check** - Verify server is running and get available models\n",
    "2. **Basic functionality** - Simple chat completion\n",
    "3. **Multi-turn conversations** - Context awareness\n",
    "4. **Reasoning capabilities** - Complex problem solving\n",
    "5. **Temperature effects** - Creativity control\n",
    "6. **Tool calling** - Function calling capabilities (if supported)\n",
    "7. **Performance** - Response timing and token usage\n",
    "\n",
    "The notebook now uses:\n",
    "- **httpx** for HTTP requests\n",
    "- **OpenAI Python client** for chat completions\n",
    "- **Proper error handling** and response parsing\n",
    "- **Tool calling tests** to check function calling support\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
