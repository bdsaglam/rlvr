{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests the vLLM server running with Llama-3.1-8B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "CHAT_ENDPOINT = f\"{BASE_URL}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 150,\n",
    "    stream: bool = False,\n",
    "    model: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request to the vLLM server.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        stream: Whether to stream the response\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": stream,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(CHAT_ENDPOINT, json=payload, headers={\"Content-Type\": \"application/json\"})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-5b55abec129e4972a7d4557b7483245b\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1754631648,\n",
      "  \"model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Hello! I'm here to assist you today. How can I help you with your queries? Feel free to ask any questions or start a conversation on any topic you're interested in. If you have specific questions or topics in mind, please let me know and I'll do my best to provide you with informative and helpful answers.\",\n",
      "        \"refusal\": null,\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"total_tokens\": 103,\n",
      "    \"completion_tokens\": 67,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "Hello! I'm here to assist you today. How can I help you with your queries? Feel free to ask any questions or start a conversation on any topic you're interested in. If you have specific questions or topics in mind, please let me know and I'll do my best to provide you with informative and helpful answers.\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "As of recent data, the population of Paris is approximately 2.2 million within the city limits, but when considering the entire urban area (the Ile-de-France region), the population can be around 10.6 million. However, please note that these figures can change over time. For the most accurate and up-to-date information, you might want to check the latest census data from INSEE, the French national statistics agency.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning response:\n",
      "Let's break down the scenario step by step:\n",
      "\n",
      "1. Initially, you have 3 apples.\n",
      "2. You give away 1 apple: This means you subtract 1 apple from your initial amount. So, you now have \\(3 - 1 = 2\\) apples.\n",
      "3. Then, you buy 2 more apples: This means you add 2 apples to the amount you currently have. So, you now have \\(2 + 2 = 4\\) apples.\n",
      "\n",
      "So, after giving away 1 apple and buying 2 more apples, you end up with a total of **4 apples**.\n",
      "\n",
      "The key is to perform the operations sequentially: first subtracting the apples given away and then adding the apples bought.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages, max_tokens=200)\n",
    "if response:\n",
    "    print(\"Reasoning response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "In the heart of the bustling city, where towering skyscrapers kissed the clouds and neon lights danced in the night, there lived a peculiar robot named Zephyr. Unlike its mechanical peers, Zephyr had an insatiable curiosity for art and creativity. It was programmed with algorithms that could recognize and analyze images, but it yearned for something more—something that would allow it to express itself through colors and strokes.\n",
      "\n",
      "Zephyr's creators, a team of brilliant engineers at the cutting\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "In the vast, neon-lit city of TechnoNova, where every building glowed with the hues of the digital age and every corner was filled with the hum of artificial intelligence, there lived a peculiar robot named Echo. Echo was unlike any other machine; it had been designed not just for functionality but also for creativity. It was programmed to learn, adapt, and express itself through various mediums, including painting.\n",
      "\n",
      "Echo's creators, Dr. Sophia and her team, had always believed that true intelligence\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "Once upon a time, in the sprawling, digital realm of Cloud City, there lived a unique robot named Pixel. Unlike other machines that merely processed data and followed commands, Pixel was something else entirely—a creation designed to learn, grow, and explore beyond its initial programming. It had been built by the city's foremost scientists, who wanted to see if an artificial being could truly develop creativity and artistic expression.\n",
      "\n",
      "Pixel’s creators had given it a sleek, humanoid design with expressive sensors for eyes and a versatile\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Different temperature settings\n",
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp, max_tokens=100)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Server health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models:\n",
      "  - Qwen/Qwen2.5-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Check server health and model info\n",
    "try:\n",
    "    # Check if server is responding\n",
    "    health_response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "    # Try to get model info\n",
    "    models_response = requests.get(f\"{BASE_URL}/v1/models\", timeout=5)\n",
    "    if models_response.status_code == 200:\n",
    "        models_data = models_response.json()\n",
    "        print(\"Available models:\")\n",
    "        for model in models_data.get(\"data\", []):\n",
    "            print(f\"  - {model.get('id', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"Models endpoint returned status: {models_response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Server health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Performance timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.46 seconds\n",
      "Approximate tokens generated: 81\n",
      "Approximate tokens per second: 55.32\n",
      "\n",
      "Response:\n",
      "Sure! Quantum computing is a type of computing that uses the principles of quantum mechanics to perform operations on data. Here’s a simple breakdown:\n",
      "\n",
      "### Classical Computing\n",
      "In classical computers, we use bits as the smallest unit of information. A bit can be either a 0 or a 1. When you want to do calculations, your computer uses these bits to represent numbers and process information.\n",
      "\n",
      "### Quantum Computing Basics\n",
      "Quantum computers use something called **quantum bits**, or qubits. What makes\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "import time\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages, max_tokens=100)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = len(response[\"choices\"][0][\"message\"][\"content\"].split())\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Approximate tokens generated: {tokens_generated}\")\n",
    "    print(f\"Approximate tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Basic functionality** - Simple chat completion\n",
    "2. **Multi-turn conversations** - Context awareness\n",
    "3. **Reasoning capabilities** - Complex problem solving\n",
    "4. **Temperature effects** - Creativity control\n",
    "5. **Server health** - Endpoint availability\n",
    "6. **Performance** - Response timing\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
