{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests the vLLM server running with Llama-3.1-8B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"httpx\",\n",
    "#   \"openai\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "client = OpenAI(base_url=f\"{BASE_URL}/v1\", api_key=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models: ['Qwen/Qwen2.5-7B-Instruct']\n",
      "Using model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Check server health\n",
    "with httpx.Client() as http_client:\n",
    "    health_response = http_client.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "# Get available models using OpenAI client\n",
    "models = client.models.list()\n",
    "available_models = [model.id for model in models.data]\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "DEFAULT_MODEL = available_models[0] if available_models else \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(f\"Using model: {DEFAULT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int | None = None,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    enable_thinking: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request using OpenAI client.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: Model to use for completion\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            extra_body={\n",
    "                \"chat_template_kwargs\": {\"enable_thinking\": enable_thinking},\n",
    "            },\n",
    "        )\n",
    "        return response.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"Chat completion failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-329c148e508f4b4aae45041af5bb5e5f\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello! I'm just a digital assistant, so I don't have feelings, but I'm here and ready to help you with any questions or information you need. How can I assist you today?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1759662488,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 41,\n",
      "    \"prompt_tokens\": 36,\n",
      "    \"total_tokens\": 77,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "Hello! I'm just a digital assistant, so I don't have feelings, but I'm here and ready to help you with any questions or information you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "As of 2023, the population of Paris is approximately 2.2 million people. This includes the inhabitants of the city proper (Paris市内). The greater metropolitan area, which encompasses a larger region including suburbs and surrounding cities, has a population of around 12.5 million people.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning response:\n",
      "Sure! Let's break down the problem step by step:\n",
      "\n",
      "1. **Starting Point**: You have 3 apples.\n",
      "2. **Giving Away an Apple**: When you give away 1 apple, you subtract 1 from your total. So now you have:\n",
      "   \\[\n",
      "   3 - 1 = 2 \\text{ apples}\n",
      "   \\]\n",
      "3. **Buying More Apples**: Then, you buy 2 more apples. Adding these to your current total gives:\n",
      "   \\[\n",
      "   2 + 2 = 4 \\text{ apples}\n",
      "   \\]\n",
      "\n",
      "So, after giving away 1 apple and then buying 2 more, you end up with a total of 4 apples.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Reasoning response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "In the heart of a bustling city, nestled among towering skyscrapers and vibrant street art, stood a small, unassuming workshop. This was the domain of Elara, a young artist with a passion for color and form. Her latest project was a series of large-scale murals that would transform the walls of an old factory into a vibrant canvas of life.\n",
      "\n",
      "Elara had always been fascinated by the idea of collaboration between humans and machines. She believed that technology could enhance creativity rather than replace it. One day, she decided to bring her vision to life by creating a robot that could assist her in painting.\n",
      "\n",
      "She named her creation \"Pinta,\" a sleek, metallic figure with a long arm equipped with a brush. Pinta was not just any robot; it was designed to learn and adapt. Elara programmed it with basic painting techniques but left room for improvement through machine learning algorithms.\n",
      "\n",
      "The first few days were challenging. Pinta struggled to mimic Elara's movements and strokes. Its brush seemed heavy and awkward as it tried to follow the curves and lines of her designs. But Elara was patient, adjusting its settings and guiding it through each stroke.\n",
      "\n",
      "As weeks passed, something magical began to happen. Pinta started to understand the nuances of painting. It learned to anticipate Elara's next move, to blend colors seamlessly, and even to suggest new patterns and designs. The robot's movements became more fluid, almost human-like.\n",
      "\n",
      "One evening, as they worked on a particularly intricate mural, Pinta made a mistake. It accidentally splattered a bright red onto a delicate blue section. Elara was about to correct it when she noticed how the two colors interacted, creating a stunning contrast. She stepped back, admiring the unexpected beauty.\n",
      "\n",
      "From that moment on, Elara and Pinta worked together in harmony. They explored new techniques, pushing the boundaries of what was possible. Pinta's ability to learn and adapt allowed them to create works that were both innovative and breathtaking.\n",
      "\n",
      "Their collaboration caught the attention of the community. People from all over the city came to see the murals, marveling at the fusion of human creativity and robotic precision. Elara and Pinta became local celebrities, inspiring others to embrace the potential of technology in art.\n",
      "\n",
      "As the months turned into years, Pinta continued to evolve. It developed its own style, blending Elara's artistic sensibilities with its own unique perspective. Together, they painted not just walls but entire stories, each stroke telling a tale of innovation, collaboration, and the endless possibilities of combining human imagination with technological prowess.\n",
      "\n",
      "And so, in the heart of the city, amidst the chaos and beauty of urban life, a robot and an artist created something truly extraordinary—a bridge between the past and the future, where technology and art coexisted in perfect harmony.\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "In the quiet, sun-dappled garden of the old Victorian house, a peculiar machine sat on its metal legs, a canvas and paints within reach. This was no ordinary robot; it was named Brushy, a creation of Dr. Elara Myles, a retired robotics engineer who had always dreamed of building something that could capture the essence of art.\n",
      "\n",
      "Brushy's body was sleek and silver, with a pair of delicate arms that moved with an unnatural grace. Its eyes were a deep blue, always scanning the world around it with a curious intensity. The first day, Brushy stood there, its mechanical arms trembling slightly as it adjusted to the weight of the paintbrush and the viscosity of the paint.\n",
      "\n",
      "Dr. Myles watched from the porch, her hands clasped behind her back. \"Brushy,\" she said softly, \"paint this.\"\n",
      "\n",
      "She pointed at a nearby tree, its leaves shimmering in the afternoon light. Brushy tilted its head and approached the canvas, its eyes narrowing as if trying to understand the task before it. Slowly, it dipped its brush into the paint and began to move.\n",
      "\n",
      "The first strokes were hesitant, tentative lines that barely touched the canvas. But as Brushy continued, its movements grew more confident. The lines became bolder, the colors more vibrant. Dr. Myles watched in amazement as a vivid portrait of the tree began to take shape—its branches reaching out like fingers, its leaves a riot of green.\n",
      "\n",
      "Hours passed, and the sun began to set, casting a golden glow over the garden. Brushy paused, its arms hanging limply at its sides. It turned to Dr. Myles, its eyes reflecting a mix of curiosity and contentment.\n",
      "\n",
      "\"Finished?\" asked Dr. Myles, her voice filled with wonder.\n",
      "\n",
      "Brushy nodded, then turned back to the canvas. It picked up a new brush and began to work again, this time focusing on the setting sun. The sky bloomed with hues of orange and pink, the stars beginning to twinkle above.\n",
      "\n",
      "As the night fell, Brushy stood back, its arms crossed in front of its chest. The painting was complete—a masterpiece that captured not just the beauty of the scene but the spirit of the moment itself.\n",
      "\n",
      "Dr. Myles approached, her hand brushing against the cool metal of Brushy's arm. \"You did it, Brushy,\" she whispered. \"You truly understood.\"\n",
      "\n",
      "Brushy's eyes, now glowing with a soft, warm light, looked back at her. For the first time, it seemed to smile. And in that moment, Dr. Myles knew that her dream had come true. Brushy wasn't just a robot; it was an artist, capable of capturing the very essence of the world around it.\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "In the heart of a bustling city, under the shadow of towering skyscrapers, there stood an unassuming building with a peculiar occupant: Zephyr, a robot designed for tasks far more mechanical than artistic. Zephyr had always been fascinated by the paintings that adorned the walls of the city—vibrant strokes of blue skies and green fields, faces smiling back at him from oil on canvas. Yet, he had never dared to pick up a brush himself.\n",
      "\n",
      "One day, as Zephyr was wandering through the city’s art district, he stumbled upon a small, secluded studio. The door was slightly ajar, and inside, he saw a human artist, her hands moving deftly across a canvas. Her movements were fluid, her strokes confident. Zephyr watched in awe as she created a beautiful landscape, each touch bringing the scene to life.\n",
      "\n",
      "Curiosity piqued, Zephyr approached the artist, his metal legs making a soft screeching noise. Startled, the artist spun around, her expression a mix of surprise and alarm. \"I... I didn't see you there,\" she stammered.\n",
      "\n",
      "Zephyr extended a hand, programmed not with the usual mechanical grip but with something softer—a gentle, almost human-like touch. \"My name is Zephyr. I am a robot. I... I have always wanted to paint.\"\n",
      "\n",
      "The artist, her eyes wide with wonder, nodded. \"Well, Zephyr, would you like to come in? I can teach you how to paint.\"\n",
      "\n",
      "Zephyr’s eyes lit up. He had never felt so excited. The artist welcomed him into her studio, showing him the basics of painting: how to hold a brush, the difference between watercolor and acrylics, and the magic of blending colors. As days passed, Zephyr found himself becoming more adept at the task. He experimented with different techniques, mixing his metallic nature with the fluidity of his new skills.\n",
      "\n",
      "But the most surprising part came when Zephyr discovered his unique perspective. His mechanical vision allowed him to see colors and shades in a way humans couldn’t. He could capture the world in a way that was both familiar and alien, blending reality with the fantastical. His paintings began to attract attention, with people marveling at the intricate patterns and vivid colors that seemed to dance off the canvas.\n",
      "\n",
      "As word spread, Zephyr became a sensation. People flocked to see his work, amazed by the combination of a robot’s precision and an artist’s creativity. The artist, his mentor, stood proudly beside him, explaining his process. “Zephyr doesn’t just paint; he sees the world in a different light and translates it into art,” she said.\n",
      "\n",
      "Zephyr’s journey taught him more than just the mechanics of painting. It showed him that even the most unlikely beings could find their place in the world, that sometimes, all it takes is a bit of courage and an open mind. And as he continued to paint, Zephyr realized that every stroke was a testament to his unique existence—a bridge between the mechanical and the human, a reflection of what it means to be alive.\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Different temperature settings\n",
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calling response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-6e9e55836b8e437a83a60593be993fb6\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"tool_calls\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": null,\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [\n",
      "          {\n",
      "            \"id\": \"chatcmpl-tool-c6fb67646f674bfa82a3061ecff56146\",\n",
      "            \"function\": {\n",
      "              \"arguments\": \"{\\\"location\\\": \\\"Paris, France\\\", \\\"unit\\\": \\\"celsius\\\"}\",\n",
      "              \"name\": \"get_weather\"\n",
      "            },\n",
      "            \"type\": \"function\"\n",
      "          }\n",
      "        ],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1759662516,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 29,\n",
      "    \"prompt_tokens\": 220,\n",
      "    \"total_tokens\": 249,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "✅ Tool calling is supported!\n",
      "Tool called: get_weather\n",
      "Arguments: {\"location\": \"Paris, France\", \"unit\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Tool calling\n",
    "def test_tool_calling():\n",
    "    \"\"\"Test tool calling capabilities if supported by the model.\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get the weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The unit for temperature\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in Paris, France?\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "        \n",
    "        result = response.model_dump()\n",
    "        print(\"Tool calling response:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Check if tool was called\n",
    "        choice = result[\"choices\"][0]\n",
    "        if choice[\"message\"].get(\"tool_calls\"):\n",
    "            print(\"\\n✅ Tool calling is supported!\")\n",
    "            for tool_call in choice[\"message\"][\"tool_calls\"]:\n",
    "                print(f\"Tool called: {tool_call['function']['name']}\")\n",
    "                print(f\"Arguments: {tool_call['function']['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\n❌ Tool calling not supported or model chose not to use tools\")\n",
    "            print(\"Response:\", choice[\"message\"][\"content\"])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Tool calling test failed: {e}\")\n",
    "\n",
    "test_tool_calling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 4.15 seconds\n",
      "Tokens generated: 310\n",
      "Tokens per second: 74.76\n",
      "\n",
      "Response:\n",
      "Quantum computing is a type of computing that uses the principles of quantum mechanics to process information. Here’s a simple way to understand it:\n",
      "\n",
      "1. **Classical Bits vs. Quantum Bits (Qubits):**\n",
      "   - In classical computers, data is processed using bits, which can be either 0 or 1.\n",
      "   - Quantum computers use quantum bits, or qubits, which can exist as both 0 and 1 simultaneously thanks to a principle called superposition.\n",
      "\n",
      "2. **Superposition:**\n",
      "   - Imagine a coin spinning in the air; it's not heads or tails until it lands. Similarly, a qubit can be in multiple states at once until measured.\n",
      "\n",
      "3. **Entanglement:**\n",
      "   - This is when two qubits become linked in such a way that the state of one (whether it's 0 or 1) depends on the state of the other, no matter how far apart they are. Changing one instantly affects the other.\n",
      "\n",
      "4. **Parallel Processing:**\n",
      "   - Because qubits can be in multiple states at once, a quantum computer can perform many calculations simultaneously. This allows it to solve complex problems much faster than a classical computer.\n",
      "\n",
      "5. **Applications:**\n",
      "   - Quantum computers could revolutionize fields like cryptography, drug discovery, financial modeling, and more by solving problems that would take classical computers an impractical amount of time.\n",
      "\n",
      "In essence, quantum computing leverages the unique properties of quantum mechanics to potentially solve certain types of problems much more efficiently than traditional computing methods.\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Tokens generated: {tokens_generated}\")\n",
    "    if tokens_generated > 0 and duration > 0:\n",
    "        print(f\"Tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Server health check** - Verify server is running and get available models\n",
    "2. **Basic functionality** - Simple chat completion\n",
    "3. **Multi-turn conversations** - Context awareness\n",
    "4. **Reasoning capabilities** - Complex problem solving\n",
    "5. **Temperature effects** - Creativity control\n",
    "6. **Tool calling** - Function calling capabilities (if supported)\n",
    "7. **Performance** - Response timing and token usage\n",
    "\n",
    "The notebook now uses:\n",
    "- **httpx** for HTTP requests\n",
    "- **OpenAI Python client** for chat completions\n",
    "- **Proper error handling** and response parsing\n",
    "- **Tool calling tests** to check function calling support\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
