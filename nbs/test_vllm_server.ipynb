{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests the vLLM server running with Llama-3.1-8B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "CHAT_ENDPOINT = f\"{BASE_URL}/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int = 150,\n",
    "    stream: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request to the vLLM server.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        stream: Whether to stream the response\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": stream,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(CHAT_ENDPOINT, json=payload, headers={\"Content-Type\": \"application/json\"})\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-30b8d58b4cdc4f10856f414eaccb5947\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1754628410,\n",
      "  \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm doing well, thank you for asking. I'm a large language model, so I don't have emotions like humans do, but I'm here and ready to help with any questions or tasks you may have. How can I assist you today?\",\n",
      "        \"refusal\": null,\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 42,\n",
      "    \"total_tokens\": 94,\n",
      "    \"completion_tokens\": 52,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have emotions like humans do, but I'm here and ready to help with any questions or tasks you may have. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "The population of Paris, the capital city of France, is approximately 2.1 million people within the city limits. However, the larger metropolitan area of Paris, also known as the ÃŽle-de-France region, has a population of around 12.2 million people.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning response:\n",
      "Let's break down the problem step by step:\n",
      "\n",
      "1. You start with 3 apples.\n",
      "2. You give away 1 apple. Now you have 3 - 1 = 2 apples.\n",
      "3. You buy 2 more apples. Now you have 2 (remaining apples) + 2 (new apples) = 4 apples.\n",
      "\n",
      "So, in total, you have 4 apples.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages, max_tokens=200)\n",
    "if response:\n",
    "    print(\"Reasoning response:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "**The Brush of Creation**\n",
      "\n",
      "In a world where technology and art coexisted in perfect harmony, a brilliant inventor, Dr. Rachel Kim, had a vision to create a robot that could bring joy and beauty to the world through the art of painting. She named her creation \"Aurora,\" a sleek and agile robot with a mind capable of learning and adapting at an exponential rate.\n",
      "\n",
      "Aurora's journey began in a state-of-the-art studio, surrounded by an array of paints, brushes\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "**The Brushstrokes of a New Mind**\n",
      "\n",
      "In a world where art and technology collided, a brilliant inventor, Dr. Rachel Kim, stood before her latest creation: a robot designed to learn the art of painting. The robot, named Nova, stood tall, its slender metal body adorned with a canvas easel and a palette of vibrant colors.\n",
      "\n",
      "Nova's creator had imbued the robot with an advanced artificial intelligence, allowing it to process and analyze data at an incredible rate. The goal was for\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "**The Brushstrokes of Genius**\n",
      "\n",
      "In a world where art and technology collided, a brilliant inventor, Dr. Elara Vex, created a revolutionary robot designed to learn the art of painting. She named the robot \"Kaleido,\" a fusion of the words \"kaleidoscope\" and \"apocalypse,\" symbolizing the possibility of creating something entirely new and unpredictable.\n",
      "\n",
      "Kaleido stood tall, its sleek metal body adorned with glowing blue circuits and a gleaming silver arm, terminating\n"
     ]
    }
   ],
   "source": [
    "# Test 4: Different temperature settings\n",
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp, max_tokens=100)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Server health check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models:\n",
      "  - meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Test 5: Check server health and model info\n",
    "try:\n",
    "    # Check if server is responding\n",
    "    health_response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "    # Try to get model info\n",
    "    models_response = requests.get(f\"{BASE_URL}/v1/models\", timeout=5)\n",
    "    if models_response.status_code == 200:\n",
    "        models_data = models_response.json()\n",
    "        print(\"Available models:\")\n",
    "        for model in models_data.get(\"data\", []):\n",
    "            print(f\"  - {model.get('id', 'Unknown')}\")\n",
    "    else:\n",
    "        print(f\"Models endpoint returned status: {models_response.status_code}\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Server health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Performance timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 1.20 seconds\n",
      "Approximate tokens generated: 70\n",
      "Approximate tokens per second: 58.19\n",
      "\n",
      "Response:\n",
      "**What is Quantum Computing?**\n",
      "\n",
      "Quantum computing is a new way of processing information that's different from the classical computers we use today. While classical computers use \"bits\" to store and process information, quantum computers use \"qubits\" (quantum bits).\n",
      "\n",
      "**Classical Computers vs. Quantum Computers**\n",
      "\n",
      "Classical computers use \"bits\" that can only be in one of two states: 0 or 1. It's like a light switch that's either on (1) or off\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "import time\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages, max_tokens=100)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = len(response[\"choices\"][0][\"message\"][\"content\"].split())\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Approximate tokens generated: {tokens_generated}\")\n",
    "    print(f\"Approximate tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Basic functionality** - Simple chat completion\n",
    "2. **Multi-turn conversations** - Context awareness\n",
    "3. **Reasoning capabilities** - Complex problem solving\n",
    "4. **Temperature effects** - Creativity control\n",
    "5. **Server health** - Endpoint availability\n",
    "6. **Performance** - Response timing\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
