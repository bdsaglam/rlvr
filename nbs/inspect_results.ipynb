{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "from ipywidgets import widgets, HBox, VBox\n",
        "from IPython.display import display, HTML\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def jprint(obj):\n",
        "    print(json.dumps(obj, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Results Inspector\n",
        "\n",
        "This notebook provides tools to inspect and analyze evaluation results from conversational multi-hop QA tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_results(file_path):\n",
        "    \"\"\"Load evaluation results from a JSONL file\"\"\"\n",
        "    results = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            results.append(json.loads(line.strip()))\n",
        "    return results\n",
        "\n",
        "\n",
        "def results_to_dataframe(results):\n",
        "    \"\"\"Convert results to a pandas DataFrame with derived columns\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Extract key information\n",
        "    df[\"question\"] = df[\"prompt\"].apply(lambda x: x[1][\"content\"].split(\"\\n\")[0] if len(x) > 1 else \"\")\n",
        "    df[\"predicted_answer\"] = df['answer']\n",
        "    df[\"reference_answers\"] = df[\"info\"].map(lambda x: str(x[\"answers\"]))\n",
        "    df[\"n_hops\"] = df[\"info\"].map(lambda x: x[\"n_hops\"])\n",
        "\n",
        "    # Conversation analysis\n",
        "    df[\"n_turns\"] = df[\"completion\"].apply(count_turns)\n",
        "    df[\"n_tool_calls\"] = df[\"completion\"].apply(count_tool_calls)\n",
        "    df[\"used_supporting_docs\"] = df.apply(\n",
        "        lambda row: check_supporting_docs_usage(row[\"completion\"], row[\"info\"][\"docs\"]), axis=1\n",
        "    )\n",
        "\n",
        "    return df.drop(columns=[\"answer\"])\n",
        "\n",
        "\n",
        "def extract_final_answer(completion):\n",
        "    \"\"\"Extract the final answer from the completion\"\"\"\n",
        "    if not completion:\n",
        "        return \"\"\n",
        "\n",
        "    # Look for the last assistant message with an <answer> tag\n",
        "    for message in reversed(completion):\n",
        "        if message.get(\"role\") == \"assistant\" and \"content\" in message:\n",
        "            content = message[\"content\"]\n",
        "            # Extract text between <answer> tags\n",
        "            answer_match = re.search(r\"<answer>(.*?)</answer>\", content, re.DOTALL)\n",
        "            if answer_match:\n",
        "                return answer_match.group(1).strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def calculate_exact_match(predicted, ground_truth_list):\n",
        "    \"\"\"Calculate exact match score\"\"\"\n",
        "    if not predicted:\n",
        "        return 0\n",
        "\n",
        "    predicted_clean = predicted.lower().strip()\n",
        "    for gt in ground_truth_list:\n",
        "        if predicted_clean == gt.lower().strip():\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def count_turns(completion):\n",
        "    \"\"\"Count conversation turns\"\"\"\n",
        "    return len([msg for msg in completion if msg.get(\"role\") == \"assistant\"])\n",
        "\n",
        "\n",
        "def count_tool_calls(completion):\n",
        "    \"\"\"Count total tool calls made\"\"\"\n",
        "    count = 0\n",
        "    for msg in completion:\n",
        "        if msg.get(\"tool_calls\"):\n",
        "            count += len(msg[\"tool_calls\"])\n",
        "    return count\n",
        "\n",
        "\n",
        "def check_supporting_docs_usage(completion, docs):\n",
        "    \"\"\"Check if supporting documents were retrieved and used\"\"\"\n",
        "    supporting_doc_ids = [str(doc[\"id\"]) for doc in docs if doc.get(\"is_supporting\", False)]\n",
        "\n",
        "    # Look for document IDs mentioned in tool responses\n",
        "    used_doc_ids = set()\n",
        "    for msg in completion:\n",
        "        if msg.get(\"role\") == \"tool\" and \"content\" in msg:\n",
        "            content = msg[\"content\"]\n",
        "            # Extract document IDs from tool responses\n",
        "            doc_id_matches = re.findall(r\"Document ID: (\\d+)\", content)\n",
        "            used_doc_ids.update(doc_id_matches)\n",
        "\n",
        "    # Check how many supporting docs were used\n",
        "    used_supporting = len(set(supporting_doc_ids) & used_doc_ids)\n",
        "    total_supporting = len(supporting_doc_ids)\n",
        "\n",
        "    return used_supporting / total_supporting if total_supporting > 0 else 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Process Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your results file\n",
        "results_file = \"../outputs/sample.jsonl\"  # Change this to your actual results file\n",
        "results = load_results(results_file)\n",
        "df = results_to_dataframe(results)\n",
        "\n",
        "print(f\"Loaded {len(df)} evaluation results\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_summary_stats(df):\n",
        "    \"\"\"Print comprehensive summary statistics\"\"\"\n",
        "    print(\"=== EVALUATION SUMMARY ===\")\n",
        "    print(f\"Total examples: {len(df)}\")\n",
        "    print(f\"Exact Match: {df['exact_match'].mean():.3f} ({df['exact_match'].sum()}/{len(df)})\")\n",
        "    print(f\"Average reward: {df['reward'].mean():.3f}\")\n",
        "    \n",
        "    print(\"\\n=== BY NUMBER OF HOPS ===\")\n",
        "    hop_stats = df.groupby('n_hops').agg({\n",
        "        'exact_match': ['count', 'mean'],\n",
        "        'reward': 'mean'\n",
        "    }).round(3)\n",
        "    hop_stats.columns = ['count', 'exact_match', 'avg_reward']\n",
        "    print(hop_stats)\n",
        "    \n",
        "    print(\"\\n=== CONVERSATION STATS ===\")\n",
        "    print(f\"Average turns: {df['n_turns'].mean():.1f}\")\n",
        "    print(f\"Average tool calls: {df['n_tool_calls'].mean():.1f}\")\n",
        "    print(f\"Average supporting docs used: {df['used_supporting_docs'].mean():.3f}\")\n",
        "    \n",
        "    print(\"\\n=== TURN DISTRIBUTION ===\")\n",
        "    turn_dist = df['n_turns'].value_counts().sort_index()\n",
        "    for turns, count in turn_dist.items():\n",
        "        print(f\"{turns} turns: {count} examples ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "print_summary_stats(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance by number of hops\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Exact match by hops\n",
        "hop_performance = df.groupby('n_hops')['exact_match'].mean()\n",
        "axes[0,0].bar(hop_performance.index, hop_performance.values)\n",
        "axes[0,0].set_title('Exact Match by Number of Hops')\n",
        "axes[0,0].set_xlabel('Number of Hops')\n",
        "axes[0,0].set_ylabel('Exact Match Rate')\n",
        "\n",
        "# Tool calls distribution\n",
        "axes[0,1].hist(df['n_tool_calls'], bins=20, alpha=0.7)\n",
        "axes[0,1].set_title('Distribution of Tool Calls')\n",
        "axes[0,1].set_xlabel('Number of Tool Calls')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "\n",
        "# Supporting docs usage vs performance\n",
        "axes[1,0].scatter(df['used_supporting_docs'], df['exact_match'], alpha=0.6)\n",
        "axes[1,0].set_title('Supporting Docs Usage vs Exact Match')\n",
        "axes[1,0].set_xlabel('Fraction of Supporting Docs Used')\n",
        "axes[1,0].set_ylabel('Exact Match')\n",
        "\n",
        "# Reward distribution\n",
        "axes[1,1].hist(df['reward'], bins=20, alpha=0.7)\n",
        "axes[1,1].set_title('Reward Distribution')\n",
        "axes[1,1].set_xlabel('Reward')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Browser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_conversation(completion):\n",
        "    \"\"\"Format conversation for display\"\"\"\n",
        "    formatted = []\n",
        "    \n",
        "    for i, message in enumerate(completion):\n",
        "        role = message.get('role', 'unknown')\n",
        "        content = message.get('content', '')\n",
        "        \n",
        "        if role == 'assistant':\n",
        "            # Check for tool calls\n",
        "            if message.get('tool_calls'):\n",
        "                tool_calls = message.get('tool_calls', [])\n",
        "                for tc in tool_calls:\n",
        "                    if isinstance(tc, str):\n",
        "                        tc_data = json.loads(tc)\n",
        "                        func_name = tc_data.get('function', {}).get('name', 'unknown')\n",
        "                        args = tc_data.get('function', {}).get('arguments', '{}')\n",
        "                        formatted.append(f\"ðŸ”§ **Tool Call**: {func_name}({args})\")\n",
        "            \n",
        "            if content:\n",
        "                formatted.append(f\"ðŸ¤– **Assistant**: {content}\")\n",
        "                \n",
        "        elif role == 'tool':\n",
        "            # Truncate long tool responses\n",
        "            truncated = content[:500] + \"...\" if len(content) > 500 else content\n",
        "            formatted.append(f\"âš™ï¸ **Tool Response**: {truncated}\")\n",
        "            \n",
        "    return \"\\n\\n\".join(formatted)\n",
        "\n",
        "def format_documents(docs):\n",
        "    \"\"\"Format document information\"\"\"\n",
        "    doc_info = []\n",
        "    for doc in docs:\n",
        "        support_status = \"âœ… Supporting\" if doc.get('is_supporting', False) else \"âŒ Non-supporting\"\n",
        "        title = doc.get('title', 'No title')\n",
        "        doc_info.append(f\"Doc {doc['id']}: {title} ({support_status})\")\n",
        "    return \"\\n\".join(doc_info)\n",
        "\n",
        "def format_example(row):\n",
        "    \"\"\"Format a single example for display\"\"\"\n",
        "    data = [\n",
        "        [\"Question\", textwrap.fill(row['question'], width=80)],\n",
        "        [\"Ground Truth\", str(row['info']['answers'])],\n",
        "        [\"Predicted\", row['predicted_answer']],\n",
        "        [\"Exact Match\", \"âœ… Yes\" if row['exact_match'] else \"âŒ No\"],\n",
        "        [\"Reward\", f\"{row['reward']:.3f}\"],\n",
        "        [\"Hops\", str(row['n_hops'])],\n",
        "        [\"Turns\", str(row['n_turns'])],\n",
        "        [\"Tool Calls\", str(row['n_tool_calls'])],\n",
        "        [\"Supporting Docs Used\", f\"{row['used_supporting_docs']:.1%}\"],\n",
        "        [\"\", \"\"],  # Separator\n",
        "        [\"Available Documents\", format_documents(row['info']['docs'])],\n",
        "        [\"\", \"\"],  # Separator\n",
        "        [\"Conversation\", format_conversation(row['completion'])],\n",
        "    ]\n",
        "    return tabulate(data, tablefmt='grid')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_browser(df):\n",
        "    \"\"\"Create interactive browser for examples\"\"\"\n",
        "    \n",
        "    # Filters\n",
        "    correct_only = widgets.Checkbox(description='Correct answers only', value=False)\n",
        "    incorrect_only = widgets.Checkbox(description='Incorrect answers only', value=False)\n",
        "    hop_filter = widgets.Dropdown(\n",
        "        options=['All'] + sorted(df['n_hops'].unique().tolist()),\n",
        "        value='All',\n",
        "        description='Hops:'\n",
        "    )\n",
        "    \n",
        "    # Navigation\n",
        "    index_widget = widgets.IntText(value=0, description='Index:')\n",
        "    prev_button = widgets.Button(description='Previous')\n",
        "    next_button = widgets.Button(description='Next')\n",
        "    random_button = widgets.Button(description='Random')\n",
        "    \n",
        "    # Output\n",
        "    output = widgets.Output()\n",
        "    \n",
        "    def get_filtered_df():\n",
        "        filtered_df = df.copy()\n",
        "        \n",
        "        if correct_only.value:\n",
        "            filtered_df = filtered_df[filtered_df['exact_match'] == 1]\n",
        "        elif incorrect_only.value:\n",
        "            filtered_df = filtered_df[filtered_df['exact_match'] == 0]\n",
        "            \n",
        "        if hop_filter.value != 'All':\n",
        "            filtered_df = filtered_df[filtered_df['n_hops'] == hop_filter.value]\n",
        "            \n",
        "        return filtered_df.reset_index(drop=True)\n",
        "    \n",
        "    def update_display():\n",
        "        filtered_df = get_filtered_df()\n",
        "        \n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            \n",
        "            if len(filtered_df) == 0:\n",
        "                print(\"No examples match the current filters.\")\n",
        "                return\n",
        "                \n",
        "            idx = max(0, min(index_widget.value, len(filtered_df) - 1))\n",
        "            index_widget.value = idx\n",
        "            \n",
        "            print(f\"Example {idx + 1} of {len(filtered_df)} (filtered)\")\n",
        "            print(\"=\" * 80)\n",
        "            print(format_example(filtered_df.iloc[idx]))\n",
        "    \n",
        "    def on_prev_clicked(b):\n",
        "        index_widget.value = max(0, index_widget.value - 1)\n",
        "        \n",
        "    def on_next_clicked(b):\n",
        "        filtered_df = get_filtered_df()\n",
        "        index_widget.value = min(len(filtered_df) - 1, index_widget.value + 1)\n",
        "        \n",
        "    def on_random_clicked(b):\n",
        "        filtered_df = get_filtered_df()\n",
        "        if len(filtered_df) > 0:\n",
        "            import numpy as np\n",
        "            index_widget.value = np.random.randint(0, len(filtered_df))\n",
        "    \n",
        "    # Wire up events\n",
        "    prev_button.on_click(on_prev_clicked)\n",
        "    next_button.on_click(on_next_clicked)\n",
        "    random_button.on_click(on_random_clicked)\n",
        "    \n",
        "    # Update display when any control changes\n",
        "    widgets.interactive_output(lambda *args: update_display(), {\n",
        "        'idx': index_widget,\n",
        "        'correct': correct_only,\n",
        "        'incorrect': incorrect_only,\n",
        "        'hops': hop_filter\n",
        "    })\n",
        "    \n",
        "    # Layout\n",
        "    filters = HBox([correct_only, incorrect_only, hop_filter])\n",
        "    navigation = HBox([prev_button, index_widget, next_button, random_button])\n",
        "    \n",
        "    display(VBox([filters, navigation, output]))\n",
        "    \n",
        "    # Initial display\n",
        "    update_display()\n",
        "\n",
        "# Create the browser\n",
        "create_browser(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_errors(df):\n",
        "    \"\"\"Analyze common error patterns\"\"\"\n",
        "    incorrect = df[df['exact_match'] == 0]\n",
        "    \n",
        "    print(f\"=== ERROR ANALYSIS ({len(incorrect)} incorrect examples) ===\")\n",
        "    \n",
        "    # By number of hops\n",
        "    print(\"\\nError rate by hops:\")\n",
        "    hop_errors = df.groupby('n_hops').agg({\n",
        "        'exact_match': lambda x: 1 - x.mean()\n",
        "    }).round(3)\n",
        "    print(hop_errors)\n",
        "    \n",
        "    # Supporting document usage\n",
        "    print(\"\\nSupporting document usage comparison:\")\n",
        "    print(f\"Correct answers: {df[df['exact_match']==1]['used_supporting_docs'].mean():.3f}\")\n",
        "    print(f\"Incorrect answers: {df[df['exact_match']==0]['used_supporting_docs'].mean():.3f}\")\n",
        "    \n",
        "    # Tool call patterns\n",
        "    print(\"\\nTool call patterns:\")\n",
        "    print(f\"Correct answers avg tool calls: {df[df['exact_match']==1]['n_tool_calls'].mean():.1f}\")\n",
        "    print(f\"Incorrect answers avg tool calls: {df[df['exact_match']==0]['n_tool_calls'].mean():.1f}\")\n",
        "    \n",
        "    # Examples with no predicted answer\n",
        "    no_answer = df[df['predicted_answer'] == '']\n",
        "    print(f\"\\nExamples with no predicted answer: {len(no_answer)} ({len(no_answer)/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    return incorrect\n",
        "\n",
        "error_df = analyze_errors(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add your custom analysis here\n",
        "# For example, analyze specific question types, patterns in tool usage, etc.\n",
        "\n",
        "# Example: Look at questions that require exactly 2 hops\n",
        "two_hop_questions = df[df['n_hops'] == 2]\n",
        "print(f\"2-hop questions performance: {two_hop_questions['exact_match'].mean():.3f}\")\n",
        "\n",
        "# Example: Correlation between tool calls and performance\n",
        "correlation = df['n_tool_calls'].corr(df['exact_match'])\n",
        "print(f\"Correlation between tool calls and exact match: {correlation:.3f}\")\n",
        "\n",
        "# Example: Most common failure modes\n",
        "print(\"\\nMost common failure cases (first 5):\")\n",
        "failed_examples = df[df['exact_match'] == 0].head()\n",
        "for idx, row in failed_examples.iterrows():\n",
        "    print(f\"- Question: {row['question'][:100]}...\")\n",
        "    print(f\"  Predicted: '{row['predicted_answer']}'\")\n",
        "    print(f\"  Expected: {row['info']['answers']}\")\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
