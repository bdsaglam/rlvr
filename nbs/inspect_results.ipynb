{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "from ipywidgets import widgets, HBox, VBox\n",
        "from IPython.display import display\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "def jprint(obj):\n",
        "    print(json.dumps(obj, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Results Inspector\n",
        "\n",
        "This notebook provides tools to inspect and analyze evaluation results from conversational multi-hop QA tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import verifiers as vf\n",
        "\n",
        "def load_results(file_path):\n",
        "    \"\"\"Load evaluation results from a JSONL file\"\"\"\n",
        "    results = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            results.append(json.loads(line.strip()))\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_predicted_answer(completion):\n",
        "    \"\"\"Extract the predicted answer from the completion\"\"\"\n",
        "    if not completion:\n",
        "        return \"\"\n",
        "\n",
        "    last_message = completion[-1]\n",
        "    if tool_calls := last_message.get(\"tool_calls\"):\n",
        "        for tool_call in tool_calls:\n",
        "            tool_call_obj = json.loads(tool_call)\n",
        "            function = tool_call_obj['function']\n",
        "            if function['name'] == \"complete\":\n",
        "                arguments = json.loads(function['arguments'])\n",
        "                return arguments['final_answer']\n",
        "    else:\n",
        "        parser = vf.XMLParser(['think', 'cite', 'answer'])\n",
        "        parsed = parser.parse(last_message['content'])\n",
        "        return parsed.answer\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def results_to_dataframe(results):\n",
        "    \"\"\"Convert results to a pandas DataFrame with derived columns\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Extract key information\n",
        "    df[\"question\"] = df[\"prompt\"].apply(lambda x: x[1][\"content\"].split(\"\\n\")[0] if len(x) > 1 else \"\")\n",
        "    df[\"reference_answers\"] = df[\"info\"].map(lambda x: str(x[\"answers\"]))\n",
        "    df[\"n_hops\"] = df[\"info\"].map(lambda x: x[\"n_hops\"])\n",
        "\n",
        "    # Conversation analysis\n",
        "    df[\"predicted_answer\"] = df[\"completion\"].apply(extract_predicted_answer)\n",
        "    df[\"n_turns\"] = df[\"completion\"].apply(count_turns)\n",
        "    df[\"n_tool_calls\"] = df[\"completion\"].apply(count_tool_calls)\n",
        "    df[\"used_supporting_docs\"] = df.apply(\n",
        "        lambda row: check_supporting_docs_usage(row[\"completion\"], row[\"info\"][\"docs\"]), axis=1\n",
        "    )\n",
        "\n",
        "    df.drop(columns=[\"reward\"], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def calculate_exact_match(predicted, ground_truth_list):\n",
        "    \"\"\"Calculate exact match score\"\"\"\n",
        "    if not predicted:\n",
        "        return 0\n",
        "\n",
        "    predicted_clean = predicted.lower().strip()\n",
        "    for gt in ground_truth_list:\n",
        "        if predicted_clean == gt.lower().strip():\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def count_turns(completion):\n",
        "    \"\"\"Count conversation turns\"\"\"\n",
        "    return len([msg for msg in completion if msg.get(\"role\") == \"assistant\"])\n",
        "\n",
        "\n",
        "def count_tool_calls(completion):\n",
        "    \"\"\"Count total tool calls made\"\"\"\n",
        "    count = 0\n",
        "    for msg in completion:\n",
        "        if msg.get(\"tool_calls\"):\n",
        "            count += len(msg[\"tool_calls\"])\n",
        "    return count\n",
        "\n",
        "\n",
        "def check_supporting_docs_usage(completion, docs):\n",
        "    \"\"\"Check if supporting documents were retrieved and used\"\"\"\n",
        "    supporting_doc_ids = [str(doc[\"id\"]) for doc in docs if doc.get(\"is_supporting\", False)]\n",
        "\n",
        "    # Look for document IDs mentioned in tool responses\n",
        "    used_doc_ids = set()\n",
        "    for msg in completion:\n",
        "        if msg.get(\"role\") == \"tool\" and \"content\" in msg:\n",
        "            content = msg[\"content\"]\n",
        "            # Extract document IDs from tool responses\n",
        "            doc_id_matches = re.findall(r\"Document ID: (\\d+)\", content)\n",
        "            used_doc_ids.update(doc_id_matches)\n",
        "\n",
        "    # Check how many supporting docs were used\n",
        "    used_supporting = len(set(supporting_doc_ids) & used_doc_ids)\n",
        "    total_supporting = len(supporting_doc_ids)\n",
        "\n",
        "    return used_supporting / total_supporting if total_supporting > 0 else 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Process Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your results file\n",
        "results_file = \"../outputs/musique-eval/20250926_181827/musique-eval-results.jsonl\"  # Change this to your actual results file\n",
        "results = load_results(results_file)\n",
        "df = results_to_dataframe(results)\n",
        "\n",
        "print(f\"Loaded {len(df)} evaluation results\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced reward metrics display\n",
        "def show_reward_breakdown(df):\n",
        "    print(\"=== DETAILED REWARD BREAKDOWN ===\")\n",
        "    reward_cols = [\n",
        "        \"exact_match_reward\",\n",
        "        \"f1_reward\",\n",
        "        \"retrieval_recall_reward\",\n",
        "        \"citation\",\n",
        "        \"format_reward\",\n",
        "        \"combined_reward\",\n",
        "    ]\n",
        "\n",
        "    for col in reward_cols:\n",
        "        if col in df.columns:\n",
        "            print(f\"{col}: {df[col].mean():.3f} Â± {df[col].std():.3f}\")\n",
        "\n",
        "\n",
        "show_reward_breakdown(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced visualizations with reward metrics\n",
        "\n",
        "# Create reward comparison plots\n",
        "reward_cols = [\n",
        "    \"exact_match_reward\",\n",
        "    \"f1_reward\",\n",
        "    \"retrieval_recall_reward\",\n",
        "    \"citation_reward\",\n",
        "    \"format_reward\",\n",
        "    \"combined_reward\",\n",
        "]\n",
        "available_rewards = [col for col in reward_cols if col in df.columns]\n",
        "\n",
        "if len(available_rewards) > 0:\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Reward distributions\n",
        "    for i, col in enumerate(available_rewards):\n",
        "        if i < len(axes):\n",
        "            axes[i].hist(df[col], bins=20, alpha=0.7, edgecolor=\"black\")\n",
        "            axes[i].set_title(f\"{col.replace('_', ' ').title()} Distribution\")\n",
        "            axes[i].set_xlabel(\"Reward Value\")\n",
        "            axes[i].set_ylabel(\"Frequency\")\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(len(available_rewards), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation heatmap of reward metrics\n",
        "    if len(available_rewards) > 1:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        reward_corr = df[available_rewards].corr()\n",
        "        sns.heatmap(reward_corr, annot=True, cmap=\"coolwarm\", center=0, fmt=\".3f\", square=True)\n",
        "        plt.title(\"Reward Metrics Correlation Matrix\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Reward metrics by number of hops\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    hop_groups = df.groupby(\"n_hops\")[available_rewards].mean()\n",
        "    hop_groups.plot(kind=\"bar\", ax=plt.gca())\n",
        "    plt.title(\"Average Reward Metrics by Number of Hops\")\n",
        "    plt.xlabel(\"Number of Hops\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No detailed reward columns found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance by number of hops\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# Exact match by hops\n",
        "hop_performance = df.groupby(\"n_hops\")[\"exact_match_reward\"].mean()\n",
        "axes[0, 0].bar(hop_performance.index, hop_performance.values)\n",
        "axes[0, 0].set_title(\"Exact Match by Number of Hops\")\n",
        "axes[0, 0].set_xlabel(\"Number of Hops\")\n",
        "axes[0, 0].set_ylabel(\"Exact Match Rate\")\n",
        "\n",
        "# Tool calls distribution\n",
        "axes[0, 1].hist(df[\"n_tool_calls\"], bins=20, alpha=0.7)\n",
        "axes[0, 1].set_title(\"Distribution of Tool Calls\")\n",
        "axes[0, 1].set_xlabel(\"Number of Tool Calls\")\n",
        "axes[0, 1].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Supporting docs usage vs performance\n",
        "axes[1, 0].scatter(df[\"used_supporting_docs\"], df[\"exact_match_reward\"], alpha=0.6)\n",
        "axes[1, 0].set_title(\"Supporting Docs Usage vs Exact Match\")\n",
        "axes[1, 0].set_xlabel(\"Fraction of Supporting Docs Used\")\n",
        "axes[1, 0].set_ylabel(\"Exact Match\")\n",
        "\n",
        "# Reward distribution\n",
        "axes[1, 1].hist(df[\"combined_reward\"], bins=20, alpha=0.7)\n",
        "axes[1, 1].set_title(\"Combined Reward Distribution\")\n",
        "axes[1, 1].set_xlabel(\"Combined Reward\")\n",
        "axes[1, 1].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Browser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced format_example function to include all reward metrics\n",
        "\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def wrap_text(text, width=120):\n",
        "    return \"\\n\\n\".join(textwrap.wrap(text, width=width))\n",
        "\n",
        "\n",
        "def format_documents(docs):\n",
        "    \"\"\"Format document information\"\"\"\n",
        "    doc_info = []\n",
        "    for doc in docs:\n",
        "        support_status = \"â Supporting\" if doc.get(\"is_supporting\", False) else \"â Non-supporting\"\n",
        "        title = doc.get(\"title\", \"No title\")\n",
        "        doc_info.append(f\"Doc {doc['id']}: {title} ({support_status})\")\n",
        "    return \"\\n\".join(doc_info)\n",
        "\n",
        "\n",
        "def format_conversation(conversation):\n",
        "    \"\"\"Format conversation for display\"\"\"\n",
        "    return \"\\n\\n\\n\".join([f\"{msg['role']}: {wrap_text(msg['content'])}\" for msg in conversation])\n",
        "\n",
        "\n",
        "def format_example_enhanced(row):\n",
        "    \"\"\"Format a single example for display with all reward metrics\"\"\"\n",
        "\n",
        "    # Basic information\n",
        "    data = [\n",
        "        [\"Question\", wrap_text(row[\"question\"])],\n",
        "        [\"Ground Truth\", str(row[\"info\"][\"answers\"])],\n",
        "        [\"Predicted\", row[\"predicted_answer\"]],\n",
        "        [\"Exact Match\", \"â Yes\" if row[\"exact_match_reward\"] else \"â No\"],\n",
        "        [\"\", \"\"],  # Separator\n",
        "    ]\n",
        "\n",
        "    # Add all reward metrics\n",
        "    reward_metrics = [\n",
        "        (\"Combined Reward\", \"combined_reward\"),\n",
        "        (\"Exact Match\", \"exact_match_reward\"),\n",
        "        (\"F1 Score\", \"f1_reward\"),\n",
        "        (\"Retrieval Recall\", \"retrieval_recall_reward\"),\n",
        "        (\"Citation Reward\", \"citation_reward\"),\n",
        "        (\"Format Reward\", \"format_reward\"),\n",
        "    ]\n",
        "\n",
        "    data.append([\"=== REWARD BREAKDOWN ===\", \"\"])\n",
        "    for name, col in reward_metrics:\n",
        "        if col in row and pd.notnull(row[col]):\n",
        "            data.append([name, f\"{row[col]:.3f}\"])\n",
        "\n",
        "    # Task and conversation info\n",
        "    data.extend(\n",
        "        [\n",
        "            [\"\", \"\"],  # Separator\n",
        "            [\"Task\", row.get(\"task\", \"default\")],\n",
        "            [\"Hops\", str(row[\"n_hops\"])],\n",
        "            [\"Turns\", str(row[\"n_turns\"])],\n",
        "            [\"Tool Calls\", str(row[\"n_tool_calls\"])],\n",
        "            [\"Supporting Docs Used\", f\"{row['used_supporting_docs']:.1%}\"],\n",
        "            [\"\", \"\"],  # Separator\n",
        "            [\"Available Documents\", format_documents(row[\"info\"][\"docs\"])],\n",
        "            [\"\", \"\"],  # Separator\n",
        "            [\"Conversation\", format_conversation(row[\"completion\"])],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return tabulate(data, tablefmt=\"grid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_enhanced_browser(df):\n",
        "    \"\"\"Create interactive browser for examples with reward metric filtering\"\"\"\n",
        "\n",
        "    # Filters\n",
        "    correct_only = widgets.Checkbox(description=\"Correct answers only\", value=False)\n",
        "    incorrect_only = widgets.Checkbox(description=\"Incorrect answers only\", value=False)\n",
        "    hop_filter = widgets.Dropdown(\n",
        "        options=[\"All\"] + sorted(df[\"n_hops\"].unique().tolist()), value=\"All\", description=\"Hops:\"\n",
        "    )\n",
        "\n",
        "    # Reward threshold sliders\n",
        "    reward_threshold = widgets.FloatSlider(\n",
        "        value=0.0, min=0.0, max=1.0, step=0.1, description=\"Min Reward:\"\n",
        "    )\n",
        "\n",
        "    f1_threshold = widgets.FloatSlider(value=0.0, min=0.0, max=1.0, step=0.1, description=\"Min F1:\")\n",
        "\n",
        "    # Navigation\n",
        "    index_widget = widgets.IntText(value=0, description=\"Index:\")\n",
        "    prev_button = widgets.Button(description=\"Previous\")\n",
        "    next_button = widgets.Button(description=\"Next\")\n",
        "    random_button = widgets.Button(description=\"Random\")\n",
        "\n",
        "    # Output\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def get_filtered_df():\n",
        "        filtered_df = df.copy()\n",
        "\n",
        "        if correct_only.value:\n",
        "            filtered_df = filtered_df[filtered_df[\"exact_match_reward\"] == 1]\n",
        "        elif incorrect_only.value:\n",
        "            filtered_df = filtered_df[filtered_df[\"exact_match_reward\"] == 0]\n",
        "\n",
        "        if hop_filter.value != \"All\":\n",
        "            filtered_df = filtered_df[filtered_df[\"n_hops\"] == hop_filter.value]\n",
        "\n",
        "        # Apply reward thresholds\n",
        "        if \"combined_reward\" in filtered_df.columns:\n",
        "            filtered_df = filtered_df[filtered_df[\"combined_reward\"] >= reward_threshold.value]\n",
        "        if \"f1_reward\" in filtered_df.columns:\n",
        "            filtered_df = filtered_df[filtered_df[\"f1_reward\"] >= f1_threshold.value]\n",
        "\n",
        "        return filtered_df.reset_index(drop=True)\n",
        "\n",
        "    def update_display():\n",
        "        filtered_df = get_filtered_df()\n",
        "\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "\n",
        "            if len(filtered_df) == 0:\n",
        "                print(\"No examples match the current filters.\")\n",
        "                return\n",
        "\n",
        "            idx = max(0, min(index_widget.value, len(filtered_df) - 1))\n",
        "            index_widget.value = idx\n",
        "\n",
        "            print(f\"Example {idx + 1} of {len(filtered_df)} (filtered)\")\n",
        "            print(\"=\" * 120)\n",
        "            print(format_example_enhanced(filtered_df.iloc[idx]))\n",
        "\n",
        "    def on_prev_clicked(b):\n",
        "        index_widget.value = max(0, index_widget.value - 1)\n",
        "        update_display()\n",
        "\n",
        "    def on_next_clicked(b):\n",
        "        filtered_df = get_filtered_df()\n",
        "        index_widget.value = min(len(filtered_df) - 1, index_widget.value + 1)\n",
        "        update_display()\n",
        "\n",
        "    def on_random_clicked(b):\n",
        "        filtered_df = get_filtered_df()\n",
        "        if len(filtered_df) > 0:\n",
        "            import numpy as np\n",
        "\n",
        "            index_widget.value = np.random.randint(0, len(filtered_df))\n",
        "        update_display()\n",
        "\n",
        "    # Wire up events\n",
        "    prev_button.on_click(on_prev_clicked)\n",
        "    next_button.on_click(on_next_clicked)\n",
        "    random_button.on_click(on_random_clicked)\n",
        "\n",
        "    # Update display when any control changes\n",
        "    widgets.interactive_output(\n",
        "        lambda *args: update_display(),\n",
        "        {\n",
        "            \"idx\": index_widget,\n",
        "            \"correct\": correct_only,\n",
        "            \"incorrect\": incorrect_only,\n",
        "            \"hops\": hop_filter,\n",
        "            \"reward_thresh\": reward_threshold,\n",
        "            \"f1_thresh\": f1_threshold,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Layout\n",
        "    filters = HBox([correct_only, incorrect_only, hop_filter])\n",
        "    thresholds = HBox([reward_threshold, f1_threshold])\n",
        "    navigation = HBox([prev_button, index_widget, next_button, random_button])\n",
        "\n",
        "    display(VBox([filters, thresholds, navigation, output]))\n",
        "\n",
        "    # Initial display\n",
        "    update_display()\n",
        "\n",
        "\n",
        "# Create the enhanced browser\n",
        "create_enhanced_browser(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced error analysis with reward breakdown\n",
        "\n",
        "\n",
        "def analyze_errors_enhanced(df):\n",
        "    \"\"\"Analyze common error patterns with detailed reward breakdown\"\"\"\n",
        "    incorrect = df[df[\"exact_match_reward\"] == 0]\n",
        "\n",
        "    print(f\"=== ENHANCED ERROR ANALYSIS ({len(incorrect)} incorrect examples) ===\")\n",
        "\n",
        "    # By number of hops\n",
        "    print(\"\\nError rate by hops:\")\n",
        "    hop_errors = df.groupby(\"n_hops\").agg({\"exact_match_reward\": lambda x: 1 - x.mean()}).round(3)\n",
        "    print(hop_errors)\n",
        "\n",
        "    # Reward component analysis\n",
        "    print(\"\\n=== REWARD COMPONENT ANALYSIS ===\")\n",
        "    reward_cols = [\n",
        "        \"exact_match_reward\",\n",
        "        \"f1_reward\",\n",
        "        \"retrieval_recall_reward\",\n",
        "        \"citation_reward\",\n",
        "        \"format_reward\",\n",
        "        \"combined_reward\",\n",
        "    ]\n",
        "\n",
        "    comparison_data = []\n",
        "    for col in reward_cols:\n",
        "        if col in df.columns:\n",
        "            correct_mean = df[df[\"exact_match_reward\"] == 1][col].mean()\n",
        "            incorrect_mean = df[df[\"exact_match_reward\"] == 0][col].mean()\n",
        "            comparison_data.append(\n",
        "                [\n",
        "                    col.replace(\"_\", \" \").title(),\n",
        "                    f\"{correct_mean:.3f}\",\n",
        "                    f\"{incorrect_mean:.3f}\",\n",
        "                    f\"{correct_mean - incorrect_mean:+.3f}\",\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if comparison_data:\n",
        "        print(tabulate(comparison_data, headers=[\"Metric\", \"Correct\", \"Incorrect\", \"Difference\"], tablefmt=\"grid\"))\n",
        "\n",
        "    # Supporting document usage\n",
        "    print(\"\\n=== SUPPORTING DOCUMENT USAGE ===\")\n",
        "    print(f\"Correct answers: {df[df['exact_match_reward'] == 1]['used_supporting_docs'].mean():.3f}\")\n",
        "    print(f\"Incorrect answers: {df[df['exact_match_reward'] == 0]['used_supporting_docs'].mean():.3f}\")\n",
        "\n",
        "    # Tool call patterns\n",
        "    print(\"\\n=== TOOL CALL PATTERNS ===\")\n",
        "    print(f\"Correct answers avg tool calls: {df[df['exact_match_reward'] == 1]['n_tool_calls'].mean():.1f}\")\n",
        "    print(f\"Incorrect answers avg tool calls: {df[df['exact_match_reward'] == 0]['n_tool_calls'].mean():.1f}\")\n",
        "\n",
        "    # Examples with no predicted answer\n",
        "    no_answer = df[df[\"predicted_answer\"] == \"\"]\n",
        "    print(f\"\\nExamples with no predicted answer: {len(no_answer)} ({len(no_answer) / len(df) * 100:.1f}%)\")\n",
        "\n",
        "    # Identify low-performing areas\n",
        "    print(\"\\n=== LOW-PERFORMING AREAS ===\")\n",
        "    if \"retrieval_recall_reward\" in df.columns:\n",
        "        low_retrieval = df[df[\"retrieval_recall_reward\"] < 0.5]\n",
        "        print(f\"Low retrieval recall (<0.5): {len(low_retrieval)} examples ({len(low_retrieval) / len(df) * 100:.1f}%)\")\n",
        "\n",
        "    if \"citation_reward\" in df.columns:\n",
        "        low_citation = df[df[\"citation_reward\"] < 0.5]\n",
        "        print(f\"Low citation quality (<0.5): {len(low_citation)} examples ({len(low_citation) / len(df) * 100:.1f}%)\")\n",
        "\n",
        "    if \"format_reward\" in df.columns:\n",
        "        low_format = df[df[\"format_reward\"] < 0.5]\n",
        "        print(f\"Low format compliance (<0.5): {len(low_format)} examples ({len(low_format) / len(df) * 100:.1f}%)\")\n",
        "\n",
        "    return incorrect\n",
        "\n",
        "\n",
        "error_df_enhanced = analyze_errors_enhanced(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of new features for the enhanced dataset\n",
        "\n",
        "print(\"=== ENHANCED EVALUATION RESULTS INSPECTOR ===\")\n",
        "print(\"This notebook now supports the new dataset structure with detailed reward metrics!\")\n",
        "print()\n",
        "print(\"NEW FEATURES:\")\n",
        "print(\"â¢ Detailed reward breakdown (exact_match_reward, f1_reward, retrieval_recall_reward, etc.)\")\n",
        "print(\"â¢ Enhanced visualizations showing reward correlations and distributions\")\n",
        "print(\"â¢ Interactive browser with reward-based filtering\")\n",
        "print(\"â¢ Improved error analysis comparing reward components\")\n",
        "print(\"â¢ Backward compatibility with legacy datasets\")\n",
        "print()\n",
        "print(\"AVAILABLE REWARD METRICS:\")\n",
        "reward_cols = [\"combined_reward\", \"exact_match_reward\", \"f1_reward\", \"retrieval_recall_reward\", \"citation_reward\", \"format_reward\"]\n",
        "for col in reward_cols:\n",
        "    if col in df.columns:\n",
        "        print(f\"  â {col}\")\n",
        "    else:\n",
        "        print(f\"  â {col} (not found in dataset)\")\n",
        "\n",
        "print()\n",
        "print(\"To explore your data:\")\n",
        "print(\"1. Check the reward breakdown above\")\n",
        "print(\"2. Run the enhanced visualizations\")\n",
        "print(\"3. Use the interactive browser with reward filtering\")\n",
        "print(\"4. Analyze error patterns by reward components\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_errors(df):\n",
        "    \"\"\"Analyze common error patterns\"\"\"\n",
        "    incorrect = df[df['exact_match_reward'] == 0]\n",
        "    \n",
        "    print(f\"=== ERROR ANALYSIS ({len(incorrect)} incorrect examples) ===\")\n",
        "    \n",
        "    # By number of hops\n",
        "    print(\"\\nError rate by hops:\")\n",
        "    hop_errors = df.groupby('n_hops').agg({\n",
        "        'exact_match_reward': lambda x: 1 - x.mean()\n",
        "    }).round(3)\n",
        "    print(hop_errors)\n",
        "    \n",
        "    # Supporting document usage\n",
        "    print(\"\\nSupporting document usage comparison:\")\n",
        "    print(f\"Correct answers: {df[df['exact_match_reward']==1]['used_supporting_docs'].mean():.3f}\")\n",
        "    print(f\"Incorrect answers: {df[df['exact_match_reward']==0]['used_supporting_docs'].mean():.3f}\")\n",
        "    \n",
        "    # Tool call patterns\n",
        "    print(\"\\nTool call patterns:\")\n",
        "    print(f\"Correct answers avg tool calls: {df[df['exact_match_reward']==1]['n_tool_calls'].mean():.1f}\")\n",
        "    print(f\"Incorrect answers avg tool calls: {df[df['exact_match_reward']==0]['n_tool_calls'].mean():.1f}\")\n",
        "    \n",
        "    # Examples with no predicted answer\n",
        "    no_answer = df[df['predicted_answer'] == '']\n",
        "    print(f\"\\nExamples with no predicted answer: {len(no_answer)} ({len(no_answer)/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    return incorrect\n",
        "\n",
        "error_df = analyze_errors(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add your custom analysis here\n",
        "# For example, analyze specific question types, patterns in tool usage, etc.\n",
        "\n",
        "# Example: Look at questions that require exactly 2 hops\n",
        "two_hop_questions = df[df['n_hops'] == 2]\n",
        "print(f\"2-hop questions performance: {two_hop_questions['exact_match_reward'].mean():.3f}\")\n",
        "\n",
        "# Example: Correlation between tool calls and performance\n",
        "correlation = df['n_tool_calls'].corr(df['exact_match_reward'])\n",
        "print(f\"Correlation between tool calls and exact match: {correlation:.3f}\")\n",
        "\n",
        "# Example: Most common failure modes\n",
        "print(\"\\nMost common failure cases (first 5):\")\n",
        "failed_examples = df[df['exact_match_reward'] == 0].head()\n",
        "for idx, row in failed_examples.iterrows():\n",
        "    print(f\"- Question: {row['question'][:100]}...\")\n",
        "    print(f\"  Predicted: '{row['predicted_answer']}'\")\n",
        "    print(f\"  Expected: {row['info']['answers']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
