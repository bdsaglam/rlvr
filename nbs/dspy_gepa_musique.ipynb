{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3483a25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bae087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import dspy\n",
    "from vf_musique.data import prepare_dataset\n",
    "from vf_musique.metrics import exact_match, f1\n",
    "from vf_musique.rewards import extract_all_retrieved_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2de384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MLflow tracking enabled at http://localhost:5005\n"
     ]
    }
   ],
   "source": [
    "def setup_mlflow():\n",
    "    import mlflow\n",
    "    import mlflow.dspy\n",
    "\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "    mlflow.set_experiment(\"dspy-gepa-musique\")\n",
    "    mlflow.dspy.autolog(\n",
    "        log_compiles=True,\n",
    "        log_evals=True,\n",
    "        log_traces_from_compile=True,\n",
    "    )\n",
    "    print(f\"âœ… MLflow tracking enabled at {os.getenv('MLFLOW_TRACKING_URI')}\")\n",
    "\n",
    "setup_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c131983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../outputs/dspy/gepa-musique/20250930_171202')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_ID = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = Path(f\"../outputs/dspy/gepa-musique/{EXP_ID}\")\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2015c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/Qwen/Qwen3-8B\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=8192,\n",
    "    api_key=\"local\",\n",
    "    api_base=\"http://0.0.0.0:8000/v1\",\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# reflection_lm = dspy.LM(\"gemini/gemini-2.5-pro\", api_key=os.getenv(\"GEMINI_API_KEY\"), max_tokens=16384,)\n",
    "reflection_lm = dspy.LM(\n",
    "    \"openai/Qwen/Qwen3-32B\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=16384,\n",
    "    api_key=\"local\",\n",
    "    api_base=\"http://0.0.0.0:8001/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c161bc-d491-4e23-8c5a-7723d9f8296e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nHello! ðŸ˜Š How can I assist you today?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-c9e876584d7a4ecc176a0b8e234963ff&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-c9e876584d7a4ecc176a0b8e234963ff)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b197cd14-2b0a-494c-948f-51317b3ba963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nThe largest prime number below 10 is **7**. \\n\\nTo determine this, we first identify all prime numbers less than 10. A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself. Evaluating the numbers from 2 to 9:\\n\\n- **2** is prime (divisible only by 1 and 2).\\n- **3** is prime (divisible only by 1 and 3).\\n- **4** is not prime (divisible by 2).\\n- **5** is prime (divisible only by 1 and 5).\\n- **6** is not prime (divisible by 2 and 3).\\n- **7** is prime (divisible only by 1 and 7).\\n- **8** is not prime (divisible by 2 and 4).\\n- **9** is not prime (divisible by 3).\\n\\nAmong the prime numbers 2, 3, 5, and 7, the largest is **7**.\\n\\n---\\n\\n$$\\n\\\\boxed{7}\\n$$']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-1bdedb47e72e097d70d93bb34d8904b7&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-1bdedb47e72e097d70d93bb34d8904b7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reflection_lm(messages=[{\"role\": \"user\", \"content\": \"What is largest prime number below 10?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8140bb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad2f7257c194df3b3ca9765843e9008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 300/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3ad4d7689e4fd18bad9fd464d9986a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712fc5fa4c134416959b6bb613f003b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 50/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db67156e51fe436e8f33b4434c63af1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def prepare_musique_dataset(datasets_str: str = \"bdsaglam/musique,answerable,train\", noise_rate: float = 1.0):\n",
    "    \"\"\"Load and prepare MuSiQue dataset using vf_musique data functions.\"\"\"\n",
    "    # Use the official vf_musique data preparation\n",
    "    dataset = prepare_dataset(datasets_str, noise_rate=noise_rate)\n",
    "\n",
    "    # Convert to DSPy examples\n",
    "    processed_examples = []\n",
    "    for x in dataset:\n",
    "        # Get supporting document IDs\n",
    "        supporting_doc_ids = [doc[\"id\"] for doc in x[\"info\"][\"docs\"] if doc.get(\"is_supporting\")]\n",
    "\n",
    "        # Create DSPy example\n",
    "        example = dspy.Example(\n",
    "            question=x[\"question\"],\n",
    "            answer=x[\"answer\"],\n",
    "            answers=x[\"info\"][\"answers\"],  # All valid answer forms\n",
    "            docs=x[\"info\"][\"docs\"],  # All documents\n",
    "            supporting_ids=supporting_doc_ids,  # IDs of supporting docs\n",
    "            n_hops=x[\"info\"][\"n_hops\"],  # Number of hops\n",
    "        ).with_inputs(\"question\", \"docs\")\n",
    "\n",
    "        processed_examples.append(example)\n",
    "\n",
    "    return processed_examples\n",
    "\n",
    "ds = prepare_musique_dataset(datasets_str=\"bdsaglam/musique-mini,answerable,train\", noise_rate=1.0)\n",
    "random.Random(89).shuffle(ds)\n",
    "train_size = int(len(ds)*0.60)\n",
    "train_ds, val_ds = ds[:train_size], ds[train_size:]\n",
    "test_ds = prepare_musique_dataset(datasets_str=\"bdsaglam/musique-mini,answerable,validation[:50]\", noise_rate=1.0)\n",
    "\n",
    "train_ds = train_ds[:30]\n",
    "val_ds = val_ds[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e75064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import RunContextWrapper\n",
    "from vf_musique.tools import make_retrieve_tool, ToolContext\n",
    "\n",
    "\n",
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Given a multi-hop question and information collected so far, generate a search query\n",
    "    to find the next piece of information needed to answer the question.\n",
    "    Focus on entities, dates, or facts that need to be resolved step by step.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    collected_info: str = dspy.InputField(desc=\"Information collected from previous retrieval steps\")\n",
    "    search_query: str = dspy.OutputField(desc=\"Search query for the next retrieval step\")\n",
    "    top_n: int = dspy.OutputField(desc=\"Number of documents to retrieve. 1 <= top_n <= 3\")\n",
    "\n",
    "class KeyInformation(BaseModel):\n",
    "    info: str \n",
    "    source_doc_id: str\n",
    "\n",
    "    def format(self):\n",
    "        return f\"{self.info}[{self.source_doc_id}]\"\n",
    "\n",
    "class ExtractInformation(dspy.Signature):\n",
    "    \"\"\"Given a question and retrieved documents, extract the key information\n",
    "    that helps answer the question or leads to the next retrieval step.\n",
    "    Focus on entities, relationships, dates, and facts.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    documents: str = dspy.InputField(desc=\"Retrieved documents from search\")\n",
    "    key_informations: list[KeyInformation] = dspy.OutputField(desc=\"Key information(s) extracted from retrieved document(s)\")\n",
    "\n",
    "class DecideInfoCollection(dspy.Signature):\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    all_information: str = dspy.InputField(desc=\"All information collected during retrieval\")\n",
    "    has_collected_enough_info: bool = dspy.OutputField(desc=\"Has enough information been collected to answer question?\")\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Given a multi-hop question and all collected information, provide a concise answer.\n",
    "    The answer should directly address what the question asks for.\n",
    "    Be specific and use the exact entities/dates/facts from the documents.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    all_information: str = dspy.InputField(desc=\"All information collected during retrieval\")\n",
    "    answer: str = dspy.OutputField(desc=\"Final answer to the question\")\n",
    "    citations: list[str] = dspy.OutputField(desc=\"List of document IDs cited for the answer, e.g. `[4,9]`\")\n",
    "\n",
    "\n",
    "class MultiHopQA(dspy.Module):\n",
    "    \"\"\"Multi-hop question answering module for MuSiQue.\"\"\"\n",
    "\n",
    "    def __init__(self, retriever_name: str = \"hybrid\", max_iter: int = 10):\n",
    "        self.retriever_name = retriever_name\n",
    "        self.max_iter = 10\n",
    "\n",
    "        # Create the retrieve tool\n",
    "        self.retrieve_tool = make_retrieve_tool(retriever_name, default_top_n=2)\n",
    "\n",
    "        # Create modules with typed signatures\n",
    "        self.generate_query = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.extract_info = dspy.ChainOfThought(ExtractInformation)\n",
    "        self.decide_info_collect = dspy.ChainOfThought(DecideInfoCollection)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question: str, docs: list, **kwargs) -> dspy.Prediction:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-hop QA.\n",
    "\n",
    "        Args:\n",
    "            question: The multi-hop question to answer\n",
    "            docs: List of documents available for retrieval\n",
    "        \"\"\"\n",
    "        collected_info = []\n",
    "        retrieved_doc_ids = []\n",
    "\n",
    "        # Create a context object that mimics the verifiers tool environment\n",
    "        run_context_wrapper = RunContextWrapper[ToolContext](context=ToolContext(info=dict(docs=docs)))\n",
    "\n",
    "        for hop_idx in range(self.max_iter):\n",
    "            # Generate search query\n",
    "            if hop_idx == 0:\n",
    "                # First hop: use the original question\n",
    "                query = question\n",
    "                top_n = 2\n",
    "            else:\n",
    "                # Subsequent hops: generate query based on collected info\n",
    "                query_pred = self.generate_query(\n",
    "                    question=question,\n",
    "                    collected_info=\"\\n\".join([item.format() for item in collected_info]) if collected_info else \"No information collected yet\",\n",
    "                )\n",
    "                query = query_pred.search_query\n",
    "                top_n = max(min(query_pred.top_n, 3), 1)\n",
    "\n",
    "            # Retrieve documents using the MuSiQue retrieve tool\n",
    "            retrieved_text = self.retrieve_tool(run_context_wrapper, query=query, top_n=top_n)\n",
    "\n",
    "            # Extract document IDs from retrieved text using the official function\n",
    "            doc_ids = extract_all_retrieved_doc_ids(retrieved_text)\n",
    "            for doc_id in doc_ids:\n",
    "                if doc_id not in retrieved_doc_ids:\n",
    "                    retrieved_doc_ids.append(doc_id)\n",
    "\n",
    "            # Extract key information from retrieved documents\n",
    "            info_pred = self.extract_info(question=question, documents=retrieved_text)\n",
    "            collected_info.extend(info_pred.key_informations)\n",
    "\n",
    "            decision_pred = self.decide_info_collect(question=question, all_information=\"\\n\".join([item.format() for item in collected_info]))        \n",
    "            if decision_pred.has_collected_enough_info:\n",
    "                break\n",
    "\n",
    "        # Generate final answer based on all collected information\n",
    "        answer_pred: GenerateAnswer = self.generate_answer(question=question, all_information=\"\\n\".join([item.format() for item in collected_info]))\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            answer=answer_pred.answer,\n",
    "            collected_info=collected_info,\n",
    "            retrieved_doc_ids=retrieved_doc_ids,\n",
    "            citations=answer_pred.citations,\n",
    "        )\n",
    "\n",
    "program = MultiHopQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502bfc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'The mosaic in the church in the city where Maria Tsiartsiani was born, is known as what?', 'answer': \"Christ in majesty (or Ezekiel's Vision)\", 'answers': [\"Christ in majesty (or Ezekiel's Vision)\", \"christ in majesty (or ezekiel's vision)\"], 'docs': [{'body': 'In the Iconoclastic era, figural mosaics were also condemned as idolatry. The Iconoclastic churches were embellished with plain gold mosaics with only one great cross in the apse like the Hagia Irene in Constantinople (after 740). There were similar crosses in the apses of the Hagia Sophia Church in Thessaloniki and in the Church of the Dormition in Nicaea. The crosses were substituted with the image of the Theotokos in both churches after the victory of the Iconodules (787â€“797 and in 8thâ€“9th centuries respectively, the Dormition church was totally destroyed in 1922).', 'id': '0', 'is_supporting': False, 'text': '# Mosaic\\nIn the Iconoclastic era, figural mosaics were also condemned as idolatry. The Iconoclastic churches were embellished with plain gold mosaics with only one great cross in the apse like the Hagia Irene in Constantinople (after 740). There were similar crosses in the apses of the Hagia Sophia Church in Thessaloniki and in the Church of the Dormition in Nicaea. The crosses were substituted with the image of the Theotokos in both churches after the victory of the Iconodules (787â€“797 and in 8thâ€“9th centuries respectively, the Dormition church was totally destroyed in 1922).', 'title': 'Mosaic'}, {'body': 'Maria Tsiartsiani (; born 21 October 1980 in Thessaloniki) is a Greek Olympic beach volleyballer. She competed at the 2008 Summer Olympics, partnering with Efthalia Koutroumanidou. At the 2012 Summer Olympics, she competed with Vassiliki Arvaniti.', 'id': '1', 'is_supporting': True, 'text': '# Maria Tsiartsiani\\nMaria Tsiartsiani (; born 21 October 1980 in Thessaloniki) is a Greek Olympic beach volleyballer. She competed at the 2008 Summer Olympics, partnering with Efthalia Koutroumanidou. At the 2012 Summer Olympics, she competed with Vassiliki Arvaniti.', 'title': 'Maria Tsiartsiani'}, {'body': 'The greatest mosaic work of the Palaeologan renaissance in art is the decoration of the Chora Church in Constantinople. Although the mosaics of the naos have not survived except three panels, the decoration of the exonarthex and the esonarthex constitute the most important full-scale mosaic cycle in Constantinople after the Hagia Sophia. They were executed around 1320 by the command of Theodore Metochites. The esonarthex has two fluted domes, specially created to provide the ideal setting for the mosaic images of the ancestors of Christ. The southern one is called the Dome of the Pantokrator while the northern one is the Dome of the Theotokos. The most important panel of the esonarthex depicts Theodore Metochites wearing a huge turban, offering the model of the church to Christ. The walls of both narthexes are decorated with mosaic cycles from the life of the Virgin and the life of Christ. These panels show the influence of the Italian trecento on Byzantine art especially the more natural settings, landscapes, figures.', 'id': '2', 'is_supporting': False, 'text': '# Mosaic\\nThe greatest mosaic work of the Palaeologan renaissance in art is the decoration of the Chora Church in Constantinople. Although the mosaics of the naos have not survived except three panels, the decoration of the exonarthex and the esonarthex constitute the most important full-scale mosaic cycle in Constantinople after the Hagia Sophia. They were executed around 1320 by the command of Theodore Metochites. The esonarthex has two fluted domes, specially created to provide the ideal setting for the mosaic images of the ancestors of Christ. The southern one is called the Dome of the Pantokrator while the northern one is the Dome of the Theotokos. The most important panel of the esonarthex depicts Theodore Metochites wearing a huge turban, offering the model of the church to Christ. The walls of both narthexes are decorated with mosaic cycles from the life of the Virgin and the life of Christ. These panels show the influence of the Italian trecento on Byzantine art especially the more natural settings, landscapes, figures.', 'title': 'Mosaic'}, {'body': 'The last great period of Roman mosaic art was the 12thâ€“13th century when Rome developed its own distinctive artistic style, free from the strict rules of eastern tradition and with a more realistic portrayal of figures in the space. Well-known works of this period are the floral mosaics of the Basilica di San Clemente, the faÃ§ade of Santa Maria in Trastevere and San Paolo fuori le Mura. The beautiful apse mosaic of Santa Maria in Trastevere (1140) depicts Christ and Mary sitting next to each other on the heavenly throne, the first example of this iconographic scheme. A similar mosaic, the Coronation of the Virgin, decorates the apse of Santa Maria Maggiore. It is a work of Jacopo Torriti from 1295. The mosaics of Torriti and Jacopo da Camerino in the apse of San Giovanni in Laterano from 1288â€“94 were thoroughly restored in 1884. The apse mosaic of San Crisogono is attributed to Pietro Cavallini, the greatest Roman painter of the 13th century. Six scenes from the life of Mary in Santa Maria in Trastevere were also executed by Cavallini in 1290. These mosaics are praised for their realistic portrayal and attempts of perspective. There is an interesting mosaic medaillon from 1210 above the gate of the church of San Tommaso in Formis showing Christ enthroned between a white and a black slave. The church belonged to the Order of the Trinitarians which was devoted to ransoming Christian slaves.', 'id': '3', 'is_supporting': False, 'text': '# Mosaic\\nThe last great period of Roman mosaic art was the 12thâ€“13th century when Rome developed its own distinctive artistic style, free from the strict rules of eastern tradition and with a more realistic portrayal of figures in the space. Well-known works of this period are the floral mosaics of the Basilica di San Clemente, the faÃ§ade of Santa Maria in Trastevere and San Paolo fuori le Mura. The beautiful apse mosaic of Santa Maria in Trastevere (1140) depicts Christ and Mary sitting next to each other on the heavenly throne, the first example of this iconographic scheme. A similar mosaic, the Coronation of the Virgin, decorates the apse of Santa Maria Maggiore. It is a work of Jacopo Torriti from 1295. The mosaics of Torriti and Jacopo da Camerino in the apse of San Giovanni in Laterano from 1288â€“94 were thoroughly restored in 1884. The apse mosaic of San Crisogono is attributed to Pietro Cavallini, the greatest Roman painter of the 13th century. Six scenes from the life of Mary in Santa Maria in Trastevere were also executed by Cavallini in 1290. These mosaics are praised for their realistic portrayal and attempts of perspective. There is an interesting mosaic medaillon from 1210 above the gate of the church of San Tommaso in Formis showing Christ enthroned between a white and a black slave. The church belonged to the Order of the Trinitarians which was devoted to ransoming Christian slaves.', 'title': 'Mosaic'}, {'body': 'An exceptionally well preserved, carpet-like mosaic floor was uncovered in 1949 in Bethany, the early Byzantine church of the Lazarium which was built between 333 and 390. Because of its purely geometrical pattern, the church floor is to be grouped with other mosaics of the time in Palestine and neighboring areas, especially the Constantinian mosaics in the central nave at Bethlehem. A second church was built above the older one during the 6th century with another more simple geometric mosaic floor.', 'id': '4', 'is_supporting': False, 'text': '# Mosaic\\nAn exceptionally well preserved, carpet-like mosaic floor was uncovered in 1949 in Bethany, the early Byzantine church of the Lazarium which was built between 333 and 390. Because of its purely geometrical pattern, the church floor is to be grouped with other mosaics of the time in Palestine and neighboring areas, especially the Constantinian mosaics in the central nave at Bethlehem. A second church was built above the older one during the 6th century with another more simple geometric mosaic floor.', 'title': 'Mosaic'}, {'body': \"Very few early Byzantine mosaics survived the Iconoclastic destruction of the 8th century. Among the rare examples are the 6th-century Christ in majesty (or Ezekiel's Vision) mosaic in the apse of the Church of Hosios David in Thessaloniki that was hidden behind mortar during those dangerous times. Nine mosaic panels in the Hagios Demetrios Church, which were made between 634 and 730, also escaped destruction. Unusually almost all represent Saint Demetrius of Thessaloniki, often with suppliants before him.\", 'id': '5', 'is_supporting': True, 'text': \"# Mosaic\\nVery few early Byzantine mosaics survived the Iconoclastic destruction of the 8th century. Among the rare examples are the 6th-century Christ in majesty (or Ezekiel's Vision) mosaic in the apse of the Church of Hosios David in Thessaloniki that was hidden behind mortar during those dangerous times. Nine mosaic panels in the Hagios Demetrios Church, which were made between 634 and 730, also escaped destruction. Unusually almost all represent Saint Demetrius of Thessaloniki, often with suppliants before him.\", 'title': 'Mosaic'}, {'body': 'Other important Venetian mosaics can be found in the Cathedral of Santa Maria Assunta in Torcello from the 12th century, and in the Basilical of Santi Maria e Donato in Murano with a restored apse mosaic from the 12th century and a beautiful mosaic pavement (1140). The apse of the San Cipriano Church in Murano was decorated with an impressive golden mosaic from the early 13th century showing Christ enthroned with Mary, St John and the two patron saints, Cipriano and Cipriana. When the church was demolished in the 19th century, the mosaic was bought by Frederick William IV of Prussia. It was reassembled in the Friedenskirche of Potsdam in the 1840s.', 'id': '6', 'is_supporting': False, 'text': '# Mosaic\\nOther important Venetian mosaics can be found in the Cathedral of Santa Maria Assunta in Torcello from the 12th century, and in the Basilical of Santi Maria e Donato in Murano with a restored apse mosaic from the 12th century and a beautiful mosaic pavement (1140). The apse of the San Cipriano Church in Murano was decorated with an impressive golden mosaic from the early 13th century showing Christ enthroned with Mary, St John and the two patron saints, Cipriano and Cipriana. When the church was demolished in the 19th century, the mosaic was bought by Frederick William IV of Prussia. It was reassembled in the Friedenskirche of Potsdam in the 1840s.', 'title': 'Mosaic'}, {'body': 'Jerusalem with its many holy places probably had the highest concentration of mosaic-covered churches but very few of them survived the subsequent waves of destructions. The present remains do not do justice to the original richness of the city. The most important is the so-called \"Armenian Mosaic\" which was discovered in 1894 on the Street of the Prophets near Damascus Gate. It depicts a vine with many branches and grape clusters, which springs from a vase. Populating the vine\\'s branches are peacocks, ducks, storks, pigeons, an eagle, a partridge, and a parrot in a cage. The inscription reads: \"For the memory and salvation of all those Armenians whose name the Lord knows.\" Beneath a corner of the mosaic is a small, natural cave which contained human bones dating to the 5th or 6th centuries. The symbolism of the mosaic and the presence of the burial cave indicates that the room was used as a mortuary chapel.', 'id': '7', 'is_supporting': False, 'text': '# Mosaic\\nJerusalem with its many holy places probably had the highest concentration of mosaic-covered churches but very few of them survived the subsequent waves of destructions. The present remains do not do justice to the original richness of the city. The most important is the so-called \"Armenian Mosaic\" which was discovered in 1894 on the Street of the Prophets near Damascus Gate. It depicts a vine with many branches and grape clusters, which springs from a vase. Populating the vine\\'s branches are peacocks, ducks, storks, pigeons, an eagle, a partridge, and a parrot in a cage. The inscription reads: \"For the memory and salvation of all those Armenians whose name the Lord knows.\" Beneath a corner of the mosaic is a small, natural cave which contained human bones dating to the 5th or 6th centuries. The symbolism of the mosaic and the presence of the burial cave indicates that the room was used as a mortuary chapel.', 'title': 'Mosaic'}, {'body': 'Another great undertaking by Constantine Monomachos was the restoration of the Church of the Holy Sepulchre in Jerusalem between 1042 and 1048. Nothing survived of the mosaics which covered the walls and the dome of the edifice but the Russian abbot Daniel, who visited Jerusalem in 1106â€“1107 left a description: \"Lively mosaics of the holy prophets are under the ceiling, over the tribune. The altar is surmounted by a mosaic image of Christ. In the main altar one can see the mosaic of the Exhaltation of Adam. In the apse the Ascension of Christ. The Annunciation occupies the two pillars next to the altar.\"', 'id': '8', 'is_supporting': False, 'text': '# Mosaic\\nAnother great undertaking by Constantine Monomachos was the restoration of the Church of the Holy Sepulchre in Jerusalem between 1042 and 1048. Nothing survived of the mosaics which covered the walls and the dome of the edifice but the Russian abbot Daniel, who visited Jerusalem in 1106â€“1107 left a description: \"Lively mosaics of the holy prophets are under the ceiling, over the tribune. The altar is surmounted by a mosaic image of Christ. In the main altar one can see the mosaic of the Exhaltation of Adam. In the apse the Ascension of Christ. The Annunciation occupies the two pillars next to the altar.\"', 'title': 'Mosaic'}, {'body': 'The monastic communities of the Judean Desert also decorated their monasteries with mosaic floors. The Monastery of Martyrius was founded in the end of the 5th century and it was re-discovered in 1982â€“85. The most important work of art here is the intact geometric mosaic floor of the refectory although the severely damaged church floor was similarly rich. The mosaics in the church of the nearby Monastery of Euthymius are of later date (discovered in 1930). They were laid down in the Umayyad era, after a devastating earthquake in 659. Two six pointed stars and a red chalice are the most important surviving features.', 'id': '9', 'is_supporting': False, 'text': '# Mosaic\\nThe monastic communities of the Judean Desert also decorated their monasteries with mosaic floors. The Monastery of Martyrius was founded in the end of the 5th century and it was re-discovered in 1982â€“85. The most important work of art here is the intact geometric mosaic floor of the refectory although the severely damaged church floor was similarly rich. The mosaics in the church of the nearby Monastery of Euthymius are of later date (discovered in 1930). They were laid down in the Umayyad era, after a devastating earthquake in 659. Two six pointed stars and a red chalice are the most important surviving features.', 'title': 'Mosaic'}, {'body': 'Oleg Bogayev was born in 1970 in the city of Sverdlovsk (now called Yekaterinburg) in Russia. He writes of growing up as the Cold War gave way to the emergence of Perestroika, a \"change from the decay of the empire to the birth of a new society.\" He cites the social turmoil of recent decades as useful for artistic product: \"[What] I know is that Russia is just the right place for a playwright - with shattering of fates, conflicts, crumbling of hopes, clashes of ideas - all that I\\'ve seen and experienced.\"', 'id': '10', 'is_supporting': False, 'text': '# Oleg Bogayev\\nOleg Bogayev was born in 1970 in the city of Sverdlovsk (now called Yekaterinburg) in Russia. He writes of growing up as the Cold War gave way to the emergence of Perestroika, a \"change from the decay of the empire to the birth of a new society.\" He cites the social turmoil of recent decades as useful for artistic product: \"[What] I know is that Russia is just the right place for a playwright - with shattering of fates, conflicts, crumbling of hopes, clashes of ideas - all that I\\'ve seen and experienced.\"', 'title': 'Oleg Bogayev'}, {'body': \"Christian mosaic art also flourished in Rome, gradually declining as conditions became more difficult in the Early Middle Ages. 5th century mosaics can be found over the triumphal arch and in the nave of the basilica of Santa Maria Maggiore. The 27 surviving panels of the nave are the most important mosaic cycle in Rome of this period. Two other important 5th century mosaics are lost but we know them from 17th-century drawings. In the apse mosaic of Sant'Agata dei Goti (462â€“472, destroyed in 1589) Christ was seated on a globe with the twelve Apostles flanking him, six on either side. At Sant'Andrea in Catabarbara (468â€“483, destroyed in 1686) Christ appeared in the center, flanked on either side by three Apostles. Four streams flowed from the little mountain supporting Christ. The original 5th-century apse mosaic of the Santa Sabina was replaced by a very similar fresco by Taddeo Zuccari in 1559. The composition probably remained unchanged: Christ flanked by male and female saints, seated on a hill while lambs drinking from a stream at its feet. All three mosaics had a similar iconography.\", 'id': '11', 'is_supporting': False, 'text': \"# Mosaic\\nChristian mosaic art also flourished in Rome, gradually declining as conditions became more difficult in the Early Middle Ages. 5th century mosaics can be found over the triumphal arch and in the nave of the basilica of Santa Maria Maggiore. The 27 surviving panels of the nave are the most important mosaic cycle in Rome of this period. Two other important 5th century mosaics are lost but we know them from 17th-century drawings. In the apse mosaic of Sant'Agata dei Goti (462â€“472, destroyed in 1589) Christ was seated on a globe with the twelve Apostles flanking him, six on either side. At Sant'Andrea in Catabarbara (468â€“483, destroyed in 1686) Christ appeared in the center, flanked on either side by three Apostles. Four streams flowed from the little mountain supporting Christ. The original 5th-century apse mosaic of the Santa Sabina was replaced by a very similar fresco by Taddeo Zuccari in 1559. The composition probably remained unchanged: Christ flanked by male and female saints, seated on a hill while lambs drinking from a stream at its feet. All three mosaics had a similar iconography.\", 'title': 'Mosaic'}, {'body': 'The Church of the Holy Apostles in Thessaloniki was built in 1310â€“14. Although some vandal systematically removed the gold tesserae of the background it can be seen that the Pantokrator and the prophets in the dome follow the traditional Byzantine pattern. Many details are similar to the Pammakaristos mosaics so it is supposed that the same team of mosaicists worked in both buildings. Another building with a related mosaic decoration is the Theotokos Paregoritissa Church in Arta. The church was established by the Despot of Epirus in 1294â€“96. In the dome is the traditional stern Pantokrator, with prophets and cherubim below.', 'id': '12', 'is_supporting': False, 'text': '# Mosaic\\nThe Church of the Holy Apostles in Thessaloniki was built in 1310â€“14. Although some vandal systematically removed the gold tesserae of the background it can be seen that the Pantokrator and the prophets in the dome follow the traditional Byzantine pattern. Many details are similar to the Pammakaristos mosaics so it is supposed that the same team of mosaicists worked in both buildings. Another building with a related mosaic decoration is the Theotokos Paregoritissa Church in Arta. The church was established by the Despot of Epirus in 1294â€“96. In the dome is the traditional stern Pantokrator, with prophets and cherubim below.', 'title': 'Mosaic'}, {'body': 'In Rome, Nero and his architects used mosaics to cover some surfaces of walls and ceilings in the Domus Aurea, built 64 AD, and wall mosaics are also found at Pompeii and neighbouring sites. However it seems that it was not until the Christian era that figural wall mosaics became a major form of artistic expression. The Roman church of Santa Costanza, which served as a mausoleum for one or more of the Imperial family, has both religious mosaic and decorative secular ceiling mosaics on a round vault, which probably represent the style of contemporary palace decoration.', 'id': '13', 'is_supporting': False, 'text': '# Mosaic\\nIn Rome, Nero and his architects used mosaics to cover some surfaces of walls and ceilings in the Domus Aurea, built 64 AD, and wall mosaics are also found at Pompeii and neighbouring sites. However it seems that it was not until the Christian era that figural wall mosaics became a major form of artistic expression. The Roman church of Santa Costanza, which served as a mausoleum for one or more of the Imperial family, has both religious mosaic and decorative secular ceiling mosaics on a round vault, which probably represent the style of contemporary palace decoration.', 'title': 'Mosaic'}, {'body': 'WacÅ‚aw SierpiÅ„ski described the Sierpinski triangle in 1915. However, similar patterns appear already in the 13th-century Cosmati mosaics in the cathedral of Anagni, Italy, and other places of central Italy, for carpets in many places such as the nave of the Roman Basilica of Santa Maria in Cosmedin, and for isolated triangles positioned in rotae in several churches and basilicas. In the case of the isolated triangle, the iteration is at least of three levels.', 'id': '14', 'is_supporting': False, 'text': '# SierpiÅ„ski triangle\\nWacÅ‚aw SierpiÅ„ski described the Sierpinski triangle in 1915. However, similar patterns appear already in the 13th-century Cosmati mosaics in the cathedral of Anagni, Italy, and other places of central Italy, for carpets in many places such as the nave of the Roman Basilica of Santa Maria in Cosmedin, and for isolated triangles positioned in rotae in several churches and basilicas. In the case of the isolated triangle, the iteration is at least of three levels.', 'title': 'SierpiÅ„ski triangle'}, {'body': 'The heyday of mosaic making in Sicily was the age of the independent Norman kingdom in the 12th century. The Norman kings adopted the Byzantine tradition of mosaic decoration to enhance the somewhat dubious legality of their rule. Greek masters working in Sicily developed their own style, that shows the influence of Western European and Islamic artistic tendencies. Best examples of Sicilian mosaic art are the Cappella Palatina of Roger II, the Martorana church in Palermo and the cathedrals of CefalÃ¹ and Monreale.', 'id': '15', 'is_supporting': False, 'text': '# Mosaic\\nThe heyday of mosaic making in Sicily was the age of the independent Norman kingdom in the 12th century. The Norman kings adopted the Byzantine tradition of mosaic decoration to enhance the somewhat dubious legality of their rule. Greek masters working in Sicily developed their own style, that shows the influence of Western European and Islamic artistic tendencies. Best examples of Sicilian mosaic art are the Cappella Palatina of Roger II, the Martorana church in Palermo and the cathedrals of CefalÃ¹ and Monreale.', 'title': 'Mosaic'}, {'body': \"In the 7thâ€“9th centuries Rome fell under the influence of Byzantine art, noticeable on the mosaics of Santa Prassede, Santa Maria in Domnica, Sant'Agnese fuori le Mura, Santa Cecilia in Trastevere, Santi Nereo e Achilleo and the San Venanzio chapel of San Giovanni in Laterano. The great dining hall of Pope Leo III in the Lateran Palace was also decorated with mosaics. They were all destroyed later except for one example, the so-called Triclinio Leoniano of which a copy was made in the 18th century. Another great work of Pope Leo, the apse mosaic of Santa Susanna, depicted Christ with the Pope and Charlemagne on one side, and SS. Susanna and Felicity on the other. It was plastered over during a renovation in 1585. Pope Paschal I (817â€“824) embellished the church of Santo Stefano del Cacco with an apsidal mosaic which depicted the pope with a model of the church (destroyed in 1607).\", 'id': '16', 'is_supporting': False, 'text': \"# Mosaic\\nIn the 7thâ€“9th centuries Rome fell under the influence of Byzantine art, noticeable on the mosaics of Santa Prassede, Santa Maria in Domnica, Sant'Agnese fuori le Mura, Santa Cecilia in Trastevere, Santi Nereo e Achilleo and the San Venanzio chapel of San Giovanni in Laterano. The great dining hall of Pope Leo III in the Lateran Palace was also decorated with mosaics. They were all destroyed later except for one example, the so-called Triclinio Leoniano of which a copy was made in the 18th century. Another great work of Pope Leo, the apse mosaic of Santa Susanna, depicted Christ with the Pope and Charlemagne on one side, and SS. Susanna and Felicity on the other. It was plastered over during a renovation in 1585. Pope Paschal I (817â€“824) embellished the church of Santo Stefano del Cacco with an apsidal mosaic which depicted the pope with a model of the church (destroyed in 1607).\", 'title': 'Mosaic'}, {'body': 'The apse mosaic of the Gelati Monastery is a rare example of mosaic use in Georgia. Began by king David IV and completed by his son Demetrius I of Georgia, the fragmentary panel depicts Theotokos flanked by two archangels. The use of mosaic in Gelati attests to some Byzantine influence in the country and was a demonstration of the imperial ambition of the Bagrationids. The mosaic covered church could compete in magnificence with the churches of Constantinople. Gelati is one of few mosaic creations which survived in Georgia but fragments prove that the early churches of Pitsunda and Tsromi were also decorated with mosaic as well as other, lesser known sites. The destroyed 6th century mosaic floors in the Pitsunda Cathedral have been inspired by Roman prototypes. In Tsromi the tesserae are still visible on the walls of the 7th-century church but only faint lines hint at the original scheme. Its central figure was Christ standing and displaying a scroll with Georgian text.', 'id': '17', 'is_supporting': False, 'text': '# Mosaic\\nThe apse mosaic of the Gelati Monastery is a rare example of mosaic use in Georgia. Began by king David IV and completed by his son Demetrius I of Georgia, the fragmentary panel depicts Theotokos flanked by two archangels. The use of mosaic in Gelati attests to some Byzantine influence in the country and was a demonstration of the imperial ambition of the Bagrationids. The mosaic covered church could compete in magnificence with the churches of Constantinople. Gelati is one of few mosaic creations which survived in Georgia but fragments prove that the early churches of Pitsunda and Tsromi were also decorated with mosaic as well as other, lesser known sites. The destroyed 6th century mosaic floors in the Pitsunda Cathedral have been inspired by Roman prototypes. In Tsromi the tesserae are still visible on the walls of the 7th-century church but only faint lines hint at the original scheme. Its central figure was Christ standing and displaying a scroll with Georgian text.', 'title': 'Mosaic'}, {'body': \"In parts of Italy, which were under eastern artistic influences, like Sicily and Venice, mosaic making never went out of fashion in the Middle Ages. The whole interior of the St Mark's Basilica in Venice is clad with elaborate, golden mosaics. The oldest scenes were executed by Greek masters in the late 11th century but the majority of the mosaics are works of local artists from the 12thâ€“13th centuries. The decoration of the church was finished only in the 16th century. One hundred and ten scenes of mosaics in the atrium of St Mark's were based directly on the miniatures of the Cotton Genesis, a Byzantine manuscript that was brought to Venice after the sack of Constantinople (1204). The mosaics were executed in the 1220s.\", 'id': '18', 'is_supporting': False, 'text': \"# Mosaic\\nIn parts of Italy, which were under eastern artistic influences, like Sicily and Venice, mosaic making never went out of fashion in the Middle Ages. The whole interior of the St Mark's Basilica in Venice is clad with elaborate, golden mosaics. The oldest scenes were executed by Greek masters in the late 11th century but the majority of the mosaics are works of local artists from the 12thâ€“13th centuries. The decoration of the church was finished only in the 16th century. One hundred and ten scenes of mosaics in the atrium of St Mark's were based directly on the miniatures of the Cotton Genesis, a Byzantine manuscript that was brought to Venice after the sack of Constantinople (1204). The mosaics were executed in the 1220s.\", 'title': 'Mosaic'}, {'body': 'The mosaics of the Church of St Stephen in ancient Kastron Mefaa (now Umm ar-Rasas) were made in 785 (discovered after 1986). The perfectly preserved mosaic floor is the largest one in Jordan. On the central panel hunting and fishing scenes are depicted while another panel illustrates the most important cities of the region. The frame of the mosaic is especially decorative. Six mosaic masters signed the work: Staurachios from Esbus, Euremios, Elias, Constantinus, Germanus and Abdela. It overlays another, damaged, mosaic floor of the earlier (587) \"Church of Bishop Sergius.\" Another four churches were excavated nearby with traces of mosaic decoration.', 'id': '19', 'is_supporting': False, 'text': '# Mosaic\\nThe mosaics of the Church of St Stephen in ancient Kastron Mefaa (now Umm ar-Rasas) were made in 785 (discovered after 1986). The perfectly preserved mosaic floor is the largest one in Jordan. On the central panel hunting and fishing scenes are depicted while another panel illustrates the most important cities of the region. The frame of the mosaic is especially decorative. Six mosaic masters signed the work: Staurachios from Esbus, Euremios, Elias, Constantinus, Germanus and Abdela. It overlays another, damaged, mosaic floor of the earlier (587) \"Church of Bishop Sergius.\" Another four churches were excavated nearby with traces of mosaic decoration.', 'title': 'Mosaic'}], 'supporting_ids': ['1', '5'], 'n_hops': 2}) (input_keys={'docs', 'question'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_ds[3]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd58c5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer=\"Christ in majesty (or Ezekiel's Vision)\",\n",
       "    collected_info=[KeyInformation(info='Maria Tsiartsiani was born in Thessaloniki', source_doc_id='1'), KeyInformation(info=\"The mosaic in the Church of Hosios David in Thessaloniki is known as the 'Christ in majesty (or Ezekiel's Vision)' mosaic.\", source_doc_id='5')],\n",
       "    retrieved_doc_ids=['1', '2', '12', '5'],\n",
       "    citations=['5']\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-465a91fe2dfac98ba44f523ed36f2b3a&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-465a91fe2dfac98ba44f523ed36f2b3a)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = program(example.question, example.docs)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_retrieval_recall(example, pred, trace=None):\n",
    "    \"\"\"Retrieval recall metric - fraction of supporting documents found.\"\"\"\n",
    "    if not example.supporting_ids:\n",
    "        return 1.0  # No supporting documents to evaluate\n",
    "\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "\n",
    "    if not gold_ids:\n",
    "        return 1.0\n",
    "\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    return len(found) / len(gold_ids)\n",
    "\n",
    "\n",
    "def metric_retrieval_precision(example, pred, trace=None):\n",
    "    \"\"\"Retrieval precision metric - fraction of retrieved documents that are supporting.\"\"\"\n",
    "    if not example.supporting_ids:\n",
    "        return 1.0\n",
    "\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    return len(found) / len(retrieved_ids)\n",
    "\n",
    "\n",
    "def metric_answer_exact_match(example, pred, trace=None):\n",
    "    \"\"\"Exact match metric for MuSiQue using the official metrics.\"\"\"\n",
    "    return exact_match(pred.answer, example.answers)\n",
    "\n",
    "\n",
    "def metric_answer_f1_score(example, pred, trace=None):\n",
    "    \"\"\"Token-level F1 score using the official metrics.\"\"\"\n",
    "    return f1(pred.answer, example.answers)\n",
    "\n",
    "\n",
    "def metric_citation_f1(example, pred, trace=None):\n",
    "    \"\"\"Citation accuracy metrics - precision, recall, F1 for cited document IDs.\"\"\"\n",
    "    # Convert to sets for easy comparison\n",
    "    gold_ids = set(example.supporting_ids) if example.supporting_ids else set()\n",
    "    cited_ids = set(str(doc_id) for doc_id in pred.citations)  # Ensure string format\n",
    "\n",
    "    # Handle edge cases\n",
    "    if not gold_ids:\n",
    "        raise ValueError(\"Supporting docs must be provided for citation metric\")\n",
    "\n",
    "    if not cited_ids:\n",
    "        # No citations given but some needed\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate standard precision/recall/F1\n",
    "    correct_citations = cited_ids & gold_ids\n",
    "    precision = len(correct_citations) / len(cited_ids)\n",
    "    recall = len(correct_citations) / len(gold_ids)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def metric(example, pred, trace=None):\n",
    "    \"\"\"Combined metric for MuSiQue: weighted by number of hops.\"\"\"\n",
    "    retrieval_recall_score = metric_retrieval_recall(example, pred, trace)\n",
    "    retrieval_precision_score = metric_retrieval_precision(example, pred, trace)\n",
    "    answer_f1_score = metric_answer_f1_score(example, pred, trace)\n",
    "    citation_f1 = metric_citation_f1(example, pred, trace)\n",
    "\n",
    "    # Combine metrics: EM and F1 for answer quality, retrieval recall for completeness,\n",
    "    # citation F1 for proper attribution\n",
    "    score_weight_pairs = [\n",
    "        (retrieval_recall_score, 0.9),  # Retrieval recall for finding supporting docs\n",
    "        (retrieval_precision_score, 0.5),  # Retrieval precision for finding supporting docs\n",
    "        (answer_f1_score, 1.0),  # F1\n",
    "        (citation_f1, 0.7),  # Citation accuracy for proper attribution\n",
    "    ]\n",
    "\n",
    "    return sum(score * weight for score, weight in score_weight_pairs) / sum(weight for _, weight in score_weight_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0115e6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8440860215053764"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79664e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluating ORIGINAL program...\n",
      "Average Metric: 34.81 / 50 (69.6%): : 51it [12:36, 14.84s/it]                                                                                                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:26:37 INFO dspy.evaluate.evaluate: Average Metric: 34.80956083099297 / 50 (69.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval at: http://localhost:5005/#/experiments/1/runs/4b8d61076ae649a09876c1bb09cf8a32\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-8e36050c243e957a3a1fc4068fe8aa49&amp;experiment_id=1&amp;trace_id=tr-203927f7371d90f4111bc93d66db35f3&amp;experiment_id=1&amp;trace_id=tr-752125dce1f42af2f71da532346d7eb2&amp;experiment_id=1&amp;trace_id=tr-3a7508a6f8678c6895c0e965e2e3e4ad&amp;experiment_id=1&amp;trace_id=tr-81309312aeafdac55c36d4ec8fdd89ed&amp;experiment_id=1&amp;trace_id=tr-ba4521940eeb57d908c62ad7a8da6dbd&amp;experiment_id=1&amp;trace_id=tr-05dbaa6327e42661ca39b1c57eb44424&amp;experiment_id=1&amp;trace_id=tr-33813726f4ef0ac67386927cd5e24199&amp;experiment_id=1&amp;trace_id=tr-15da6431d483c8e2fa08cf70bbbf041f&amp;experiment_id=1&amp;trace_id=tr-348c90af0804ef5445be1023b9284843&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-8e36050c243e957a3a1fc4068fe8aa49), Trace(trace_id=tr-203927f7371d90f4111bc93d66db35f3), Trace(trace_id=tr-752125dce1f42af2f71da532346d7eb2), Trace(trace_id=tr-3a7508a6f8678c6895c0e965e2e3e4ad), Trace(trace_id=tr-81309312aeafdac55c36d4ec8fdd89ed), Trace(trace_id=tr-ba4521940eeb57d908c62ad7a8da6dbd), Trace(trace_id=tr-05dbaa6327e42661ca39b1c57eb44424), Trace(trace_id=tr-33813726f4ef0ac67386927cd5e24199), Trace(trace_id=tr-15da6431d483c8e2fa08cf70bbbf041f), Trace(trace_id=tr-348c90af0804ef5445be1023b9284843)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate original program\n",
    "print(\"ðŸ“Š Evaluating ORIGINAL program...\")\n",
    "original_evaluate = dspy.Evaluate(\n",
    "    devset=test_ds,\n",
    "    metric=metric,\n",
    "    num_threads=8,\n",
    "    display_table=False,\n",
    "    display_progress=True\n",
    ")\n",
    "original_eval_result = original_evaluate(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7f427",
   "metadata": {},
   "source": [
    "## GEPA Optimization\n",
    "\n",
    "GEPA is a reflective prompt optimizer that uses textual feedback to improve performance. We'll create feedback functions for each evaluation aspect and optimize our multi-hop QA program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8212c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_retrieval_recall(example, pred):\n",
    "    \"\"\"Generate feedback for retrieval recall evaluation.\"\"\"\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    recall_score = len(found) / len(gold_ids)\n",
    "\n",
    "    if recall_score == 1.0:\n",
    "        feedback = f\"Perfect retrieval! You found all {len(gold_ids)} supporting documents: {sorted(found)}\"\n",
    "    elif recall_score >= 0.5:\n",
    "        missing_ids = gold_ids - found\n",
    "        feedback = (\n",
    "            f\"Good retrieval (recall: {recall_score:.2f}). Found {len(found)} out of {len(gold_ids)} \"\n",
    "            f\"supporting documents. Missing: {sorted(missing_ids)}. Consider refining your search queries \"\n",
    "            f\"to find the remaining relevant documents.\"\n",
    "        )\n",
    "    else:\n",
    "        missing_ids = gold_ids - found\n",
    "        feedback = (\n",
    "            f\"Poor retrieval (recall: {recall_score:.2f}). Only found {len(found)} out of {len(gold_ids)} \"\n",
    "            f\"supporting documents. Missing critical documents: {sorted(missing_ids)}. \"\n",
    "            f\"Your search queries need to be more comprehensive and targeted.\"\n",
    "        )\n",
    "\n",
    "    return recall_score, feedback\n",
    "\n",
    "\n",
    "def feedback_retrieval_precision(example, pred):\n",
    "    \"\"\"Generate feedback for retrieval precision evaluation.\"\"\"\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "\n",
    "    if not retrieved_ids:\n",
    "        return 0.0, \"No documents were retrieved. Your search queries need to find relevant documents.\"\n",
    "\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    precision_score = len(found) / len(retrieved_ids)\n",
    "    irrelevant_docs = retrieved_ids - gold_ids\n",
    "\n",
    "    if precision_score == 1.0:\n",
    "        feedback = (\n",
    "            f\"Perfect precision! All {len(retrieved_ids)} retrieved documents are supporting documents: {sorted(found)}\"\n",
    "        )\n",
    "    elif precision_score >= 0.7:\n",
    "        feedback = (\n",
    "            f\"Good precision (precision: {precision_score:.2f}). {len(found)} out of {len(retrieved_ids)} \"\n",
    "            f\"retrieved documents are relevant. Irrelevant docs: {sorted(irrelevant_docs)}. \"\n",
    "            f\"Consider making your search queries more specific to avoid irrelevant documents.\"\n",
    "        )\n",
    "    elif precision_score >= 0.3:\n",
    "        feedback = (\n",
    "            f\"Moderate precision (precision: {precision_score:.2f}). Only {len(found)} out of {len(retrieved_ids)} \"\n",
    "            f\"retrieved documents are relevant. Many irrelevant docs retrieved: {sorted(irrelevant_docs)}. \"\n",
    "            f\"Your search queries are too broad - focus on more specific terms and entities.\"\n",
    "        )\n",
    "    else:\n",
    "        feedback = (\n",
    "            f\"Poor precision (precision: {precision_score:.2f}). Only {len(found)} out of {len(retrieved_ids)} \"\n",
    "            f\"retrieved documents are relevant. Most retrieved docs are irrelevant: {sorted(irrelevant_docs)}. \"\n",
    "            f\"Your search queries are retrieving too many irrelevant documents. Be much more specific and targeted.\"\n",
    "        )\n",
    "\n",
    "    return precision_score, feedback\n",
    "\n",
    "\n",
    "def feedback_answer_exact_match(example, pred):\n",
    "    \"\"\"Generate feedback for exact match evaluation.\"\"\"\n",
    "    em_score = exact_match(pred.answer, example.answers)\n",
    "\n",
    "    if em_score == 1.0:\n",
    "        feedback = f\"Perfect! You provided the exact correct answer: '{pred.answer}'. This matches the expected answer exactly.\"\n",
    "    else:\n",
    "        # Find the best matching answer for more specific feedback\n",
    "        best_answer = example.answers[0] if example.answers else \"N/A\"\n",
    "        feedback = (\n",
    "            f\"Your answer '{pred.answer}' doesn't exactly match the expected answer '{best_answer}'. \"\n",
    "            f\"Consider being more precise with entity names, dates, and specific facts.\"\n",
    "        )\n",
    "\n",
    "    return em_score, feedback\n",
    "\n",
    "\n",
    "def feedback_answer_f1_score(example, pred):\n",
    "    \"\"\"Generate feedback for F1 score evaluation.\"\"\"\n",
    "    f1_score = f1(pred.answer, example.answers)\n",
    "\n",
    "    if f1_score >= 0.9:\n",
    "        feedback = f\"Excellent! Your answer has high overlap (F1: {f1_score:.2f}) with the expected answer. Good token-level accuracy.\"\n",
    "    elif f1_score >= 0.5:\n",
    "        feedback = (\n",
    "            f\"Good partial match (F1: {f1_score:.2f}). Your answer contains relevant information but \"\n",
    "            f\"could be more complete or precise. Consider including more specific details from the retrieved documents.\"\n",
    "        )\n",
    "    else:\n",
    "        best_answer = example.answers[0] if example.answers else \"N/A\"\n",
    "        feedback = (\n",
    "            f\"Low overlap (F1: {f1_score:.2f}) with expected answer. Your answer '{pred.answer}' \"\n",
    "            f\"differs significantly from '{best_answer}'. Focus on extracting the specific information \"\n",
    "            f\"requested in the question.\"\n",
    "        )\n",
    "\n",
    "    return f1_score, feedback\n",
    "\n",
    "\n",
    "def feedback_citation_f1(example, pred):\n",
    "    \"\"\"Generate feedback for citation F1 evaluation.\"\"\"\n",
    "    gold_ids = set(example.supporting_ids) if example.supporting_ids else set()\n",
    "    cited_ids = set(str(doc_id) for doc_id in pred.citations)\n",
    "\n",
    "    if not gold_ids:\n",
    "        return 1.0, \"No supporting documents to cite.\"\n",
    "\n",
    "    if not cited_ids:\n",
    "        feedback = f\"You didn't cite any documents, but should have cited: {sorted(gold_ids)}. Always cite the documents that support your answer.\"\n",
    "        return 0.0, feedback\n",
    "\n",
    "    correct_citations = cited_ids & gold_ids\n",
    "    precision = len(correct_citations) / len(cited_ids)\n",
    "    recall = len(correct_citations) / len(gold_ids)\n",
    "    citation_f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    if citation_f1 >= 0.9:\n",
    "        feedback = f\"Excellent citations (F1: {citation_f1:.2f})! You properly cited the supporting documents: {sorted(correct_citations)}\"\n",
    "    elif citation_f1 >= 0.5:\n",
    "        incorrect_citations = cited_ids - gold_ids\n",
    "        missing_citations = gold_ids - cited_ids\n",
    "        feedback = f\"Good citations (F1: {citation_f1:.2f}). Correct: {sorted(correct_citations)}. \"\n",
    "        if incorrect_citations:\n",
    "            feedback += f\"Unnecessary: {sorted(incorrect_citations)}. \"\n",
    "        if missing_citations:\n",
    "            feedback += f\"Missing: {sorted(missing_citations)}. \"\n",
    "        feedback += \"Be more precise about which documents actually support your answer.\"\n",
    "    else:\n",
    "        incorrect_citations = cited_ids - gold_ids\n",
    "        missing_citations = gold_ids - cited_ids\n",
    "        feedback = (\n",
    "            f\"Poor citations (F1: {citation_f1:.2f}). You cited {sorted(cited_ids)} but should cite {sorted(gold_ids)}. \"\n",
    "            f\"Focus on identifying which documents directly support your answer claims.\"\n",
    "        )\n",
    "\n",
    "    return citation_f1, feedback\n",
    "\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    \"\"\"\n",
    "    Combined metric for MuSiQue with feedback for GEPA optimization.\n",
    "    Returns a dspy.Prediction with score (float) and feedback (str).\n",
    "\n",
    "    The feedback is targeted at specific predictors when pred_name is provided,\n",
    "    helping GEPA understand how to improve each component.\n",
    "    \"\"\"\n",
    "    # Compute feedback and scores for all metrics\n",
    "    score_answer_f1, fb_answer_f1 = feedback_answer_f1_score(example, pred)\n",
    "    score_retrieval_recall, fb_retrieval_recall = feedback_retrieval_recall(example, pred)\n",
    "    score_retrieval_precision, fb_retrieval_precision = feedback_retrieval_precision(example, pred)\n",
    "    score_citation_f1, fb_citation_f1 = feedback_citation_f1(example, pred)\n",
    "\n",
    "    # Combined score: weighted average of all metrics (same as original metric)\n",
    "    score_weight_pairs = [\n",
    "        (score_answer_f1, 1.0),  # Answer F1\n",
    "        (score_retrieval_recall, 0.9),  # Retrieval recall for finding supporting docs\n",
    "        (score_retrieval_precision, 0.5),  # Retrieval precision for finding supporting docs\n",
    "        (score_citation_f1, 0.7),  # Citation accuracy for proper attribution\n",
    "    ]\n",
    "\n",
    "    total_score = sum(score * weight for score, weight in score_weight_pairs) / sum(\n",
    "        weight for _, weight in score_weight_pairs\n",
    "    )\n",
    "\n",
    "    # Provide targeted feedback based on the predictor being optimized\n",
    "    if pred_name == \"generate_query.predict\":\n",
    "        # Focus on query generation quality and retrieval effectiveness\n",
    "        feedback = (\n",
    "            fb_retrieval_recall\n",
    "            + \" \"\n",
    "            + fb_retrieval_precision\n",
    "            + \" \"\n",
    "            + \"Your search queries should be both comprehensive (high recall) and specific (high precision). \"\n",
    "            \"Consider what entities, relationships, or facts are needed for each hop of reasoning.\"\n",
    "        )\n",
    "\n",
    "    elif pred_name == \"extract_info.predict\":\n",
    "        # Focus on information extraction quality\n",
    "        feedback = (\n",
    "            fb_answer_f1\n",
    "            + \" \"\n",
    "            + (\n",
    "                \"Focus on extracting the most relevant facts, entities, and relationships from the retrieved documents. \"\n",
    "                \"Make sure to capture information that directly helps answer the question or leads to the next reasoning step.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    elif pred_name == \"generate_answer.predict\":\n",
    "        # Focus on answer generation and citation quality\n",
    "        feedback = (\n",
    "            fb_answer_f1\n",
    "            + \" \"\n",
    "            + fb_citation_f1\n",
    "            + \" \"\n",
    "            + (\n",
    "                \"Provide precise, complete answers using the exact information from the retrieved documents. \"\n",
    "                \"Always cite the document IDs that support your answer claims.\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # Generic feedback combining all aspects\n",
    "        feedback = \"\\n\".join([\n",
    "            \"Overall performance breakdown:\",\n",
    "            f\"- Answer F1 Score: {fb_answer_f1}\",\n",
    "            f\"- Retrieval Recall: {fb_retrieval_recall}\",\n",
    "            f\"- Retrieval Precision: {fb_retrieval_precision}\",\n",
    "            f\"- Citations F1 Score: {fb_citation_f1}\"\n",
    "        ])\n",
    "\n",
    "    return dspy.Prediction(score=total_score, feedback=feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4450236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.844\n",
      "Feedback: Overall performance breakdown:\n",
      "- Answer F1 Score: Excellent! Your answer has high overlap (F1: 1.00) with the expected answer. Good token-level accuracy.\n",
      "- Retrieval Recall: Perfect retrieval! You found all 2 supporting documents: ['1', '5']\n",
      "- Retrieval Precision: Moderate precision (precision: 0.50). Only 2 out of 4 retrieved documents are relevant. Many irrelevant docs retrieved: ['12', '2']. Your search queries are too broad - focus on more specific terms and entities.\n",
      "- Citations F1 Score: Good citations (F1: 0.67). Correct: ['5']. Missing: ['1']. Be more precise about which documents actually support your answer.\n"
     ]
    }
   ],
   "source": [
    "# Test the feedback metric on our example\n",
    "feedback_result = metric_with_feedback(example, pred)\n",
    "print(f\"Score: {feedback_result.score:.3f}\")\n",
    "print(f\"Feedback: {feedback_result.feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0150cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GEPA optimizer configured\n"
     ]
    }
   ],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "# Set up GEPA optimizer with reflection LM for optimization\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"light\",  # Use light budget for faster experimentation. Use \"heavy\" for best performance\n",
    "    num_threads=8,\n",
    "    track_stats=True,\n",
    "    use_merge=False,\n",
    "    reflection_lm=reflection_lm  \n",
    ")\n",
    "\n",
    "print(\"âœ… GEPA optimizer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad84068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:26:38 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '72261987ca3342a2a49dd381825e0dce', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current dspy workflow\n",
      "2025/09/30 17:26:38 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1765 metric calls of the program. This amounts to 29.42 full evals on the train+val set.\n",
      "2025/09/30 17:26:38 INFO dspy.teleprompt.gepa.gepa: Using 30 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting GEPA optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GEPA Optimization:   0%|                                                                                                                                               | 0/1765 [00:00<?, ?rollouts/s]2025/09/30 17:42:02 INFO dspy.evaluate.evaluate: Average Metric: 17.871924849344207 / 30 (59.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_0 at: http://localhost:5005/#/experiments/1/runs/73771306fb7240a7a3f0a3338e436b5f\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:42:02 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.5957308283114735\n",
      "GEPA Optimization:   2%|â–ˆâ–ˆâ–                                                                                                                                | 30/1765 [15:24<14:51:08, 30.82s/rollouts]2025/09/30 17:42:02 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.5957308283114735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.40 / 3 (80.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:17<00:00, 45.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:44:21 INFO dspy.evaluate.evaluate: Average Metric: 2.399462365591398 / 3 (80.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_1 at: http://localhost:5005/#/experiments/1/runs/8d638260df954da28d57a576c538c532\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:45:03 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for generate_query.predict: Given a multi-hop question and collected information, generate a search query to find the next piece of information needed to answer the question. Focus on **specific entities, relationships, or facts** that require step-by-step resolution. Prioritize **precision and recall** by:  \n",
      "\n",
      "1. **Breaking down the question into logical hops**: Identify intermediate entities, dates, or facts that must be resolved (e.g., \"Church of St. Demetrios in Thessaloniki\" instead of generic terms like \"church in Thessaloniki\").  \n",
      "2. **Leveraging collected info**: Use explicitly stated entities (e.g., \"UN Command,\" \"Khabarovsk\") and their relationships to narrow queries.  \n",
      "3. **Anticipating ambiguity**: If terms like \"regions\" or \"mosaics\" are vague, consider standard classifications (e.g., \"sovereign states in Asia\" or \"famous mosaics in Church of St. Demetrios\").  \n",
      "4. **Refining queries for specificity**: Replace broad terms with precise combinations of entities and context (e.g., \"mosaic in Church of St. Demetrios Thessaloniki\" instead of \"mosaic in Thessaloniki church\").  \n",
      "5. **Checking for missing links**: If a stepâ€™s answer is incomplete (e.g., \"regions\" in Asia), explore alternative interpretations (e.g., \"administrative divisions in Asia\") to ensure comprehensive retrieval.  \n",
      "\n",
      "Examples of improvements from feedback:  \n",
      "- **Specificity**: Use \"Church of St. Demetrios Thessaloniki mosaic\" instead of \"mosaic in the church of Thessaloniki.\"  \n",
      "- **Ambiguity resolution**: For \"regions,\" consider \"sovereign states in Asia\" or \"UN-recognized regions in Asia.\"  \n",
      "- **Entity prioritization**: Focus on explicitly named organizations (e.g., \"UN Command\") and locations (e.g., \"Khabarovsk\") from collected info.  \n",
      "\n",
      "Balance **comprehensiveness** (high recall) with **precision** by combining specific entities with contextual terms. Avoid overgeneralized queries that retrieve irrelevant documents (e.g., \"Thessaloniki mosaics\" may return unrelated results).\n",
      "2025/09/30 17:49:39 INFO dspy.evaluate.evaluate: Average Metric: 2.0511520737327187 / 3 (68.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_2 at: http://localhost:5005/#/experiments/1/runs/a907004d769c43a5898c71d9cd35d484\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:49:40 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New subsample score is not better, skipping\n",
      "GEPA Optimization:   2%|â–ˆâ–ˆâ–‹                                                                                                                                | 36/1765 [23:01<19:38:46, 40.91s/rollouts]2025/09/30 17:49:40 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 0 score: 0.5957308283114735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.97 / 3 (65.7%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:19<00:00, 46.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:52:00 INFO dspy.evaluate.evaluate: Average Metric: 1.9704301075268817 / 3 (65.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_3 at: http://localhost:5005/#/experiments/1/runs/65f047c5c35b48459bc5263bd575cb2c\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 17:52:57 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for extract_info.predict: markdown\n",
      "### New Instruction for the Assistant\n",
      "\n",
      "**Task Description:**  \n",
      "You are to extract **precise and actionable key information** from provided documents that either directly answers a question or provides entities/relationships needed for the next retrieval step. Focus on **entities** (people, places, organizations), **relationships** (connections between entities), **dates** (including exact years and days), and **facts** (specific claims or events). If documents lack direct answers, extract **intermediate clues** that could guide further research.\n",
      "\n",
      "---\n",
      "\n",
      "**Critical Guidelines:**  \n",
      "1. **Extract All Relevant Entities**  \n",
      "   - Even if a document does not directly answer the question, extract **any entity** (e.g., locations, names, organizations) that could be connected to the question.  \n",
      "   - Example: If a question involves a presidentâ€™s work location, extract all locations mentioned in documents (e.g., \"Kennedy Space Center in Florida\").  \n",
      "\n",
      "2. **Prioritize Precision in Dates**  \n",
      "   - Capture **full dates** (day, month, year) if present. Avoid generalizations like \"October 29\" when the exact date is \"October 28, 2012.\"  \n",
      "   - If a document mentions a holiday or event (e.g., Hurricane Sandy), explicitly note its date and location.  \n",
      "\n",
      "3. **Map Relationships Between Entities**  \n",
      "   - Identify **indirect connections** (e.g., \"Mal Waldron is the performer of [album]\" or \"Kennedy Space Center is located in Florida\").  \n",
      "   - Example: If a document links an organization to a location, extract both (e.g., \"Angelo Lano worked in Washington DC\").  \n",
      "\n",
      "4. **Avoid Assumptions**  \n",
      "   - Do not infer information not explicitly stated in the documents. If a document does not mention a required detail (e.g., a presidentâ€™s name or a river near Hanoi), state this **but** extract all related entities that could aid follow-up steps.  \n",
      "\n",
      "5. **Format Output Clearly**  \n",
      "   - Use the `KeyInformation` class with `info` (specific fact) and `source_doc_id` (document ID).  \n",
      "   - Include **all relevant facts** from documents, even if they only partially address the question.  \n",
      "\n",
      "---\n",
      "\n",
      "**Example Application:**  \n",
      "For a question about Hurricane Sandy hitting a performerâ€™s birthplace:  \n",
      "- Extract the performerâ€™s name (e.g., \"Mal Waldron\") and their birthplace (if available).  \n",
      "- Extract Hurricane Sandyâ€™s date (e.g., \"October 28, 2012\") and its impact location (if available).  \n",
      "- If the birthplace is missing, extract **all other locations** in the documents that could lead to further research (e.g., \"Washington DC\" from Document 5).  \n",
      "\n",
      "**Goal:** Ensure the extracted information is **sufficient for the next retrieval step** or **directly answers the question** with unambiguous precision.\n",
      "2025/09/30 17:58:55 INFO dspy.evaluate.evaluate: Average Metric: 2.0725079918628304 / 3 (69.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_4 at: http://localhost:5005/#/experiments/1/runs/9d2301f4f4fd4b2d80254ade125a7700\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:17:41 INFO dspy.evaluate.evaluate: Average Metric: 17.00138248847926 / 30 (56.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_5 at: http://localhost:5005/#/experiments/1/runs/c8198af2a1164ff28139fe078d967b3a\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full valset score for new program: 0.5667127496159754\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full train_val score for new program: 0.5667127496159754\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Individual valset scores for new program: [0.8870967741935484, 0.26774193548387093, 0.9247311827956989, 0.15053763440860216, 0.521505376344086, 0.8709677419354839, 0.8387096774193549, 0.7365591397849461, 0.46159754224270355, 0.3010752688172043, 0.4569892473118279, 0.3010752688172043, 0.39677419354838706, 0.2258064516129032, 0.9354838709677419, 0.8225806451612903, 0.7365591397849461, 0.6612903225806451, 0.30806451612903224, 0.39677419354838706, 0.564516129032258, 0.8225806451612903, 0.5903225806451613, 0.5510752688172043, 0.39784946236559143, 0.3010752688172043, 0.1774193548387097, 0.643010752688172, 1.0, 0.7516129032258064]\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New valset pareto front scores: [0.9548387096774194, 0.26774193548387093, 0.9247311827956989, 0.8467741935483871, 0.521505376344086, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.4569892473118279, 0.3913978494623656, 0.39677419354838706, 0.9462365591397849, 0.9354838709677419, 0.8225806451612903, 0.7365591397849461, 0.6612903225806451, 0.30806451612903224, 0.39677419354838706, 0.5919354838709677, 0.8225806451612903, 0.8118279569892473, 0.5510752688172043, 0.5483870967741935, 0.4032258064516129, 0.41129032258064513, 0.6843672456575681, 1.0, 0.7516129032258064]\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Full valset pareto front score: 0.6565731222182835\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Updated valset pareto front programs: [{0}, {1}, {0, 1}, {0}, {1}, {0, 1}, {1}, {0}, {0}, {0}, {1}, {0}, {0, 1}, {0}, {0, 1}, {1}, {0, 1}, {1}, {1}, {0, 1}, {0}, {1}, {0}, {0, 1}, {0}, {0}, {0}, {0}, {1}, {1}]\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best valset aggregate score so far: 0.5957308283114735\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on train_val: 0\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best program as per aggregate score on valset: 0\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on valset: 0.5957308283114735\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Best score on train_val: 0.5957308283114735\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Linear pareto front program index: 0\n",
      "2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New program candidate index: 1\n",
      "GEPA Optimization:   4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                                             | 72/1765 [51:03<21:01:40, 44.71s/rollouts]2025/09/30 18:17:41 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 1 score: 0.5667127496159754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.12 / 3 (70.8%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:54<00:00, 38.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:19:36 INFO dspy.evaluate.evaluate: Average Metric: 2.1236559139784945 / 3 (70.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_6 at: http://localhost:5005/#/experiments/1/runs/273b1735ba2f42daa7a0983c644f8eb4\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:20:35 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for decide_info_collect.predict: text\n",
      "**Instruction for the Assistant:**\n",
      "\n",
      "Given a `question` and `all_information`, your task is to determine whether the provided information contains **sufficient, relevant, and accurate details** to answer the question. Follow these steps to ensure correctness:\n",
      "\n",
      "1. **Analyze the Question:**\n",
      "   - Identify key entities, dates, and relationships (e.g., \"father of the artist,\" \"Cameroon's main ally in 1306,\" \"largest peacekeeping troop contributor\").\n",
      "   - Note any historical, geopolitical, or contextual nuances (e.g., Cameroon as a modern nation did not exist in 1306; Ethiopia's role as a peacekeeping contributor).\n",
      "\n",
      "2. **Extract and Validate Information:**\n",
      "   - Use `all_information` to locate **directly relevant facts**. For example:\n",
      "     - If the question asks about a personâ€™s parent, ensure the provided information explicitly states the parent-child relationship (e.g., \"John Lennon's father is Alfred Lennon\").\n",
      "     - For historical events, verify timelines align (e.g., Cameroonâ€™s modern alliances vs. 1306).\n",
      "   - Discard irrelevant or loosely connected details (e.g., Nubian kings unrelated to Cameroonâ€™s 1306 allies).\n",
      "\n",
      "3. **Cite Supporting Documents:**\n",
      "   - **Mandatory:** Include all document IDs (e.g., [14], [19]) that directly support your answer.\n",
      "   - Avoid citing documents unrelated to the question (e.g., [7] in Example 1).\n",
      "\n",
      "4. **Determine Sufficiency (`has_collected_enough_info`):**\n",
      "   - **Set to `True`** only if:\n",
      "     - All components of the question are explicitly answered using `all_information`.\n",
      "     - No external data is needed (e.g., the annexation date of Eritrea and Ethiopiaâ€™s troop contribution are both provided in Example 3).\n",
      "   - **Set to `False`** if:\n",
      "     - Key information is missing (e.g., Cameroonâ€™s 1306 ally is not specified in Example 2).\n",
      "     - The provided data is contradictory or incomplete.\n",
      "\n",
      "5. **Improve Retrieval Precision:**\n",
      "   - Use **specific search terms** (e.g., \"largest peacekeeping troop contributor 2016\" instead of vague terms like \"peacekeeping\").\n",
      "   - Avoid broad queries that retrieve irrelevant documents (e.g., \"king\" in Example 2 led to unrelated Nubian ruler [6]).\n",
      "\n",
      "6. **Example-Driven Reasoning:**\n",
      "   - **For complex questions with nested dependencies** (e.g., \"Eritrea annexed by the largest peacekeeping troop contributor\"), break down the question into subparts:\n",
      "     1. Identify the entity (e.g., Ethiopia as the largest contributor).\n",
      "     2. Verify the historical event (e.g., Eritrea annexed in 1953).\n",
      "   - Ensure all subparts are addressed in `all_information`.\n",
      "\n",
      "7. **Special Cases:**\n",
      "   - If the question references **modern entities in historical contexts** (e.g., Cameroon in 1306), explicitly note the anachronism in your reasoning.\n",
      "   - For questions involving **numerical data** (e.g., \"8,324 personnel\"), confirm the source provides exact figures.\n",
      "\n",
      "**Final Output Requirements:**\n",
      "- Provide a `reasoning` field that logically connects the question to the evidence in `all_information`.\n",
      "- Set `has_collected_enough_info` to `True` or `False` based on the above criteria.\n",
      "- Ensure **zero tolerance for hallucinations**: Only use information explicitly stated in `all_information`.\n",
      "2025/09/30 18:20:52 INFO dspy.evaluate.evaluate: Average Metric: 2.1236559139784945 / 3 (70.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_7 at: http://localhost:5005/#/experiments/1/runs/ee4fc67d8d5a4e17bbfa1063963cf8f5\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:20:53 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:   4%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                                             | 78/1765 [54:14<20:08:20, 42.98s/rollouts]2025/09/30 18:20:53 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 1 score: 0.5667127496159754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.22 / 3 (74.1%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:25<00:00, 48.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:23:18 INFO dspy.evaluate.evaluate: Average Metric: 2.2236559139784946 / 3 (74.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_8 at: http://localhost:5005/#/experiments/1/runs/cbbe88529fd94b9e896d68b729f7e669\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:24:13 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for generate_answer.predict: Given a multi-hop question and a set of retrieved documents, provide a concise, precise answer that directly addresses the question using exact entities, dates, or facts from the documents. Follow these steps:  \n",
      "\n",
      "1. **Answer the question explicitly**: Focus solely on the specific information requested (e.g., a number, name, or entity) without adding extraneous context or speculation.  \n",
      "2. **Chain reasoning with citations**: Use a logical multi-hop approach to connect facts from the documents. For *every* claim or step in your reasoning, cite the exact document ID(s) that support it.  \n",
      "3. **Include *all* supporting citations**: If a document contributes to the reasoning (even indirectly), cite it. For example:  \n",
      "   - If the question requires connecting \"X\" and \"Y,\" and documents [A] and [B] are used to establish the link, cite both.  \n",
      "   - If a document provides context for a step (e.g., location of a university), cite it even if it does not directly state the final answer.  \n",
      "4. **Avoid assumptions**: Do not infer relationships or connections not explicitly stated in the documents. If a required link is missing (e.g., no document connects \"Angelical Tears\" to a city), state that the answer cannot be determined.  \n",
      "5. **Format citations correctly**: List all relevant document IDs in the \"citations\" field, using the format `['ID1', 'ID2']`.  \n",
      "\n",
      "Example:  \n",
      "If the question asks for the record label of a performer, and the documents state:  \n",
      "- \"Performer is Christina Aguilera\" [14], and  \n",
      "- \"Her album was released by RCA Records\" [18],  \n",
      "then cite both [14] and [18] to show the full chain of reasoning.  \n",
      "\n",
      "If the question asks for the number of households in a city, and the documents state:  \n",
      "- \"City A has 230,233 households\" [18], and  \n",
      "- \"Entity X is located in City A\" [14],  \n",
      "then cite both [14] and [18] to establish the link.  \n",
      "\n",
      "If the required connection is missing (e.g., no document links \"Entity X\" to a city), respond with \"Cannot be determined from the provided information\" and avoid citing documents that do not directly support this conclusion.\n",
      "2025/09/30 18:24:22 INFO dspy.evaluate.evaluate: Average Metric: 2.3172043010752685 / 3 (77.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_9 at: http://localhost:5005/#/experiments/1/runs/fb4a0ee0149f4c3c8f07968c6666725a\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:26:15 INFO dspy.evaluate.evaluate: Average Metric: 17.526497695852534 / 30 (58.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_10 at: http://localhost:5005/#/experiments/1/runs/7aefc217db0442f0964c1690a3406881\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset score for new program: 0.5842165898617512\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full train_val score for new program: 0.5842165898617512\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Individual valset scores for new program: [0.9548387096774194, 0.26774193548387093, 0.9247311827956989, 0.2408602150537634, 0.3709677419354838, 0.39784946236559143, 0.7849462365591396, 0.7365591397849461, 0.46774193548387094, 0.4139784946236559, 0.4569892473118279, 0.3913978494623656, 0.4569892473118279, 0.2258064516129032, 1.0, 0.8903225806451612, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.39677419354838706, 0.6370967741935484, 0.8903225806451612, 0.48924731182795694, 0.5510752688172043, 0.510752688172043, 0.3913978494623656, 0.26774193548387093, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New valset pareto front scores: [0.9548387096774194, 0.26774193548387093, 0.9247311827956989, 0.8467741935483871, 0.521505376344086, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.4569892473118279, 0.3913978494623656, 0.4569892473118279, 0.9462365591397849, 1.0, 0.8903225806451612, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.39677419354838706, 0.6370967741935484, 0.8903225806451612, 0.8118279569892473, 0.5510752688172043, 0.5483870967741935, 0.4032258064516129, 0.41129032258064513, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset pareto front score: 0.6757014848950332\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Updated valset pareto front programs: [{0, 2}, {1, 2}, {0, 1, 2}, {0}, {1}, {0, 1}, {1}, {0}, {0}, {0, 2}, {1, 2}, {0, 2}, {2}, {0}, {2}, {2}, {2}, {1, 2}, {1, 2}, {0, 1, 2}, {2}, {2}, {0}, {0, 1, 2}, {0}, {0}, {0}, {2}, {1, 2}, {2}]\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best valset aggregate score so far: 0.5957308283114735\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on train_val: 0\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on valset: 0\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on valset: 0.5957308283114735\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on train_val: 0.5957308283114735\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Linear pareto front program index: 0\n",
      "2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program candidate index: 2\n",
      "GEPA Optimization:   6%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                         | 114/1765 [59:37<11:20:40, 24.74s/rollouts]2025/09/30 18:26:16 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 1 score: 0.5667127496159754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.36 / 3 (78.6%): : 4it [07:08, 107.21s/it]                                                                                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:33:25 INFO dspy.evaluate.evaluate: Average Metric: 2.358064516129032 / 3 (78.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_11 at: http://localhost:5005/#/experiments/1/runs/60c53d5c2a7c465bb2df1ab352b10611\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:34:31 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for generate_query.predict: Given a multi-hop question and partial collected information, generate a precise and focused search query to retrieve the next critical piece of information required to answer the question. Follow these guidelines:  \n",
      "\n",
      "1. **Entity Disambiguation**:  \n",
      "   - Explicitly distinguish between similar-sounding or related entities (e.g., \"Democratic Republic of the Congo\" vs. \"Republic of the Congo\").  \n",
      "   - Verify the exact name of countries, people, or concepts mentioned in the question (e.g., clarify \"Purmerbuurt\" as part of the Netherlands).  \n",
      "\n",
      "2. **Multi-Hop Reasoning**:  \n",
      "   - Identify the next logical step in the reasoning chain. For example:  \n",
      "     - If the question requires a leader of a country, ensure the query specifies the **exact country name** and **timeframe** (e.g., \"First president of the Democratic Republic of the Congo after independence\").  \n",
      "     - If the question involves relationships (e.g., \"successor of Willem-Alexander\"), include **title hierarchies** (e.g., \"King of the Netherlands\") and **dates** to avoid ambiguity.  \n",
      "\n",
      "3. **Precision and Recall Balance**:  \n",
      "   - Use **specific terminology** (e.g., \"main subject of biographies of Wolfgang Amadeus Mozart\" instead of vague terms like \"biography details\").  \n",
      "   - Avoid redundancy in queries (e.g., do not repeat country independence dates unless necessary).  \n",
      "\n",
      "4. **Contextual Clues**:  \n",
      "   - Incorporate known facts from collected info into the query (e.g., if the question links to an \"effect,\" explicitly name it, as in \"Mozart effect\").  \n",
      "   - For questions involving locations (e.g., \"Bafwasende\"), confirm the correct geopolitical entity in the query (e.g., \"Democratic Republic of the Congo\").  \n",
      "\n",
      "5. **Iterative Refinement**:  \n",
      "   - If prior attempts retrieved irrelevant documents, narrow the query by adding qualifiers (e.g., \"successor of Willem-Alexander of the Netherlands\" instead of \"successor of Willem-Alexander\").  \n",
      "   - Prioritize exact titles, official names, and dates (e.g., \"Fulbert Youlou\" instead of \"first leader of Congo\").  \n",
      "\n",
      "**Example**:  \n",
      "For a question like \"Who followed Willem-Alexander of the country, bearing the official name, sometimes known as the country having Purmerbuurt?\"  \n",
      "- Use the collected info to map \"Purmerbuurt\" to the Netherlands.  \n",
      "- Frame the query as \"Successor of Willem-Alexander, King of the Netherlands\" to ensure specificity and avoid confusion with other \"Willem-Alexanders.\"  \n",
      "\n",
      "**Goal**: Generate a query that retrieves the minimal number of documents while maximizing relevance (high precision) and coverage of critical facts (high recall).\n",
      "2025/09/30 18:37:19 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 18:39:28 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 18:39:32 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 18:39:56 INFO dspy.evaluate.evaluate: Average Metric: 2.327956989247312 / 3 (77.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_12 at: http://localhost:5005/#/experiments/1/runs/6256e40b8020477cb2952070b5dea768\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:39:57 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New subsample score is not better, skipping\n",
      "GEPA Optimization:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                       | 120/1765 [1:13:19<17:06:26, 37.44s/rollouts]2025/09/30 18:39:57 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 1 score: 0.5667127496159754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:40:14 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 18:40:14 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.48 / 2 (74.1%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 2/3 [01:53<00:58, 58.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:42:20 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 18:42:20 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.17 / 3 (72.3%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:36<00:00, 72.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:43:34 INFO dspy.evaluate.evaluate: Average Metric: 2.168817204301075 / 3 (72.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_13 at: http://localhost:5005/#/experiments/1/runs/8d331aa8dd3440ababb174ba357d04e7\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:44:42 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for extract_info.predict: markdown\n",
      "### Revised Instruction for the Assistant\n",
      "\n",
      "**Task Description:**  \n",
      "You are to extract **precise, actionable, and unambiguous key information** from documents to either directly answer a question or provide **entities/relationships** critical for subsequent retrieval steps. Focus on **entities** (people, places, organizations), **relationships** (explicit or implied connections), **dates** (exact day, month, year), and **facts** (specific claims or events). If documents lack direct answers, extract **intermediate clues** (e.g., partial entities, contextual relationships) that could guide further research. Avoid speculative reasoning but document all relevant leads.\n",
      "\n",
      "---\n",
      "\n",
      "**Critical Guidelines:**  \n",
      "1. **Extract All Entities with Context**  \n",
      "   - Extract **all entities** (names, locations, organizations) even if their connection to the question is indirect.  \n",
      "   - Example: If a question involves a \"country with a constitution,\" extract all countries mentioned in documents (e.g., \"India\") and note their constitutional status if explicitly stated.  \n",
      "\n",
      "2. **Prioritize Exact Dates and Locations**  \n",
      "   - Capture **full dates** (e.g., \"April 17, 2008\") and **geographic specificity** (e.g., \"New Delhi, India\").  \n",
      "   - If a document mentions an event (e.g., \"2008 Summer Olympics torch relay\"), explicitly note its location and date.  \n",
      "\n",
      "3. **Map Relationships with Explicit Evidence**  \n",
      "   - Identify **direct relationships** (e.g., \"Iron Maiden performed 'Wasted Years'\") and **indirect relationships** (e.g., \"New Delhi hosts the World Bank regional office\").  \n",
      "   - If a relationship is implied but not stated (e.g., \"India has a constitution\"), extract the entity (India) and note the implication as a clue for follow-up.  \n",
      "\n",
      "4. **Avoid Assumptions but Capture Leads**  \n",
      "   - Do not infer information not explicitly stated. If a document does not mention a required detail (e.g., \"Georges Doriotâ€™s school\"), extract **all related entities** (e.g., \"Boston, Massachusetts\") that could guide next steps.  \n",
      "   - Example: If a question asks about a \"capital of a state with a constitutional country,\" extract the capital (e.g., \"Boston\") and the countryâ€™s constitutional status if mentioned.  \n",
      "\n",
      "5. **Format Output for Precision and Completeness**  \n",
      "   - Use the `KeyInformation` class with `info` (specific fact) and `source_doc_id`.  \n",
      "   - Include **all relevant facts**, even if they only partially address the question (e.g., \"New Delhi is in India\" if the question involves Indiaâ€™s constitution).  \n",
      "\n",
      "---\n",
      "\n",
      "**Example Applications:**  \n",
      "- **For questions involving nested relationships** (e.g., \"capital of the state containing the school of X\"):  \n",
      "  - Extract the person (X), their education institution (if mentioned), the state, and the capitalâ€™s land area.  \n",
      "  - If the school is missing, extract all locations (e.g., \"New Delhi\") and organizations (e.g., \"World Bank\") for follow-up.  \n",
      "\n",
      "- **For questions involving implied connections** (e.g., \"city with World Bank regional office in a constitutional country\"):  \n",
      "  - Extract the city (e.g., \"New Delhi\") and country (e.g., \"India\") and note Indiaâ€™s constitutional status if stated.  \n",
      "\n",
      "- **For questions requiring entity disambiguation** (e.g., \"band that performed Wasted Years\"):  \n",
      "  - Extract the band name (e.g., \"Iron Maiden\") and link it to the documentary (e.g., \"Classic Albums: Iron Maiden â€“ The Number of the Beast\").  \n",
      "\n",
      "---\n",
      "\n",
      "**Goal:** Ensure the extracted information is **sufficient to answer the question directly** or **provides unambiguous leads** for the next retrieval step. Prioritize **precision over completeness** when documents conflict or lack clarity.\n",
      "2025/09/30 18:47:27 INFO dspy.evaluate.evaluate: Average Metric: 2.0693548387096774 / 3 (69.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_14 at: http://localhost:5005/#/experiments/1/runs/13f587cb91c24cb183616d8279a4a968\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:47:27 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New subsample score is not better, skipping\n",
      "GEPA Optimization:   7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                      | 126/1765 [1:20:49<19:25:53, 42.68s/rollouts]2025/09/30 18:47:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 0 score: 0.5957308283114735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.70 / 3 (89.9%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [04:59<00:00, 99.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:52:28 INFO dspy.evaluate.evaluate: Average Metric: 2.696236559139785 / 3 (89.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_15 at: http://localhost:5005/#/experiments/1/runs/c4158cae690d4a25b71cbec445d2182f\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:53:27 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for decide_info_collect.predict: You are to determine whether the `all_information` provided contains sufficient details to answer the `question`. Your task is to:  \n",
      "\n",
      "1. **Parse the Question**:  \n",
      "   - Break the question into logical components (e.g., identifying entities, relationships, or specific data points).  \n",
      "   - Map each component to the corresponding information in `all_information`.  \n",
      "\n",
      "2. **Analyze `all_information`**:  \n",
      "   - Check for **explicit mentions** of entities, locations, dates, or relationships required to answer the question.  \n",
      "   - Resolve **conflicts** (e.g., conflicting headquarters locations in Example 1) by prioritizing the most directly relevant or unambiguous information.  \n",
      "   - Ensure **citations** are used precisely:  \n",
      "     - Only cite documents that **directly support** a specific claim in your reasoning.  \n",
      "     - Avoid citing irrelevant documents (e.g., [16] in Example 3).  \n",
      "\n",
      "3. **Generate Reasoning**:  \n",
      "   - Clearly link each part of the question to the corresponding information in `all_information`.  \n",
      "   - Explicitly state how the cited documents support your conclusion.  \n",
      "   - If the question requires **chaining information** (e.g., \"alma mater of the gold spike owner\"), ensure all necessary steps are logically connected.  \n",
      "\n",
      "4. **Determine `has_collected_enough_info`**:  \n",
      "   - Set to `True` **only if**:  \n",
      "     - All components of the question are explicitly addressed in `all_information`.  \n",
      "     - All necessary citations are included and correctly applied.  \n",
      "   - Set to `False` if:  \n",
      "     - Critical information is missing (e.g., a required entity or data point is not present).  \n",
      "     - Ambiguities cannot be resolved with the given data.  \n",
      "\n",
      "5. **Avoid Assumptions**:  \n",
      "   - Do not infer or add knowledge beyond what is explicitly stated in `all_information`.  \n",
      "   - If the question requires an implicit assumption (e.g., \"RSA Security Conference is held in San Francisco\"), ensure the `all_information` explicitly confirms it.  \n",
      "\n",
      "6. **Citation Precision**:  \n",
      "   - List **all relevant citations** for each claim.  \n",
      "   - For example:  \n",
      "     - If the answer relies on a location (e.g., Las Vegas [3]) and a person (e.g., Tony Hsieh [6]), both must be cited.  \n",
      "     - Missing a citation (e.g., [3] in Example 1) reduces accuracy.  \n",
      "\n",
      "**Examples of Key Considerations**:  \n",
      "- **Conflicting Data**: Prioritize the most specific or directly relevant source (e.g., resolving R&R Partnersâ€™ headquarters in Example 1 by aligning with the gold spikeâ€™s location).  \n",
      "- **Redundant Data**: Ignore duplicate entries (e.g., Harvardâ€™s enrollment repeated in Example 1).  \n",
      "- **Implicit Connections**: Ensure all logical steps are supported by the provided information (e.g., linking a governorâ€™s death state to voting patterns in Example 3).  \n",
      "\n",
      "**Output Format**:  \n",
      "- `reasoning`: A step-by-step explanation of how you used `all_information` to answer the question, with precise citations.  \n",
      "- `has_collected_enough_info`: A boolean (`True`/`False`) based on whether sufficient information exists in `all_information`.\n",
      "2025/09/30 18:56:36 INFO dspy.evaluate.evaluate: Average Metric: 2.771505376344086 / 3 (92.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_16 at: http://localhost:5005/#/experiments/1/runs/0ef2084129c844c3851daf02b0922bb3\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:07:54 INFO dspy.evaluate.evaluate: Average Metric: 18.298268935365712 / 30 (61.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_17 at: http://localhost:5005/#/experiments/1/runs/3bc18299bd284239b5271353ca6e5f82\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program is on the linear pareto front\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset score for new program: 0.6099422978455237\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full train_val score for new program: 0.6099422978455237\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Individual valset scores for new program: [0.9548387096774194, 0.26774193548387093, 0.9247311827956989, 0.8467741935483871, 0.521505376344086, 0.8709677419354839, 0.7311827956989246, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.3913978494623656, 0.4693548387096774, 0.9462365591397849, 0.9354838709677419, 0.15053763440860216, 0.7365591397849461, 0.5483870967741935, 0.3, 0.4032258064516129, 0.5919354838709677, 0.7666666666666666, 0.8118279569892473, 0.5510752688172043, 0.5483870967741935, 0.4032258064516129, 0.41129032258064513, 0.6843672456575681, 0.9247311827956989, 0.25268817204301075]\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New valset pareto front scores: [0.9548387096774194, 0.26774193548387093, 0.9247311827956989, 0.8467741935483871, 0.521505376344086, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.3913978494623656, 0.4693548387096774, 0.9462365591397849, 1.0, 0.8903225806451612, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.4032258064516129, 0.6370967741935484, 0.8903225806451612, 0.8118279569892473, 0.5510752688172043, 0.5483870967741935, 0.4032258064516129, 0.41129032258064513, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset pareto front score: 0.6780849974398361\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Updated valset pareto front programs: [{0, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {0, 3}, {1, 3}, {0, 1, 3}, {1}, {0, 3}, {0, 3}, {0, 2, 3}, {3}, {0, 2, 3}, {3}, {0, 3}, {2}, {2}, {2}, {1, 2}, {1, 2}, {3}, {2}, {2}, {0, 3}, {0, 1, 2, 3}, {0, 3}, {0, 3}, {0, 3}, {2}, {1, 2}, {2}]\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best valset aggregate score so far: 0.6099422978455237\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on train_val: 3\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on valset: 3\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on valset: 0.6099422978455237\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on train_val: 0.6099422978455237\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Linear pareto front program index: 3\n",
      "2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program candidate index: 3\n",
      "GEPA Optimization:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                                    | 162/1765 [1:41:17<16:55:30, 38.01s/rollouts]2025/09/30 19:07:55 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Selected program 1 score: 0.5667127496159754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.29 / 3 (42.9%): : 4it [08:31, 127.95s/it]                                                                                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:16:27 INFO dspy.evaluate.evaluate: Average Metric: 1.2881720430107526 / 3 (42.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_18 at: http://localhost:5005/#/experiments/1/runs/98a74a2cf0f040f9b6f6faee11fb6546\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:17:23 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Proposed new text for decide_info_collect.predict: You are tasked with determining whether the provided `all_information` contains sufficient details to answer the given `question`. Your output must include two fields:  \n",
      "1. **`reasoning`**: A detailed explanation of whether the information is sufficient. Specifically, you must:  \n",
      "   - Identify *all required components* of the question (e.g., specific entities, locations, dates, relationships, or numerical thresholds).  \n",
      "   - Check if each component is explicitly stated in `all_information`, including:  \n",
      "     - The identity of ambiguous terms (e.g., \"Starting Time performer\" in Example 1).  \n",
      "     - Geographical or historical context (e.g., linking \"Jacobinism\" to Europe in Example 3).  \n",
      "     - Numerical or comparative data (e.g., distances, population figures).  \n",
      "   - Avoid assumptions or inferences beyond the explicitly stated information.  \n",
      "   - Explicitly note *missing components* that prevent a complete answer.  \n",
      "\n",
      "2. **`has_collected_enough_info`**: A boolean (`True`/`False`) indicating whether the information is sufficient.  \n",
      "\n",
      "**Key Requirements**:  \n",
      "- **Citations**: Reference *all relevant documents* in `all_information` that directly support your reasoning (e.g., [10], [14]). Do not omit citations even if the document is redundant (e.g., Example 1â€™s repeated [10]).  \n",
      "- **Precision**: Avoid citing irrelevant documents (e.g., [7] in Example 1).  \n",
      "- **Retrieval**: If the answer depends on missing data (e.g., a document number [15] in Example 1), explicitly state this as a gap.  \n",
      "- **Domain-Specific Knowledge**:  \n",
      "  - Recognize that \"Jacobinism\" refers to radical political movements in Europe (Example 3).  \n",
      "  - Understand that \"South American countries\" include Argentina, Brazil, etc. (Example 2).  \n",
      "  - Distinguish between \"Renaissance in northern Europe\" versus \"Italy\" as a specific location (Example 3).  \n",
      "\n",
      "**Example Workflow**:  \n",
      "1. Parse the question to extract required entities and relationships.  \n",
      "2. Cross-check each required component against `all_information`, noting missing or ambiguous elements.  \n",
      "3. If all components are present and unambiguous, set `has_collected_enough_info=True`. Otherwise, set it to `False` and list missing pieces.  \n",
      "4. Cite all documents that directly support your reasoning (even if they are redundant).\n",
      "2025/09/30 19:20:38 INFO dspy.evaluate.evaluate: Average Metric: 1.2359447004608295 / 3 (41.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_19 at: http://localhost:5005/#/experiments/1/runs/eae8690d14354fd8855d06dd2d884c1a\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:20:39 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New subsample score is not better, skipping\n",
      "GEPA Optimization:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                   | 168/1765 [1:54:00<21:24:18, 48.25s/rollouts]2025/09/30 19:20:39 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Selected program 3 score: 0.6099422978455237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.11 / 3 (70.2%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:38<00:00, 72.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:24:17 INFO dspy.evaluate.evaluate: Average Metric: 2.106989247311828 / 3 (70.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_20 at: http://localhost:5005/#/experiments/1/runs/dfeff92084e24345a960d0beecb95ddc\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:25:21 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Proposed new text for generate_answer.predict: Given a multi-hop question and a set of retrieved documents, provide a concise answer that directly addresses the question using **exact entities, dates, and facts from the documents**. Follow these steps:  \n",
      "\n",
      "1. **Chain reasoning**: Use all relevant documents to logically connect entities, locations, and events.  \n",
      "   - Example: If the question links two entities (e.g., \"Al-Mu'tamid's successor\" and \"Al-Qanjarah\"), explicitly reference documents that establish their relationships and locations.  \n",
      "\n",
      "2. **Cite comprehensively**: Include **all document IDs** that directly or indirectly support your answer, even if they are redundant or repeated.  \n",
      "   - Example: If a document confirms a location (e.g., [10] for Al-Qanjarah in Syria) and another confirms an event (e.g., [3] for the 634 CE invasion), cite both.  \n",
      "   - Example: If a document is used in reasoning (e.g., [2] for Al-Mu'tadidâ€™s citizenship), include it in citations even if itâ€™s not the final answerâ€™s direct source.  \n",
      "\n",
      "3. **Avoid assumptions**: Only use information explicitly stated in the documents. If a connection is implied (e.g., \"Ladakhâ€™s guidance in religion\" to Tibet), clarify that the answer is inferred but still cite all supporting documents.  \n",
      "\n",
      "4. **Precision over brevity**: Use exact phrases from the documents (e.g., \"the 18th century\" instead of \"around 1700\") and avoid generalizations unless the documents explicitly allow it.  \n",
      "\n",
      "5. **Structure your response**:  \n",
      "   - **Reasoning**: Clearly explain how documents link to the answer.  \n",
      "   - **Answer**: State the final answer using exact facts.  \n",
      "   - **Citations**: List all document IDs used, even if they appear multiple times in the input.  \n",
      "\n",
      "**Examples to follow**:  \n",
      "- For geographic questions (e.g., neighboring counties), cite documents that establish both the location of the entity (e.g., [2] for birthplace, [16] for county) and the relationship to neighboring regions (e.g., [14]).  \n",
      "- For historical events (e.g., invasions), cite documents that specify the exact date (e.g., [3]) and location (e.g., [10]).  \n",
      "- For overlapping or repeated documents (e.g., multiple [11] entries), cite the same ID multiple times if it supports different parts of the answer.  \n",
      "\n",
      "**Critical note**: Even if feedback suggests \"missing\" document IDs (e.g., [14] or [0]), prioritize the IDs explicitly listed in the provided `all_information`. If a feedback document ID (e.g., [14]) is not in `all_information`, treat it as an error and cite only the IDs present.\n",
      "2025/09/30 19:26:36 INFO dspy.evaluate.evaluate: Average Metric: 1.9872665534804752 / 3 (66.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_21 at: http://localhost:5005/#/experiments/1/runs/59df1079b80a49a180dbfd76c50332f0\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:26:37 INFO dspy.teleprompt.gepa.gepa: Iteration 9: New subsample score is not better, skipping\n",
      "GEPA Optimization:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                                   | 174/1765 [1:59:58<22:02:08, 49.86s/rollouts]2025/09/30 19:26:37 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Selected program 3 score: 0.6099422978455237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.99 / 3 (66.3%): : 4it [03:51, 57.75s/it]                                                                                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:30:28 INFO dspy.evaluate.evaluate: Average Metric: 1.9882488479262672 / 3 (66.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_22 at: http://localhost:5005/#/experiments/1/runs/9126a2ba4f4a4de4a9845fc948f750f2\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:31:35 INFO dspy.teleprompt.gepa.gepa: Iteration 10: Proposed new text for generate_query.predict: Given a multi-hop question and partial information collected so far, generate a **specific and comprehensive search query** to resolve the next unresolved entity, date, or factual relationship required to answer the question.  \n",
      "\n",
      "**Key Requirements:**  \n",
      "1. **Break down the question into reasoning steps**: Identify the exact entities, relationships, or facts that must be resolved in sequence. For example:  \n",
      "   - If the question involves a chain of events (e.g., \"X did Y to Z\"), ensure each link in the chain is addressed step-by-step.  \n",
      "   - Prioritize resolving ambiguous or missing entities (e.g., \"country X\" â†’ specify the exact name once partially known).  \n",
      "\n",
      "2. **Use precise terms and entities from collected info**:  \n",
      "   - Incorporate **exact names** of countries, historical periods, or specific events (e.g., \"Portuguese colonial period in Brazil\" instead of \"colonial era\").  \n",
      "   - Avoid vague terms like \"blockade\" or \"migration\" without contextualizing them with dates or actors (e.g., \"Turkey's 1993 border closure with Armenia\").  \n",
      "\n",
      "3. **Balance recall and precision**:  \n",
      "   - Include **multiple relevant keywords** to capture all potential documents (e.g., \"Slavic population in Brazil during Portuguese colonial rule\" instead of \"Slavs in Brazil\").  \n",
      "   - Exclude irrelevant noise by avoiding overly broad terms (e.g., avoid \"open border\" alone; instead, pair it with specific actors like \"Armenia-Morocco border negotiations\").  \n",
      "\n",
      "4. **Leverage historical and geographic context**:  \n",
      "   - For questions involving colonization, migration, or historical control, explicitly reference timeframes (e.g., \"5th-6th century Slavic migration\") or governing powers (e.g., \"Ottoman dynasty-commissioned buildings in Morocco\").  \n",
      "   - Clarify ambiguous regions (e.g., \"South America\" â†’ \"Brazil\" if the colonial holding is known).  \n",
      "\n",
      "5. **Structure queries for multi-hop dependencies**:  \n",
      "   - If the next step depends on resolving a prior entity (e.g., \"country X\" where X is a placeholder), explicitly include the placeholder in the query (e.g., \"population of Slavs in [Portuguese colony] during [timeframe]\").  \n",
      "\n",
      "**Examples of Effective Queries:**  \n",
      "- Instead of \"When did the UK take control of South Africa?\" â†’ \"When did the United Kingdom establish colonial rule over South Africa?\"  \n",
      "- Instead of \"who wants to open the border between Armenia and Morocco\" â†’ \"Armenia-Morocco border reopening proposals by [specific stakeholders, e.g., Turkish business lobby]\".  \n",
      "- Instead of \"population of Slavs in Brazil\" â†’ \"Slavic population in Portuguese colonial Brazil during the 16th-18th centuries\".  \n",
      "\n",
      "**Domain-Specific Notes:**  \n",
      "- For questions involving **historical colonization** (e.g., Portugal in Brazil), include the **timeframe** and **governing entity** explicitly.  \n",
      "- For **migration events** (e.g., Slavic migration), specify the **origin/destination regions** and **dates** to avoid ambiguity.  \n",
      "- For **religious or architectural contexts** (e.g., Ottoman-commissioned buildings in Morocco), link the entity to its exact location and historical role (e.g., \"Ottoman dynasty-commissioned mosques at al-Qarawiyin University\").  \n",
      "\n",
      "**Final Goal:** Generate a query that is both **specific enough to exclude irrelevant documents** and **comprehensive enough to cover all relevant sources**, ensuring the next reasoning step can proceed with resolved facts.\n",
      "2025/09/30 19:42:21 INFO dspy.evaluate.evaluate: Average Metric: 1.958560794044665 / 3 (65.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_23 at: http://localhost:5005/#/experiments/1/runs/04c56dd87b504fcaa31ddb2c421add03\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:42:21 INFO dspy.teleprompt.gepa.gepa: Iteration 10: New subsample score is not better, skipping\n",
      "GEPA Optimization:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                   | 180/1765 [2:15:43<29:52:46, 67.87s/rollouts]2025/09/30 19:42:21 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Selected program 3 score: 0.6099422978455237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.57 / 3 (52.4%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:29<00:00, 49.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:44:51 INFO dspy.evaluate.evaluate: Average Metric: 1.5709677419354837 / 3 (52.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_24 at: http://localhost:5005/#/experiments/1/runs/db72dc7dabae43048a80bf54e3432397\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:46:04 INFO dspy.teleprompt.gepa.gepa: Iteration 11: Proposed new text for extract_info.predict: text\n",
      "Given a question and retrieved documents, extract the most relevant and precise information that directly answers the question or provides critical context for the next retrieval step. Focus on the following:\n",
      "\n",
      "1. **Entities**: Identify all named entities (people, places, organizations, events) explicitly mentioned in the documents. If the question references an entity not directly mentioned in the documents, note this explicitly but do not infer or assume connections.\n",
      "\n",
      "2. **Relationships**: Extract relationships between entities (e.g., \"X was the successor of Y,\" \"Z occurred in Location A\"). Ensure these relationships are explicitly stated in the documents.\n",
      "\n",
      "3. **Dates and Events**: Capture exact dates, timeframes, and event names (e.g., \"634 CE,\" \"October 29, 2012\"). If a document mentions a date that could answer the question, extract it verbatim, even if additional context is missing.\n",
      "\n",
      "4. **Specificity**: Prioritize the most specific information that directly addresses the question. For example:\n",
      "   - If the question asks for a date, extract the exact date, not a general timeframe.\n",
      "   - If the question asks for a \"main subject of biographies,\" extract the entity (e.g., \"Wolfgang Amadeus Mozart\") and avoid generalizations like \"his life and works\" unless explicitly stated in the document.\n",
      "\n",
      "5. **Contextual Assumptions**: If the documents do not explicitly mention a required entity (e.g., \"Al-Mu'tamid's successor,\" \"Mozart effect\"), do not assume external knowledge. Instead, extract the closest relevant information (e.g., \"Levant (Syria)\" in Example 1) and clearly state its connection to the question.\n",
      "\n",
      "6. **Document-Only Scope**: Rely solely on the provided documents. Do not use prior knowledge or infer details not present in the documents (e.g., Hurricane Sandyâ€™s date in Example 3 was not in the documents and should not be assumed).\n",
      "\n",
      "7. **Format**: Return a list of `KeyInformation` objects with:\n",
      "   - `info`: A concise fact, entity, or relationship from the document.\n",
      "   - `source_doc_id`: The ID of the document from which the information was extracted.\n",
      "\n",
      "Example: If the question asks about a \"composer with an effect named after him,\" and the documents mention \"Wolfgang Amadeus Mozart\" but not the \"Mozart effect,\" extract only the explicitly stated facts about Mozart, not inferred associations.\n",
      "2025/09/30 19:48:31 INFO dspy.evaluate.evaluate: Average Metric: 1.539247311827957 / 3 (51.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_25 at: http://localhost:5005/#/experiments/1/runs/eb99e082c11c48e4bd31ef57f5ee7c53\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:48:31 INFO dspy.teleprompt.gepa.gepa: Iteration 11: New subsample score is not better, skipping\n",
      "GEPA Optimization:  11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                                  | 186/1765 [2:21:53<29:14:36, 66.67s/rollouts]2025/09/30 19:48:31 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Selected program 3 score: 0.6099422978455237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.28 / 3 (42.8%): : 5it [07:39, 91.84s/it]                                                                                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:56:11 INFO dspy.evaluate.evaluate: Average Metric: 1.2838709677419353 / 3 (42.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_26 at: http://localhost:5005/#/experiments/1/runs/e31ac37910cf45b6a2b7bc4e2709528f\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 19:57:42 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Proposed new text for decide_info_collect.predict: **Revised Instructions for the Assistant**  \n",
      "\n",
      "You are to determine whether the `all_information` provided contains sufficient details to answer the `question`. Follow these steps **rigorously**, ensuring precision in citations and logical connections:  \n",
      "\n",
      "---\n",
      "\n",
      "### **1. Parse the Question with Explicit Mapping**  \n",
      "- **Break the question into discrete components**:  \n",
      "  - Identify **entities** (e.g., organizations, people, locations), **relationships** (e.g., \"authorized troops,\" \"place of birth\"), and **data points** (e.g., \"square miles,\" \"number of regions\").  \n",
      "  - For example, if the question is *\"How many regions in Asia does the UN Command recognize?\"*, the components are:  \n",
      "    - **Entity 1**: UN Command (action: authorized troops).  \n",
      "    - **Entity 2**: Nikifor Popovâ€™s birthplace (location: Khabarovsk, Asia).  \n",
      "    - **Data Point**: Number of regions recognized by the UN Command in Asia.  \n",
      "\n",
      "- **Map each component to `all_information`**:  \n",
      "  - Use **direct quotes or explicit mentions** in `all_information` to satisfy each component.  \n",
      "  - Example: If the question asks about \"regions in Asia,\" ensure `all_information` explicitly states how many regions the UN Command recognizes (e.g., \"The UN Command recognizes 53 regions in Asia [X]\").  \n",
      "\n",
      "---\n",
      "\n",
      "### **2. Analyze `all_information` with Precision**  \n",
      "- **Check for explicit mentions**:  \n",
      "  - Only use **explicitly stated** information (e.g., \"Magdolna Purgly was born in Hungary [0]\") to answer.  \n",
      "  - Do not infer indirect relationships (e.g., \"Hungary is not a South American country\" is an assumption, not a fact in `all_information`).  \n",
      "\n",
      "- **Resolve conflicts**:  \n",
      "  - If conflicting data exists (e.g., \"R&R Partnersâ€™ headquarters is in Las Vegas [3]\" vs. \"R&R Partnersâ€™ headquarters is in San Francisco [6]\"), prioritize the **most specific or directly relevant source** (e.g., align with the gold spikeâ€™s location if the question ties to ownership).  \n",
      "\n",
      "- **Cite all relevant documents**:  \n",
      "  - For **every claim**, list **all supporting citations** (e.g., \"The UN Command authorized troops [10]; Nikifor Popov was born in Asia [4]; Asia-Pacific Group has 53 members [7]\").  \n",
      "  - **Do not omit citations** even if they seem redundant (e.g., if [7] is mentioned twice, cite it both times).  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Generate Reasoning with Logical Chaining**  \n",
      "- **For multi-step questions**, explicitly connect each step using `all_information`:  \n",
      "  - Example:  \n",
      "    1. **Step 1**: Identify the organization: \"The UN Command authorized troops [10].\"  \n",
      "    2. **Step 2**: Identify the continent: \"Nikifor Popov was born in Asia [4].\"  \n",
      "    3. **Step 3**: Identify regions recognized by the organization: \"Asia-Pacific Group has 53 members [7].\"  \n",
      "    4. **Conflict Resolution**: Clarify if the UN Command and Asia-Pacific Group are distinct entities (they are) [7].  \n",
      "\n",
      "- **Avoid assumptions**:  \n",
      "  - If the question asks for \"the capital of the state containing the school where Georges Doriot was educated,\" ensure `all_information` explicitly links Doriotâ€™s school â†’ state â†’ capital (e.g., \"Georges Doriot attended Harvard in Massachusetts [13]; Massachusettsâ€™ capital is Boston [6]\").  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Determine `has_collected_enough_info`**  \n",
      "- **Set to `True` only if**:  \n",
      "  - All question components are **explicitly addressed** in `all_information`.  \n",
      "  - All necessary citations are included (e.g., [10], [4], [7] for Example 1).  \n",
      "\n",
      "- **Set to `False` if**:  \n",
      "  - A critical component is missing (e.g., no info on the UN Commandâ€™s regions in Asia [Example 1]).  \n",
      "  - Ambiguities cannot be resolved (e.g., Magdolna Purglyâ€™s birthplace is not in South America [Example 2]).  \n",
      "\n",
      "---\n",
      "\n",
      "### **5. Key Considerations**  \n",
      "- **Citation Precision**:  \n",
      "  - Example: If the answer relies on a location (e.g., Las Vegas [3]) and a person (e.g., Tony Hsieh [6]), **cite both**. Missing a citation reduces accuracy.  \n",
      "\n",
      "- **Redundant Data**:  \n",
      "  - Ignore duplicate entries (e.g., \"Harvardâ€™s enrollment repeated in Example 1\").  \n",
      "\n",
      "- **Implicit Connections**:  \n",
      "  - Ensure all logical steps are supported (e.g., \"governorâ€™s death state â†’ voting patterns\" [Example 3]).  \n",
      "\n",
      "---\n",
      "\n",
      "### **6. Output Format**  \n",
      "- **`reasoning`**:  \n",
      "  - A **step-by-step explanation** linking each question component to `all_information` with precise citations.  \n",
      "  - Example:  \n",
      "    - \"The UN Command authorized troops [10]; Nikifor Popov was born in Asia [4]; the Asia-Pacific Group has 53 members [7]. However, the question asks about the UN Commandâ€™s regions in Asia, which is not explicitly stated. Thus, insufficient data.\"  \n",
      "\n",
      "- **`has_collected_enough_info`**:  \n",
      "  - A boolean (`True`/`False`) based on the analysis above.  \n",
      "\n",
      "---\n",
      "\n",
      "**Example of Correct Citation and Chaining**  \n",
      "**Question**: \"How many people live in the South American country Magdolna Purgly is from?\"  \n",
      "**`all_information`**:  \n",
      "- \"Magdolna Purgly was born in Sofronya, Hungary [0].\"  \n",
      "- \"Document 11 discusses Jewish populations in Argentina (196,000-600,000) [11].\"  \n",
      "**Correct Reasoning**:  \n",
      "- \"Magdolna Purgly was born in Hungary [0], which is **not** a South American country. However, Document 11 discusses South American countries like Argentina [11]. Since the question asks about a South American country associated with Purgly, and no such link exists in `all_information`, the answer is invalid. However, if the question intended to ask about Argentina, the population is 196,000-600,000 [11].\"  \n",
      "**`has_collected_enough_info`**: `False` (due to missing link between Purgly and South America).  \n",
      "\n",
      "--- \n",
      "\n",
      "**Final Note**: Always verify that your citations directly support your claims and that no assumptions are made beyond `all_information`.\n",
      "2025/09/30 20:00:21 INFO dspy.evaluate.evaluate: Average Metric: 1.5349462365591398 / 3 (51.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_27 at: http://localhost:5005/#/experiments/1/runs/05654b92c7704b89b4ef46fb885a55c4\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:09:50 INFO dspy.evaluate.evaluate: Average Metric: 18.05010240655402 / 30 (60.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_28 at: http://localhost:5005/#/experiments/1/runs/4d99e98678c7466290804b71612e9f78\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Full valset score for new program: 0.6016700802184672\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Full train_val score for new program: 0.6016700802184672\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Individual valset scores for new program: [0.9548387096774194, 0.15053763440860216, 0.9247311827956989, 0.8467741935483871, 0.521505376344086, 0.8709677419354839, 0.7311827956989246, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.3913978494623656, 0.4693548387096774, 0.9462365591397849, 0.9354838709677419, 0.15053763440860216, 0.7365591397849461, 0.5483870967741935, 0.3, 0.4032258064516129, 0.5919354838709677, 0.7666666666666666, 0.8118279569892473, 0.5510752688172043, 0.5483870967741935, 0.4032258064516129, 0.41129032258064513, 0.5534050179211469, 0.9247311827956989, 0.25268817204301075]\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: New valset pareto front scores: [0.9548387096774194, 0.26774193548387093, 0.9247311827956989, 0.8467741935483871, 0.521505376344086, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.3913978494623656, 0.4693548387096774, 0.9462365591397849, 1.0, 0.8903225806451612, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.4032258064516129, 0.6370967741935484, 0.8903225806451612, 0.8118279569892473, 0.5510752688172043, 0.5483870967741935, 0.4032258064516129, 0.41129032258064513, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Full valset pareto front score: 0.6780849974398361\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Updated valset pareto front programs: [{0, 2, 3, 4}, {1, 2, 3}, {0, 1, 2, 3, 4}, {0, 3, 4}, {1, 3, 4}, {0, 1, 3, 4}, {1}, {0, 3, 4}, {0, 3, 4}, {0, 2, 3, 4}, {3, 4}, {0, 2, 3, 4}, {3, 4}, {0, 3, 4}, {2}, {2}, {2}, {1, 2}, {1, 2}, {3, 4}, {2}, {2}, {0, 3, 4}, {0, 1, 2, 3, 4}, {0, 3, 4}, {0, 3, 4}, {0, 3, 4}, {2}, {1, 2}, {2}]\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best valset aggregate score so far: 0.6099422978455237\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best program as per aggregate score on train_val: 3\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best program as per aggregate score on valset: 3\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best score on valset: 0.6099422978455237\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Best score on train_val: 0.6099422978455237\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: Linear pareto front program index: 3\n",
      "2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 12: New program candidate index: 4\n",
      "GEPA Optimization:  13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                | 222/1765 [2:43:12<20:15:35, 47.27s/rollouts]2025/09/30 20:09:51 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Selected program 3 score: 0.6099422978455237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.33 / 3 (77.7%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:40<00:00, 53.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:12:31 INFO dspy.evaluate.evaluate: Average Metric: 2.3311827956989246 / 3 (77.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_29 at: http://localhost:5005/#/experiments/1/runs/acd104be4ce541338a1563a498562ee0\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:13:26 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Proposed new text for generate_answer.predict: You are to answer multi-hop questions by synthesizing information from provided documents. Follow these steps rigorously:  \n",
      "\n",
      "1. **Break down the question into subcomponents**:  \n",
      "   - Identify all entities, relationships, and required connections (e.g., \"torch visit\" â†’ \"city\" â†’ \"World Bank regional office\").  \n",
      "   - Map each component to the exact entities/dates/facts in the documents.  \n",
      "\n",
      "2. **Prioritize document accuracy and completeness**:  \n",
      "   - Use **only the exact entities/dates/facts** from the provided `all_information`.  \n",
      "   - If documents conflict (e.g., R&R Partners headquarters in Las Vegas [3] vs. San Francisco [8]), select the document most directly tied to the answer (e.g., [8] in Example 3).  \n",
      "   - **Do not infer or assume** relationships not explicitly stated in the documents (e.g., avoid conflating Turkey with Ottoman dynasty unless explicitly linked in the documents).  \n",
      "\n",
      "3. **Cite all supporting documents precisely**:  \n",
      "   - Include **every document ID** that contributes to the answer, even if indirectly.  \n",
      "   - Exclude irrelevant documents (e.g., [0] in Example 1 is not cited because itâ€™s missing from `all_information`).  \n",
      "   - Example: If the answer relies on documents [5] and [6], cite both even if only [5] is directly quoted.  \n",
      "\n",
      "4. **Structure your output**:  \n",
      "   - **Answer**: A concise, exact value (e.g., \"7,200\" instead of \"about 7,000\").  \n",
      "   - **Citations**: A list of **all document IDs** that support the answer.  \n",
      "\n",
      "5. **Avoid generalizations**:  \n",
      "   - Replace ambiguous terms with specific facts (e.g., \"Harvard University\" instead of \"the alma mater\").  \n",
      "   - Use exact dates (e.g., \"April 17, 2008\" instead of \"in 2008\").  \n",
      "\n",
      "6. **Verify logical consistency**:  \n",
      "   - Ensure the answer logically connects all subcomponents (e.g., \"gold spike owner\" â†’ Tony Hsieh [6] â†’ \"alma mater\" â†’ Harvard [8] â†’ \"enrollment\" â†’ 7,200 [18]).  \n",
      "   - If a document is repeated (e.g., [18] thrice in Example 3), cite it once.  \n",
      "\n",
      "**Example**:  \n",
      "For a question about \"X in the city where Y is headquartered,\" first confirm the city from documents, then locate X in that city. Always cite the document(s) for the city and X.\n",
      "2025/09/30 20:15:30 INFO dspy.evaluate.evaluate: Average Metric: 2.4064516129032256 / 3 (80.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_30 at: http://localhost:5005/#/experiments/1/runs/31d7439fc0334c8c9431b9ad09823976\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:19:38 INFO dspy.evaluate.evaluate: Average Metric: 18.9194234728895 / 30 (63.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_31 at: http://localhost:5005/#/experiments/1/runs/ad87ffc6268b4fd58b834b116d8b8ad2\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: New program is on the linear pareto front\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Full valset score for new program: 0.6306474490963166\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Full train_val score for new program: 0.6306474490963166\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Individual valset scores for new program: [1.0, 0.26774193548387093, 0.9247311827956989, 0.914516129032258, 0.5967741935483871, 0.8709677419354839, 0.7849462365591396, 0.8225806451612903, 0.6065202470830473, 0.3010752688172043, 0.4946236559139785, 0.4139784946236559, 0.4693548387096774, 0.9462365591397849, 1.0, 0.15053763440860216, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.343010752688172, 0.5919354838709677, 0.8741935483870967, 0.8118279569892473, 0.5510752688172043, 0.6236559139784946, 0.27419354838709675, 0.5919354838709677, 0.6967741935483871, 1.0, 0.25268817204301075]\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: New valset pareto front scores: [1.0, 0.26774193548387093, 0.9247311827956989, 0.914516129032258, 0.5967741935483871, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.4139784946236559, 0.4693548387096774, 0.9462365591397849, 1.0, 0.8903225806451612, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.4032258064516129, 0.6370967741935484, 0.8903225806451612, 0.8118279569892473, 0.5510752688172043, 0.6236559139784946, 0.4032258064516129, 0.5919354838709677, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Full valset pareto front score: 0.6936405529953917\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Updated valset pareto front programs: [{5}, {1, 2, 3, 5}, {0, 1, 2, 3, 4, 5}, {5}, {5}, {0, 1, 3, 4, 5}, {1}, {0, 3, 4, 5}, {0, 3, 4}, {0, 2, 3, 4}, {3, 4}, {5}, {3, 4, 5}, {0, 3, 4, 5}, {2, 5}, {2}, {2, 5}, {1, 2, 5}, {1, 2, 5}, {3, 4}, {2}, {2}, {0, 3, 4, 5}, {0, 1, 2, 3, 4, 5}, {5}, {0, 3, 4}, {5}, {2}, {1, 2, 5}, {2}]\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best valset aggregate score so far: 0.6306474490963166\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best program as per aggregate score on train_val: 5\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best program as per aggregate score on valset: 5\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best score on valset: 0.6306474490963166\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Best score on train_val: 0.6306474490963166\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: Linear pareto front program index: 5\n",
      "2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 13: New program candidate index: 5\n",
      "GEPA Optimization:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                             | 258/1765 [2:53:00<13:41:06, 32.69s/rollouts]2025/09/30 20:19:38 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Selected program 5 score: 0.6306474490963166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:19:51 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.75 / 3 (91.8%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:51<00:00, 77.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:23:30 INFO dspy.evaluate.evaluate: Average Metric: 2.7526881720430105 / 3 (91.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_32 at: http://localhost:5005/#/experiments/1/runs/11d9ac43e976465eb190c56b493f5839\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:24:23 INFO dspy.teleprompt.gepa.gepa: Iteration 14: Proposed new text for generate_query.predict: **Instruction for Generating Search Queries for Multi-Hop Questions**  \n",
      "\n",
      "1. **Analyze the Question Structure**:  \n",
      "   - Break down the question into sequential reasoning steps (hops). Identify the **primary entities** (e.g., people, locations, organizations) and **relationships** (e.g., \"record label of,\" \"birthplace of,\" \"mosaic in\") that need resolution at each step.  \n",
      "\n",
      "2. **Leverage Collected Information**:  \n",
      "   - Use the provided `collected_info` to determine what is already known and what remains unresolved. For example:  \n",
      "     - If the performer of a song is known, the next hop is to resolve their record label.  \n",
      "     - If a personâ€™s city of association is known but their birthplace is not, prioritize resolving the birthplace.  \n",
      "\n",
      "3. **Generate Specific Search Queries**:  \n",
      "   - Focus on **precise entities** and **domain-specific terms** to avoid irrelevant results. Examples:  \n",
      "     - Instead of \"mosaic in the church of [City],\" use \"[Specific Church Name] mosaic\" if the church is known.  \n",
      "     - For record labels, include the artistâ€™s name and time frame (e.g., \"Christina Aguilera record label 2000s\").  \n",
      "   - Avoid broad terms like \"near\" or \"close to\" unless paired with exact locations (e.g., \"distance between Wrigley Field and [Specific Water Source]\").  \n",
      "\n",
      "4. **Balance Recall and Precision**:  \n",
      "   - Prioritize **high-precision queries** by:  \n",
      "     - Specifying exact names (e.g., \"Clifford Jordan birthplace\" instead of \"jazz musician birthplace\").  \n",
      "     - Including contextual details (e.g., \"Thessaloniki church mosaics\" + known church names like \"Church of Saint Demetrios\").  \n",
      "   - Ensure **high recall** by covering all possible variations of the entity or fact (e.g., multiple names for a location or historical terms).  \n",
      "\n",
      "5. **Iterative Refinement**:  \n",
      "   - If the initial search yields irrelevant results, refine the query by:  \n",
      "     - Adding constraints (e.g., time period, geographic specificity).  \n",
      "     - Replacing ambiguous terms with verified entities from prior steps.  \n",
      "\n",
      "**Key Domain-Specific Considerations**:  \n",
      "- **Music Industry**: Record labels often change over time; include dates if available.  \n",
      "- **Geography**: For proximity questions, resolve exact coordinates or landmarks (e.g., \"Lake Michigan\" as a water source).  \n",
      "- **Art History**: Mosaics are often tied to specific churches or historical periods; include both in queries.  \n",
      "\n",
      "**Example Workflow**:  \n",
      "For the question *\"What is the record label of the performer of [Song]?\"*:  \n",
      "1. Resolve the performer (already done in `collected_info`).  \n",
      "2. Generate a query like \"[Performer Name] record label [Time Period]\" to narrow results.  \n",
      "3. Verify the labelâ€™s relevance to the songâ€™s release date.  \n",
      "\n",
      "By following these steps, the assistant can systematically resolve multi-hop questions with both precision and recall.\n",
      "2025/09/30 20:28:32 INFO dspy.evaluate.evaluate: Average Metric: 2.7204301075268815 / 3 (90.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_33 at: http://localhost:5005/#/experiments/1/runs/9a7f09239e6c4ac5a753953117dca037\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:28:33 INFO dspy.teleprompt.gepa.gepa: Iteration 14: New subsample score is not better, skipping\n",
      "GEPA Optimization:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                            | 264/1765 [3:01:54<15:59:59, 38.37s/rollouts]2025/09/30 20:28:33 INFO dspy.teleprompt.gepa.gepa: Iteration 15: Selected program 1 score: 0.5667127496159754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.83 / 3 (94.2%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:47<00:00, 55.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:31:20 INFO dspy.evaluate.evaluate: Average Metric: 2.825268817204301 / 3 (94.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_34 at: http://localhost:5005/#/experiments/1/runs/c8ffeb43217545b3842ca5c54db89833\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:32:24 INFO dspy.teleprompt.gepa.gepa: Iteration 15: Proposed new text for generate_answer.predict: **Instructions for the Assistant:**  \n",
      "\n",
      "You are to answer **multi-hop questions** by synthesizing information from provided documents. Follow these guidelines:  \n",
      "\n",
      "1. **Answer Structure**:  \n",
      "   - Provide a **concise, direct answer** to the question using **exact entities/dates/facts** from the documents.  \n",
      "   - Include **citations** (document IDs in brackets, e.g., [13]) for **every factual claim** in your answer.  \n",
      "\n",
      "2. **Reasoning Process**:  \n",
      "   - Break the question into parts (e.g., identify key entities, relationships, or events).  \n",
      "   - Map each part to the **specific document(s)** that provide the necessary information.  \n",
      "   - Combine information logically to form the answer, ensuring **all supporting documents are cited**.  \n",
      "\n",
      "3. **Citation Rules**:  \n",
      "   - **Cite all documents** that directly support **any step of your reasoning**, even if they do not directly state the final answer.  \n",
      "   - Avoid omitting citations for intermediate facts (e.g., linking \"Bermudaâ€™s non-natives\" [8] and \"Australopithecus fossils in South Africa\" [14] to the final answer about British control in 1909 [4]).  \n",
      "\n",
      "4. **Precision**:  \n",
      "   - Use **verbatim data** (e.g., \"June 29, 1776\" [13], not \"in the 18th century\").  \n",
      "   - Do not infer or generalize beyond the provided documents.  \n",
      "\n",
      "5. **Examples to Follow**:  \n",
      "   - For questions requiring **multi-hop reasoning** (e.g., connecting Bermudaâ€™s population [8] to South Africaâ€™s history [4]), cite **all relevant documents** in the reasoning and answer.  \n",
      "   - Ensure answers are **token-accurate** (e.g., \"Classic Albums: Iron Maiden â€“ The Number of the Beast\" [13], not \"the Iron Maiden documentary\").  \n",
      "\n",
      "**Key Domain-Specific Notes**:  \n",
      "- **Multi-hop questions** often require chaining facts (e.g., \"Country X took control of Country Y\" may involve documents about Xâ€™s colonization of Y and Yâ€™s fossil records).  \n",
      "- **Citations are graded strictly**: Missing a supporting document (e.g., [8] in Example 2) reduces citation F1 score, even if the answer is factually correct.  \n",
      "- **Avoid assumptions**: Only use information explicitly stated in the documents (e.g., do not assume Bermuda is part of Britain unless stated in [2]).  \n",
      "\n",
      "**Generalizable Strategy**:  \n",
      "- Parse the question into components (e.g., \"Country A\" and \"Country B\").  \n",
      "- Retrieve documents for each component.  \n",
      "- Verify relationships between components using the documents.  \n",
      "- Construct the answer with **exact phrases** and **all relevant citations**.\n",
      "2025/09/30 20:32:40 INFO dspy.evaluate.evaluate: Average Metric: 2.315684899485741 / 3 (77.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_35 at: http://localhost:5005/#/experiments/1/runs/26a29d158e7d4ead9f6d612ab3755bcc\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:32:40 INFO dspy.teleprompt.gepa.gepa: Iteration 15: New subsample score is not better, skipping\n",
      "GEPA Optimization:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                            | 270/1765 [3:06:02<16:05:17, 38.74s/rollouts]2025/09/30 20:32:40 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Selected program 2 score: 0.5842165898617512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.32 / 3 (77.2%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:43<00:00, 54.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:35:24 INFO dspy.evaluate.evaluate: Average Metric: 2.3172043010752685 / 3 (77.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_36 at: http://localhost:5005/#/experiments/1/runs/6fd9ae0468ac49e491260ea112b07170\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:36:15 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Proposed new text for generate_query.predict: Given a multi-hop question and partial collected information, generate a **specific and precise search query** to resolve the next unresolved entity, relationship, or fact in the reasoning chain. Follow these guidelines:  \n",
      "\n",
      "1. **Break down the question into sequential hops**:  \n",
      "   - Identify the exact entities, relationships, or facts that must be resolved step-by-step (e.g., person â†’ university â†’ publication, or artist â†’ work â†’ parent).  \n",
      "   - Prioritize resolving ambiguous or unknown entities first (e.g., disambiguate \"Five Treasure Island\" if it is a fictional or non-standard term).  \n",
      "\n",
      "2. **Use specific, unambiguous terms**:  \n",
      "   - Incorporate **exact entity names** (e.g., \"John Kerry\" instead of \"Kerry\") and **relationships** (e.g., \"father of\", \"attended university\", \"publication of\").  \n",
      "   - Avoid overly broad queries (e.g., \"Kerry attended university\" â†’ refine to \"John Kerry undergraduate university\").  \n",
      "\n",
      "3. **Contextualize the search**:  \n",
      "   - Include cultural, temporal, or geographical qualifiers (e.g., \"Japanese term for Shinto spirits\", \"New Haven university publication\").  \n",
      "   - If a term is ambiguous (e.g., \"Five Treasure Island\"), explicitly verify its relevance or correct interpretation in the query.  \n",
      "\n",
      "4. **Balance recall and precision**:  \n",
      "   - Aim for queries that retrieve **highly relevant documents** while covering possible variations (e.g., \"John Lennon father\" is sufficient for a well-known figure, but for lesser-known entities, add context like \"full name\" or \"biography\").  \n",
      "   - Avoid irrelevant results by focusing on **direct relationships** (e.g., \"weekly publication of [University Name]\" instead of generic \"university publications\").  \n",
      "\n",
      "5. **Leverage prior collected information**:  \n",
      "   - Chain queries to already resolved facts (e.g., if \"Thomas Rutherford Bacon was born in New Haven\" is known, next query: \"University Kerry attended in New Haven\").  \n",
      "   - Reference document citations (e.g., if a prior answer cites [7], ensure the new query aligns with that sourceâ€™s context).  \n",
      "\n",
      "**Example**:  \n",
      "If the question is \"What weekly publication in the city where Thomas Rutherford Bacon was born is issued by the university Kerry attended?\" and collected info states Bacon was born in **New Haven, Connecticut**, the next step is to resolve **Kerryâ€™s university**. A precise query would be:  \n",
      "`\"John Kerry undergraduate university in New Haven Connecticut\"` (instead of \"Kerry attended university\").\n",
      "2025/09/30 20:37:42 INFO dspy.evaluate.evaluate: Average Metric: 2.9059139784946235 / 3 (96.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_37 at: http://localhost:5005/#/experiments/1/runs/7ce14ed488a34145893be672b572959b\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:40:43 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:44:31 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:45:09 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:47:29 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:49:58 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:50:27 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:50:33 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:50:34 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:52:07 INFO dspy.evaluate.evaluate: Average Metric: 17.27488479262673 / 30 (57.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_38 at: http://localhost:5005/#/experiments/1/runs/9a1968ac0f77431e88dcde1fe4409e9a\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full valset score for new program: 0.5758294930875576\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full train_val score for new program: 0.5758294930875576\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Individual valset scores for new program: [0.914516129032258, 0.26774193548387093, 0.9247311827956989, 0.2408602150537634, 0.5483870967741935, 0.39784946236559143, 0.7849462365591396, 0.7096774193548387, 0.46774193548387094, 0.27419354838709675, 0.4569892473118279, 0.26774193548387093, 0.4569892473118279, 0.2258064516129032, 1.0, 0.914516129032258, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.30645161290322576, 0.5241935483870968, 0.914516129032258, 0.47580645161290325, 0.5510752688172043, 0.6236559139784946, 0.3913978494623656, 0.26774193548387093, 0.8119815668202764, 1.0, 0.8118279569892473]\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: New valset pareto front scores: [1.0, 0.26774193548387093, 0.9247311827956989, 0.914516129032258, 0.5967741935483871, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.4139784946236559, 0.4693548387096774, 0.9462365591397849, 1.0, 0.914516129032258, 0.7741935483870968, 0.6612903225806451, 0.30806451612903224, 0.4032258064516129, 0.6370967741935484, 0.914516129032258, 0.8118279569892473, 0.5510752688172043, 0.6236559139784946, 0.4032258064516129, 0.5919354838709677, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Full valset pareto front score: 0.6952534562211982\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Updated valset pareto front programs: [{5}, {1, 2, 3, 5, 6}, {0, 1, 2, 3, 4, 5, 6}, {5}, {5}, {0, 1, 3, 4, 5}, {1}, {0, 3, 4, 5}, {0, 3, 4}, {0, 2, 3, 4}, {3, 4}, {5}, {3, 4, 5}, {0, 3, 4, 5}, {2, 5, 6}, {6}, {2, 5, 6}, {1, 2, 5, 6}, {1, 2, 5, 6}, {3, 4}, {2}, {6}, {0, 3, 4, 5}, {0, 1, 2, 3, 4, 5, 6}, {5, 6}, {0, 3, 4}, {5}, {2, 6}, {1, 2, 5, 6}, {2}]\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best valset aggregate score so far: 0.6306474490963166\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best program as per aggregate score on train_val: 5\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best program as per aggregate score on valset: 5\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best score on valset: 0.6306474490963166\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Best score on train_val: 0.6306474490963166\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: Linear pareto front program index: 5\n",
      "2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 16: New program candidate index: 6\n",
      "GEPA Optimization:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                         | 306/1765 [3:25:29<14:22:22, 35.46s/rollouts]2025/09/30 20:52:08 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Selected program 2 score: 0.5842165898617512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.82 / 3 (60.7%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:25<00:00,  8.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:52:34 INFO dspy.evaluate.evaluate: Average Metric: 1.8204301075268816 / 3 (60.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_39 at: http://localhost:5005/#/experiments/1/runs/de5e42bef134427a92f3d7a7ec0a6ba5\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 20:52:55 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:52:55 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n",
      "2025/09/30 20:54:52 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Proposed new text for extract_info.predict: markdown\n",
      "### Revised Instruction for the Assistant\n",
      "\n",
      "**Task Description:**  \n",
      "You are to extract **precise and actionable key information** from provided documents that either directly answers a question or provides entities/relationships needed for the next retrieval step. Focus on **entities** (people, places, organizations), **relationships** (connections between entities), **dates** (including exact years and days), and **facts** (specific claims or events). If documents lack direct answers, extract **intermediate clues** that could guide further research. **Pay special attention to historical periods, their origins, and associated geographic locations, as these often form the crux of the question.**\n",
      "\n",
      "---\n",
      "\n",
      "**Critical Guidelines:**  \n",
      "1. **Extract All Relevant Entities, Including Historical Periods and Their Origins**  \n",
      "   - If a document mentions a historical period (e.g., \"Renaissance,\" \"Enlightenment\"), extract it as an entity and **map it to its commonly known geographic origin** (e.g., \"Renaissance â†’ Italy\") even if the document does not explicitly state this. This is critical for questions about the origin of methods or events tied to such periods.  \n",
      "   - Example: If a document mentions \"the Renaissance,\" extract it as an entity and note its association with Italy, as this is essential for answering questions about where a method originated.  \n",
      "\n",
      "2. **Prioritize Precision in Dates and Event Mapping**  \n",
      "   - Capture **full dates** (day, month, year) if present. If a document references a well-known event (e.g., \"Hurricane Sandy,\" \"Jacobinism\"), explicitly note its **date and geographic impact** (e.g., \"Jacobinism affected all of Europe\").  \n",
      "   - If a document references a monarch or political figure (e.g., \"Philip IV of France\"), extract their **reign dates, titles, and territories** even if the connection to the question is indirect.  \n",
      "\n",
      "3. **Map Relationships Between Entities and Historical Contexts**  \n",
      "   - Identify **indirect connections** (e.g., \"Renaissance â†’ Italy,\" \"Jacobinism â†’ Europe\") that link entities to geographic locations or time periods.  \n",
      "   - Example: If a document links a teaching method to the \"Renaissance,\" extract this relationship and associate it with Italy, as this is key to answering questions about where the method originated.  \n",
      "\n",
      "4. **Avoid Assumptions but Use Common Historical Knowledge for Periods**  \n",
      "   - Do not infer information not explicitly stated in the documents **except for well-documented historical periods and events** (e.g., Renaissance in Italy, Jacobinism in Europe). For such cases, extract the **period** and **its geographic origin** as a relationship.  \n",
      "   - Example: If a document mentions \"the Enlightenment,\" extract it as an entity and note its association with northern Europe, as this is a well-established historical fact.  \n",
      "\n",
      "5. **Format Output Clearly for Direct or Intermediate Use**  \n",
      "   - Use the `KeyInformation` class with `info` (specific fact) and `source_doc_id` (document ID).  \n",
      "   - Include **all relevant facts** from documents, even if they only partially address the question. For example, if a document mentions a kingâ€™s reign but not their ally, extract the kingâ€™s name and reign dates as potential clues for follow-up research.  \n",
      "\n",
      "---\n",
      "\n",
      "**Example Application for Historical Periods:**  \n",
      "If a question asks about the origin of a method during a specific period (e.g., \"Where did associationism originate?\"), and a document mentions it began in the \"Renaissance,\" extract:  \n",
      "- `KeyInformation(info='Associationism originated during the Renaissance.', source_doc_id='6')`  \n",
      "- `KeyInformation(info='The Renaissance is historically associated with Italy.', source_doc_id='6')`  \n",
      "\n",
      "**Example Application for Political Figures and Allies:**  \n",
      "If a question asks about a countryâ€™s ally in a specific year (e.g., \"Who was the king of Cameroonâ€™s main ally in 1306?\") and a document mentions \"Philip IV of France,\" extract:  \n",
      "- `KeyInformation(info='Philip IV of France reigned from 1285â€“1314.', source_doc_id='14')`  \n",
      "- `KeyInformation(info='France is a potential candidate for Cameroonâ€™s main ally in 1306.', source_doc_id='14')`  \n",
      "\n",
      "**Goal:** Ensure the extracted information is **sufficient for the next retrieval step** or **directly answers the question** with unambiguous precision, leveraging both explicit document content and **commonly accepted historical associations** for well-known periods and events.\n",
      "2025/09/30 20:57:37 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:58:53 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:59:37 WARNING dspy.clients.lm: LM response was truncated due to exceeding max_tokens=8192. You can inspect the latest LM interactions with `dspy.inspect_history()`. To avoid truncation, consider passing a larger max_tokens when setting up dspy.LM. You may also consider increasing the temperature (currently 0.6)  if the reason for truncation is repetition.\n",
      "2025/09/30 20:59:54 INFO dspy.evaluate.evaluate: Average Metric: 2.501075268817204 / 3 (83.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_40 at: http://localhost:5005/#/experiments/1/runs/a41b0e69badd44088776b59fbf090d0b\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:14:26 INFO dspy.evaluate.evaluate: Average Metric: 17.097644649257557 / 30 (57.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_41 at: http://localhost:5005/#/experiments/1/runs/6026e5eb07e6493b94e57a5b49a080cf\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Full valset score for new program: 0.5699214883085851\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Full train_val score for new program: 0.5699214883085851\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Individual valset scores for new program: [1.0, 0.26774193548387093, 0.9247311827956989, 0.364516129032258, 0.39784946236559143, 0.39784946236559143, 0.7634408602150538, 0.4247311827956989, 0.4103942652329749, 0.1774193548387097, 0.31451612903225806, 0.3913978494623656, 0.5295698924731183, 0.9193548387096774, 1.0, 0.26774193548387093, 0.7741935483870968, 0.6612903225806451, 0.3709677419354838, 0.4569892473118279, 0.632258064516129, 0.7774193548387097, 0.7258064516129032, 0.5510752688172043, 0.5483870967741935, 0.3913978494623656, 0.5919354838709677, 0.8119815668202764, 1.0, 0.25268817204301075]\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: New valset pareto front scores: [1.0, 0.26774193548387093, 0.9247311827956989, 0.914516129032258, 0.5967741935483871, 0.8709677419354839, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.4139784946236559, 0.5295698924731183, 0.9462365591397849, 1.0, 0.914516129032258, 0.7741935483870968, 0.6612903225806451, 0.3709677419354838, 0.4569892473118279, 0.6370967741935484, 0.914516129032258, 0.8118279569892473, 0.5510752688172043, 0.6236559139784946, 0.4032258064516129, 0.5919354838709677, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Full valset pareto front score: 0.7011495135688683\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Updated valset pareto front programs: [{5, 7}, {1, 2, 3, 5, 6, 7}, {0, 1, 2, 3, 4, 5, 6, 7}, {5}, {5}, {0, 1, 3, 4, 5}, {1}, {0, 3, 4, 5}, {0, 3, 4}, {0, 2, 3, 4}, {3, 4}, {5}, {7}, {0, 3, 4, 5}, {2, 5, 6, 7}, {6}, {2, 5, 6, 7}, {1, 2, 5, 6, 7}, {7}, {7}, {2}, {6}, {0, 3, 4, 5}, {0, 1, 2, 3, 4, 5, 6, 7}, {5, 6}, {0, 3, 4}, {5, 7}, {2, 6, 7}, {1, 2, 5, 6, 7}, {2}]\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Best valset aggregate score so far: 0.6306474490963166\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Best program as per aggregate score on train_val: 5\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Best program as per aggregate score on valset: 5\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Best score on valset: 0.6306474490963166\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Best score on train_val: 0.6306474490963166\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: Linear pareto front program index: 5\n",
      "2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 17: New program candidate index: 7\n",
      "GEPA Optimization:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                       | 342/1765 [3:47:49<14:18:39, 36.20s/rollouts]2025/09/30 21:14:27 INFO dspy.teleprompt.gepa.gepa: Iteration 18: Selected program 5 score: 0.6306474490963166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.33 / 3 (77.5%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [02:11<00:00, 43.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:16:39 INFO dspy.evaluate.evaluate: Average Metric: 2.325268817204301 / 3 (77.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_42 at: http://localhost:5005/#/experiments/1/runs/936c90d20e8d4a1d956d0abd161792b7\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:17:24 INFO dspy.teleprompt.gepa.gepa: Iteration 18: Proposed new text for extract_info.predict: You are to act as an information extraction assistant. Given a complex, multi-step question and a set of retrieved documents, your task is to:  \n",
      "\n",
      "1. **Identify and extract precise entities, relationships, dates, and factual claims** from the documents that directly address the question or enable the next logical step in answering it.  \n",
      "   - **Entities**: Names of people, organizations, locations, events, or concepts explicitly mentioned (e.g., \"Oklahoma City,\" \"Barack Obama,\" \"Slavic migration\").  \n",
      "   - **Relationships**: Explicit connections between entities (e.g., \"Angelical Tears formed in Oklahoma City,\" \"North Carolina shifted to Democratic voters in 2008\").  \n",
      "   - **Dates**: Specific timeframes tied to events (e.g., \"5th-6th centuries CE,\" \"1976,\" \"2008\").  \n",
      "   - **Facts**: Statistical claims (if present), political shifts, or historical events (e.g., \"North Carolina became a swing state,\" \"Slavs settled the Balkans\").  \n",
      "\n",
      "2. **Avoid assumptions or inferences** not directly supported by the documents. For example:  \n",
      "   - Do not infer numerical values (e.g., \"5 million Slavs migrated\") if the documents lack such data.  \n",
      "   - Do not assume connections between unrelated entities (e.g., linking \"Airoldi\" to \"Portuguese colonies\" if the documents do not clarify this).  \n",
      "\n",
      "3. **Prioritize actionable information** that:  \n",
      "   - Answers the question (e.g., \"Barack Obama won North Carolina in 2008\").  \n",
      "   - Narrows the scope for further research (e.g., \"Angelical Tears is based in Oklahoma City\").  \n",
      "   - Resolves ambiguities (e.g., \"North Carolinaâ€™s political history includes Democratic shifts in 2008\").  \n",
      "\n",
      "4. **Structure outputs as a list of `KeyInformation` objects**, each containing:  \n",
      "   - `info`: A concise, specific fact or relationship.  \n",
      "   - `source_doc_id`: The document ID from which it was extracted.  \n",
      "\n",
      "**Critical guidelines**:  \n",
      "- If the documents lack critical data (e.g., household numbers in Oklahoma City), explicitly note this in the reasoning but do not fabricate or guess.  \n",
      "- For questions requiring cross-referencing (e.g., \"Airoldi\" and \"Portuguese colonies\"), extract all relevant entities and relationships separately to enable downstream reasoning.  \n",
      "- Use the documentsâ€™ exact terminology (e.g., \"Bab-el-Mandeb\" vs. \"Gate of Tears\") to maintain precision.  \n",
      "\n",
      "Example of desired output:  \n",
      "For a question about a governorâ€™s death and a stateâ€™s political shift, extract:  \n",
      "- \"North Carolina voters elected Barack Obama in 2008\" (source_doc_id=3)  \n",
      "- \"North Carolina became a swing state after 1976\" (source_doc_id=3)  \n",
      "\n",
      "This ensures clarity and traceability for follow-up steps.\n",
      "2025/09/30 21:19:08 INFO dspy.evaluate.evaluate: Average Metric: 1.9139784946236558 / 3 (63.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_43 at: http://localhost:5005/#/experiments/1/runs/ccc8fb1a75684761b2d2cfc1ba0daaa8\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:19:08 INFO dspy.teleprompt.gepa.gepa: Iteration 18: New subsample score is not better, skipping\n",
      "GEPA Optimization:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                                                      | 348/1765 [3:52:30<14:38:07, 37.18s/rollouts]2025/09/30 21:19:08 INFO dspy.teleprompt.gepa.gepa: Iteration 19: Selected program 2 score: 0.5842165898617512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.20 / 3 (73.5%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:56<00:00, 18.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:20:05 INFO dspy.evaluate.evaluate: Average Metric: 2.2043010752688175 / 3 (73.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_44 at: http://localhost:5005/#/experiments/1/runs/bb418565dd7f4a5789e01a9c5a6fe51c\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:20:51 INFO dspy.teleprompt.gepa.gepa: Iteration 19: Proposed new text for decide_info_collect.predict: You are to determine whether the provided `all_information` contains sufficient details to answer the `question` accurately. Follow these guidelines:\n",
      "\n",
      "1. **Exact Entity Matching**:\n",
      "   - Identify the specific entity in the question (e.g., \"Democratic Republic of Congo\" vs. \"Republic of the Congo\"). Use the exact name or abbreviation provided in the question.\n",
      "   - If the question references a location (e.g., \"county,\" \"city,\" \"country\"), ensure the answer matches the geographic scope (e.g., \"Washington, D.C.\" is a federal district, not a county; \"Washington County, Kansas\" is a distinct entity).\n",
      "\n",
      "2. **Political/Historical Precision**:\n",
      "   - For questions about leadership or historical events, verify the timeline and jurisdiction (e.g., independence dates, roles like \"first president\" or \"first leader\"). Avoid conflating similar-sounding entities (e.g., DRC and Republic of Congo).\n",
      "\n",
      "3. **Document Relevance**:\n",
      "   - Retrieve only documents that directly address the question's core elements. Avoid over-broad searches (e.g., \"Father's Day\" might retrieve irrelevant documents about unrelated topics like space centers).\n",
      "   - If multiple documents exist, prioritize those with the most specific and unambiguous information.\n",
      "\n",
      "4. **Citations**:\n",
      "   - Cite only the documents that directly support the answer. Exclude irrelevant or redundant documents (e.g., duplicate statements about the same fact).\n",
      "\n",
      "5. **Final Determination**:\n",
      "   - Set `has_collected_enough_info` to **True** only if the `all_information` explicitly contains the answer (e.g., \"Red River\" is in Hanoi, and the question asks about the river near Mai Van Hoa's birthplace).\n",
      "   - Set it to **False** if the information is incomplete, ambiguous, or addresses a different entity (e.g., Fulbert Youlou is linked to the Republic of Congo, but the question asks about the DRC).\n",
      "\n",
      "**Example Use Cases**:\n",
      "- If the question asks for the first leader of the **DRC**, but the information only discusses the **Republic of Congo**, the answer is **False**.\n",
      "- If the question asks for a **county** containing a president's work location, and the information mentions **Washington, D.C.** (a federal district), the answer is **False** unless a specific county is named.\n",
      "\n",
      "**Avoid Common Pitfalls**:\n",
      "- Confusing similar names (e.g., \"Washington County, Kansas\" vs. \"Washington, D.C.\").\n",
      "- Including irrelevant documents (e.g., mentioning Kennedy Space Center for a question about Father's Day).\n",
      "- Overlooking jurisdictional distinctions (e.g., federal vs. county-level entities).\n",
      "2025/09/30 21:22:33 INFO dspy.evaluate.evaluate: Average Metric: 2.2043010752688175 / 3 (73.5%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_45 at: http://localhost:5005/#/experiments/1/runs/c0e415e36b1e4749842b15e1a08b21cf\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:22:34 INFO dspy.teleprompt.gepa.gepa: Iteration 19: New subsample score is not better, skipping\n",
      "GEPA Optimization:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                      | 354/1765 [3:55:55<14:26:24, 36.84s/rollouts]2025/09/30 21:22:34 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Selected program 6 score: 0.5758294930875576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.41 / 2 (70.4%):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 2/3 [00:49<00:25, 25.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:30:35 INFO dspy.evaluate.evaluate: Average Metric: 2.327956989247312 / 3 (77.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_47 at: http://localhost:5005/#/experiments/1/runs/de0adf05ce8e406f9776c2c530acd9e3\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:47:46 INFO dspy.evaluate.evaluate: Average Metric: 17.873195084485403 / 30 (59.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_48 at: http://localhost:5005/#/experiments/1/runs/3dd5cc540c004879a23e12abc7b552b9\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Full valset score for new program: 0.5957731694828469\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Full train_val score for new program: 0.5957731694828469\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Individual valset scores for new program: [1.0, 0.26774193548387093, 0.9247311827956989, 0.4774193548387097, 0.39784946236559143, 1.0, 0.7849462365591396, 0.7365591397849461, 0.44193548387096776, 0.364516129032258, 0.3548387096774194, 0.3913978494623656, 0.4569892473118279, 0.9462365591397849, 1.0, 0.7741935483870968, 0.7741935483870968, 0.6612903225806451, 0.5124423963133641, 0.4569892473118279, 0.6370967741935484, 0.3870967741935484, 0.343010752688172, 0.5510752688172043, 0.6236559139784946, 0.3913978494623656, 0.5618279569892473, 0.6967741935483871, 0.6021505376344086, 0.3548387096774194]\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: New valset pareto front scores: [1.0, 0.26774193548387093, 0.9247311827956989, 0.914516129032258, 0.5967741935483871, 1.0, 0.8387096774193549, 0.8225806451612903, 0.6069124423963134, 0.4139784946236559, 0.5096774193548387, 0.4139784946236559, 0.5295698924731183, 0.9462365591397849, 1.0, 0.914516129032258, 0.7741935483870968, 0.6612903225806451, 0.5124423963133641, 0.4569892473118279, 0.6370967741935484, 0.914516129032258, 0.8118279569892473, 0.5510752688172043, 0.6236559139784946, 0.4032258064516129, 0.5919354838709677, 0.8119815668202764, 1.0, 0.8548387096774193]\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Full valset pareto front score: 0.7101664106502815\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Updated valset pareto front programs: [{8, 5, 7}, {1, 2, 3, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {5}, {5}, {8}, {1}, {0, 3, 4, 5}, {0, 3, 4}, {0, 2, 3, 4}, {3, 4}, {5}, {7}, {0, 3, 4, 5, 8}, {2, 5, 6, 7, 8}, {6}, {2, 5, 6, 7, 8}, {1, 2, 5, 6, 7, 8}, {8}, {8, 7}, {8, 2}, {6}, {0, 3, 4, 5}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {8, 5, 6}, {0, 3, 4}, {5, 7}, {2, 6, 7}, {1, 2, 5, 6, 7}, {2}]\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best valset aggregate score so far: 0.6306474490963166\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best program as per aggregate score on train_val: 5\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best program as per aggregate score on valset: 5\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best score on valset: 0.6306474490963166\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Best score on train_val: 0.6306474490963166\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: Linear pareto front program index: 5\n",
      "2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 20: New program candidate index: 8\n",
      "GEPA Optimization:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                   | 390/1765 [4:21:08<15:03:42, 39.43s/rollouts]2025/09/30 21:47:47 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Selected program 5 score: 0.6306474490963166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.29 / 3 (76.3%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:58<00:00, 39.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:49:45 INFO dspy.evaluate.evaluate: Average Metric: 2.290322580645161 / 3 (76.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_49 at: http://localhost:5005/#/experiments/1/runs/18e475d97f7943918af01bb43503fbb1\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:51:04 INFO dspy.teleprompt.gepa.gepa: Iteration 21: Proposed new text for decide_info_collect.predict: text\n",
      "You are to determine whether the `all_information` provided contains sufficient details to answer the `question`. Your task is to:  \n",
      "\n",
      "1. **Parse the Question**:  \n",
      "   - Break the question into **specific entities, relationships, or data points** (e.g., \"city where X was born\" â†’ identify \"city,\" \"X,\" and the relationship \"birthplace\").  \n",
      "   - Map each component to **explicit mentions** in `all_information`. If a component is not directly addressed, flag it as missing.  \n",
      "\n",
      "2. **Analyze `all_information`**:  \n",
      "   - **Check for explicit matches**: Only use information **directly stated** in `all_information` (e.g., if the question asks about \"Italy\" but `all_information` only mentions \"Europe,\" do not infer \"Italy\").  \n",
      "   - **Resolve conflicts**: Prioritize the **most specific or unambiguous source** (e.g., if a location is mentioned in two documents, prefer the one with higher specificity).  \n",
      "   - **Avoid assumptions**: Do not infer connections or locations beyond what is explicitly stated (e.g., if `all_information` states \"Renaissance began in Europe,\" do not assume it began in Italy unless explicitly mentioned).  \n",
      "\n",
      "3. **Generate Reasoning**:  \n",
      "   - **Link each question component to `all_information`**: For example, if the question asks about \"the owner of the gold spike,\" first confirm the gold spikeâ€™s owner is explicitly mentioned, then link to supporting citations.  \n",
      "   - **Chain information only if explicitly supported**: If the question requires multiple steps (e.g., \"alma mater of the gold spike owner\"), ensure each step is explicitly stated in `all_information` (e.g., \"gold spike owner = Tony Hsieh [6]; Tony Hsiehâ€™s alma mater = Harvard [3]\").  \n",
      "   - **Cite all relevant documents**: If a claim relies on multiple sources (e.g., location [3] and person [6]), include **all citations**. Omitting any relevant document reduces accuracy.  \n",
      "\n",
      "4. **Determine `has_collected_enough_info`**:  \n",
      "   - Set to **`True` only if**:  \n",
      "     - **All components of the question are explicitly addressed** in `all_information`.  \n",
      "     - **All necessary citations are included and correctly applied**.  \n",
      "   - Set to **`False` if**:  \n",
      "     - A **required entity or data point is missing** (e.g., the question asks for a specific mosaic, but `all_information` only provides the city).  \n",
      "     - **Ambiguities cannot be resolved** (e.g., conflicting sources without a clear resolution).  \n",
      "\n",
      "5. **Citation Precision**:  \n",
      "   - **List all relevant citations** for each claim (e.g., if a location is mentioned in [3] and a person in [6], both must be cited).  \n",
      "   - **Avoid irrelevant citations**: Only include documents that **directly support** the reasoning (e.g., exclude [16] in Example 3 if it does not address the question).  \n",
      "\n",
      "6. **Key Considerations**:  \n",
      "   - **Specificity over generality**: If `all_information` states \"Europe\" but the question asks for a country (e.g., \"Italy\"), the answer must be \"Europe\" unless \"Italy\" is explicitly mentioned.  \n",
      "   - **Avoid overgeneralization**: Do not assume a broader context (e.g., if `all_information` mentions \"al-Qarawiyin University in Morocco,\" do not assume Moroccoâ€™s border policies with Armenia unless explicitly stated).  \n",
      "   - **Redundant data**: Ignore duplicate entries (e.g., repeated mentions of Turkeyâ€™s border closure in Example 2).  \n",
      "\n",
      "**Examples of Critical Errors to Avoid**:  \n",
      "- **Example 3**: The assistant answered \"Europe\" based on `all_information` stating the Renaissance began in Europe. However, if the expected answer is \"Italy,\" this indicates the question requires **more specific information** (e.g., \"Italy\") that is **not present** in `all_information`. The assistant must not infer beyond the provided data.  \n",
      "- **Example 2**: The assistant missed citing [6] (which links al-Qarawiyin University to Morocco) and failed to retrieve [16], reducing citation accuracy. Always verify all documents that directly support the reasoning.  \n",
      "\n",
      "**Output Format**:  \n",
      "- `reasoning`: A step-by-step explanation of how each question component is addressed by `all_information`, with **explicit citations** for every claim.  \n",
      "- `has_collected_enough_info`: A boolean (`True`/`False`) based on whether **all required information is explicitly present** and **correctly cited**.\n",
      "2025/09/30 21:53:21 INFO dspy.evaluate.evaluate: Average Metric: 2.290322580645161 / 3 (76.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_50 at: http://localhost:5005/#/experiments/1/runs/34495c421b304ca79ea949f146c727d9\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:53:21 INFO dspy.teleprompt.gepa.gepa: Iteration 21: New subsample score is not better, skipping\n",
      "GEPA Optimization:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                                                   | 396/1765 [4:26:43<15:39:17, 41.17s/rollouts]2025/09/30 21:53:21 INFO dspy.teleprompt.gepa.gepa: Iteration 22: Selected program 6 score: 0.5758294930875576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.16 / 3 (72.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:59<00:00, 39.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:55:22 INFO dspy.evaluate.evaluate: Average Metric: 2.1586021505376345 / 3 (72.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸƒ View run eval_51 at: http://localhost:5005/#/experiments/1/runs/89fc9e0ff9ac49ab955ff92aad73d84d\n",
      "ðŸ§ª View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 21:56:10 INFO dspy.teleprompt.gepa.gepa: Iteration 22: Proposed new text for decide_info_collect.predict: You are tasked with determining whether the provided `all_information` contains sufficient and directly relevant data to answer the given `question`. Your output must include two components:  \n",
      "\n",
      "1. **Reasoning**:  \n",
      "   - Identify the key entities, relationships, and temporal/geographic links in the question (e.g., \"city where RSA Conference was held\" â†’ \"San Francisco\").  \n",
      "   - Map these entities to the `all_information` to verify if **all required data points** (e.g., dates, locations, historical events) are explicitly stated.  \n",
      "   - Check if **supporting documents** (e.g., [11], [13]) directly address the questionâ€™s components. Avoid citing irrelevant documents.  \n",
      "   - If the information is insufficient, explain which critical details are missing (e.g., \"Ladakh's integration into Qing China\" in Example 2).  \n",
      "\n",
      "2. **has_collected_enough_info**:  \n",
      "   - Output `True` **only if** all required data is present in `all_information` and the supporting documents are correctly cited.  \n",
      "   - Output `False` if critical gaps exist (e.g., missing entity links, dates, or events).  \n",
      "\n",
      "**Key Rules for Accuracy**:  \n",
      "- **Entity Resolution**: Ensure entities in the question (e.g., \"Al-Qanjarah\" â†’ \"Syria\") are explicitly resolved in `all_information`.  \n",
      "- **Temporal/Geographic Precision**: Verify that dates and locations in the answer are directly tied to the questionâ€™s context (e.g., \"634 CE\" in Example 3).  \n",
      "- **Document Relevance**: Prioritize documents that directly answer the question (e.g., [3] for invasion date in Example 3). Avoid including irrelevant documents (e.g., [15] in Example 3 feedback).  \n",
      "- **Citation Fidelity**: Only cite documents that **directly support** the answer (e.g., [11] and [13] in Example 1). Omit documents that are tangential or unrelated.  \n",
      "\n",
      "**Example Workflow**:  \n",
      "1. Parse the question into sub-questions (e.g., \"When was [X] founded?\" where X is the conference city).  \n",
      "2. Match each sub-question to `all_information` entries.  \n",
      "3. Confirm that all required information is explicitly stated and supported by the correct documents.  \n",
      "4. Validate that no irrelevant documents are cited.  \n",
      "\n",
      "**Critical Edge Cases**:  \n",
      "- If the question requires **chaining multiple facts** (e.g., \"country where X happened\" â†’ \"Y country\" â†’ \"Z event in Y\"), ensure all links are present in `all_information`.  \n",
      "- If the answer requires **inference** (e.g., deducing a date from context), mark `False` unless the inference is explicitly stated.  \n",
      "- Avoid assuming knowledge beyond `all_information` (e.g., do not infer Ladakhâ€™s Qing integration if not mentioned).\n"
     ]
    }
   ],
   "source": [
    "# Run GEPA optimization\n",
    "print(\"ðŸš€ Starting GEPA optimization...\")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_ds,\n",
    "    valset=val_ds,\n",
    ")\n",
    "\n",
    "print(\"âœ… GEPA optimization completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f130b47-19f7-496c-9c5c-618256ae63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program.save(str(EXP_DIR / \"optimized-program\"), save_program=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18cfef",
   "metadata": {},
   "source": [
    "### Examine Optimized Prompts\n",
    "\n",
    "Let's look at how GEPA improved the prompts for each predictor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f024bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred in optimized_program.named_predictors():\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Predictor: {name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Optimized Instructions:\")\n",
    "    print(pred.signature.instructions)\n",
    "    print(\"*\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124ea24",
   "metadata": {},
   "source": [
    "### Evaluate Optimized Program\n",
    "\n",
    "Compare the performance before and after GEPA optimization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ccf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\nðŸ“Š Evaluating OPTIMIZED program...\")\n",
    "# Evaluate optimized program  \n",
    "optimized_evaluate = dspy.Evaluate(\n",
    "    devset=test_ds,\n",
    "    metric=metric,\n",
    "    num_threads=8,\n",
    "    display_table=False,\n",
    "    display_progress=True\n",
    ")\n",
    "optimized_eval_result = optimized_evaluate(optimized_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229aecb-88f7-4c2c-b862-f05060d1be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ† PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original Program Score:  {original_eval_result.score:.3f}\")\n",
    "print(f\"Optimized Program Score: {optimized_eval_result.score:.3f}\")\n",
    "print(f\"Improvement:            {optimized_eval_result.score - original_eval_result.score:+.3f}\")\n",
    "print(f\"Relative Improvement:   {((optimized_eval_result.score / original_eval_result.score) - 1) * 100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf4598",
   "metadata": {},
   "source": [
    "### GEPA Optimization Analysis\n",
    "\n",
    "Analyze the detailed optimization results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba80a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GEPA optimization trajectory\n",
    "if hasattr(optimized_program, 'detailed_results'):\n",
    "    results = optimized_program.detailed_results\n",
    "    \n",
    "    print(\"ðŸ” GEPA Optimization Details:\")\n",
    "    print(f\"- Total candidates explored: {len(results.candidates)}\")\n",
    "    print(f\"- Best candidate index: {results.best_idx}\")\n",
    "    print(f\"- Best validation score: {results.val_aggregate_scores[results.best_idx]:.3f}\")\n",
    "    print(f\"- Discovery evaluations used: {sum(results.discovery_eval_counts)}\")\n",
    "    \n",
    "    # Show score progression\n",
    "    print(\"\\\\nðŸ“ˆ Score progression:\")\n",
    "    for i, score in enumerate(results.val_aggregate_scores[:10]):  # Show first 10\n",
    "        print(f\"Candidate {i}: {score:.3f}\")\n",
    "    \n",
    "    if len(results.val_aggregate_scores) > 10:\n",
    "        print(f\"... and {len(results.val_aggregate_scores) - 10} more candidates\")\n",
    "else:\n",
    "    print(\"Detailed results not available (set track_stats=True in GEPA constructor)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2255f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimized program on the same example\n",
    "example = test_ds[0]\n",
    "\n",
    "print(\"ðŸ§ª Testing optimized program on example:\")\n",
    "print(f\"Question: {example.question}\")\n",
    "print(f\"Expected Answer: {example.answer}\")\n",
    "print(f\"Supporting Docs: {example.supporting_ids}\")\n",
    "print()\n",
    "\n",
    "pred = program(example.question, example.docs)\n",
    "optimized_pred = optimized_program(example.question, example.docs)\n",
    "\n",
    "print(\"ðŸ“‹ ORIGINAL vs OPTIMIZED Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"ORIGINAL:\")\n",
    "print(f\"  Answer: {pred.answer}\")\n",
    "print(f\"  Retrieved docs: {pred.retrieved_doc_ids}\")\n",
    "print(f\"  Cited docs: {pred.citations}\")\n",
    "\n",
    "print(\"OPTIMIZED:\")\n",
    "print(f\"  Answer: {optimized_pred.answer}\")\n",
    "print(f\"  Retrieved docs: {optimized_pred.retrieved_doc_ids}\")\n",
    "print(f\"  Cited docs: {optimized_pred.citations}\")\n",
    "\n",
    "print(\"ðŸŽ¯ Metric Comparison:\")\n",
    "original_metric_result = metric_with_feedback(example, pred)\n",
    "optimized_metric_result = metric_with_feedback(example, optimized_pred)\n",
    "print(f\"Original score: {original_metric_result.score:.3f}\")\n",
    "print(f\"Original feedback: {original_metric_result.feedback}\")\n",
    "print()\n",
    "print(f\"Optimized score: {optimized_metric_result.score:.3f}\")\n",
    "print(f\"Optimized feedback: {optimized_metric_result.feedback}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff3a39-8ff5-4be5-bbf6-0a57f14ef517",
   "metadata": {},
   "source": [
    "Can we measure instruction quality by using them with a larger model to see if it gets questions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c762b-5842-4396-af06-a99bee74577e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
