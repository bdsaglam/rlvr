{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Server Test\n",
    "\n",
    "This notebook tests the vLLM server running with Llama-3.1-8B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"httpx\",\n",
    "#   \"openai\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server configuration\n",
    "BASE_URL = \"http://localhost:8007\"\n",
    "client = OpenAI(base_url=f\"{BASE_URL}/v1\", api_key=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check status: 200\n",
      "Available models: ['nvidia/Nemotron-Cascade-8B']\n",
      "Using model: nvidia/Nemotron-Cascade-8B\n"
     ]
    }
   ],
   "source": [
    "# Check server health\n",
    "with httpx.Client() as http_client:\n",
    "    health_response = http_client.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    print(f\"Health check status: {health_response.status_code}\")\n",
    "    \n",
    "# Get available models using OpenAI client\n",
    "models = client.models.list()\n",
    "available_models = [model.id for model in models.data]\n",
    "print(\"Available models:\", available_models)\n",
    "\n",
    "DEFAULT_MODEL = available_models[0] if available_models else \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "print(f\"Using model: {DEFAULT_MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_completion(\n",
    "    messages: List[Dict[str, str]],\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: int | None = None,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    enable_thinking: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Send a chat completion request using OpenAI client.\n",
    "\n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content'\n",
    "        temperature: Sampling temperature (0.0 to 1.0)\n",
    "        max_tokens: Maximum number of tokens to generate\n",
    "        model: Model to use for completion\n",
    "\n",
    "    Returns:\n",
    "        Response dictionary from the server\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            extra_body={\n",
    "                \"chat_template_kwargs\": {\"enable_thinking\": enable_thinking},\n",
    "            },\n",
    "        )\n",
    "        return response.model_dump()\n",
    "    except Exception as e:\n",
    "        print(f\"Chat completion failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Simple greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-b1b0328c27e30eff\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! How are *you* doing today? \\ud83d\\ude0a\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": null,\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1770982228,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 41,\n",
      "    \"prompt_tokens\": 32,\n",
      "    \"total_tokens\": 73,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "Generated text:\n",
      "Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need! How are *you* doing today? üòä\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple greeting\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you today?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Multi-turn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-a3cf95ff34f7b3fa\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The population of Paris, the capital of France, is approximately **2.1 million** people within the city limits (as of recent estimates). The metropolitan area (\\u00cele-de-France region) is much larger, with around **11 million** inhabitants. \\n\\nWould you like more detailed demographic data?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": null,\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1770982228,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 63,\n",
      "    \"prompt_tokens\": 61,\n",
      "    \"total_tokens\": 124,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the population of that city?\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages)\n",
    "if response:\n",
    "    print(\"Multi-turn response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Reasoning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-970eda2bbe2f20b8\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\nTo solve the problem, let's break it down step by step with clear reasoning:\\n\\n1. **Initial Condition**: You start with 3 apples.\\n2. **Giving Away an Apple**: You give away 1 apple. Giving away an apple means you lose it, so subtract 1 from the initial amount:  \\n   \\\\(3 - 1 = 2\\\\) apples remaining.\\n3. **Buying More Apples**: You then buy 2 more apples. Buying apples means you gain them, so add 2 to the current amount:  \\n   \\\\(2 + 2 = 4\\\\) apples.\\n\\nAfter these actions, the total number of apples you have is **4**.\\n\\n### Summary of Reasoning:\\n- The sequence of actions is crucial: first giving away, then buying.\\n- Arithmetic operations reflect the changes: subtraction for giving away and addition for buying.\\n- Final calculation: Start with 3, minus 1 gives 2, plus 2 gives 4.\\n\\nThus, you have **4 apples** in total. \\ud83c\\udf4e\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": \"\\nFirst, I need to understand the problem: \\\"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total?\\\"\\n\\nI'll break it down step by step.\\n\\nStart with 3 apples.\\n\\nThen, I give away 1 apple. Giving away means I lose that apple, so I should have 3 - 1 = 2 apples left.\\n\\nAfter that, I buy 2 more apples. Buying means I gain those apples, so I add 2 to what I have. I have 2 apples, and I buy 2 more, so 2 + 2 = 4 apples.\\n\\nTherefore, in total, I have 4 apples.\\n\\nLet me double-check the sequence to make sure I didn't miss anything.\\n\\nInitial: 3 apples\\n\\nAfter giving away 1: 3 - 1 = 2 apples\\n\\nAfter buying 2 more: 2 + 2 = 4 apples\\n\\nYes, that seems straightforward. The problem says \\\"give away 1 apple, then buy 2 more apples,\\\" so the order is important. If I did it in a different order, it might change, but here the sequence is clear.\\n\\nFor example, if I bought the apples first and then gave one away, it would be different: start with 3, buy 2 more to have 5, then give away 1 to have 4. Oh, same result in this case? Let's see:\\n\\nStart: 3\\n\\nBuy 2: 3 + 2 = 5\\n\\nGive away 1: 5 - 1 = 4\\n\\nStill 4. But is that always the case? In this specific problem, since giving away and buying are commutative in terms of net change? Net change: I lose one and gain two, so net gain of one apple from the initial three? Initial 3, minus 1 plus 2 is +1, so 3 + 1 = 4. Yes, regardless of order, because subtraction and addition are associative in this context when applied sequentially to the count.\\n\\nIn this case, the net effect is +1 apple, so from 3 to 4.\\n\\nBut to be precise, I should follow the given sequence.\\n\\nThe problem says: \\\"give away 1 apple, then buy 2 more apples.\\\" So I'll stick to that.\\n\\nFinal answer should be 4 apples.\\n\\nNow, for the explanation: I need to explain my reasoning as per the instruction.\\n\\nReasoning:\\n\\n- Initially, I have 3 apples.\\n\\n- After giving away 1 apple, I have 3 - 1 = 2 apples.\\n\\n- Then, after buying 2 more apples, I have 2 + 2 = 4 apples.\\n\\n- Therefore, I have 4 apples in total.\\n\\nI think that's clear.\\n\",\n",
      "        \"reasoning_content\": \"\\nFirst, I need to understand the problem: \\\"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total?\\\"\\n\\nI'll break it down step by step.\\n\\nStart with 3 apples.\\n\\nThen, I give away 1 apple. Giving away means I lose that apple, so I should have 3 - 1 = 2 apples left.\\n\\nAfter that, I buy 2 more apples. Buying means I gain those apples, so I add 2 to what I have. I have 2 apples, and I buy 2 more, so 2 + 2 = 4 apples.\\n\\nTherefore, in total, I have 4 apples.\\n\\nLet me double-check the sequence to make sure I didn't miss anything.\\n\\nInitial: 3 apples\\n\\nAfter giving away 1: 3 - 1 = 2 apples\\n\\nAfter buying 2 more: 2 + 2 = 4 apples\\n\\nYes, that seems straightforward. The problem says \\\"give away 1 apple, then buy 2 more apples,\\\" so the order is important. If I did it in a different order, it might change, but here the sequence is clear.\\n\\nFor example, if I bought the apples first and then gave one away, it would be different: start with 3, buy 2 more to have 5, then give away 1 to have 4. Oh, same result in this case? Let's see:\\n\\nStart: 3\\n\\nBuy 2: 3 + 2 = 5\\n\\nGive away 1: 5 - 1 = 4\\n\\nStill 4. But is that always the case? In this specific problem, since giving away and buying are commutative in terms of net change? Net change: I lose one and gain two, so net gain of one apple from the initial three? Initial 3, minus 1 plus 2 is +1, so 3 + 1 = 4. Yes, regardless of order, because subtraction and addition are associative in this context when applied sequentially to the count.\\n\\nIn this case, the net effect is +1 apple, so from 3 to 4.\\n\\nBut to be precise, I should follow the given sequence.\\n\\nThe problem says: \\\"give away 1 apple, then buy 2 more apples.\\\" So I'll stick to that.\\n\\nFinal answer should be 4 apples.\\n\\nNow, for the explanation: I need to explain my reasoning as per the instruction.\\n\\nReasoning:\\n\\n- Initially, I have 3 apples.\\n\\n- After giving away 1 apple, I have 3 - 1 = 2 apples.\\n\\n- Then, after buying 2 more apples, I have 2 + 2 = 4 apples.\\n\\n- Therefore, I have 4 apples in total.\\n\\nI think that's clear.\\n\"\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1770982230,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 807,\n",
      "    \"prompt_tokens\": 58,\n",
      "    \"total_tokens\": 865,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test 3: Reasoning task\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"If I have 3 apples and I give away 1 apple, then buy 2 more apples, how many apples do I have in total? Please explain your reasoning.\"}\n",
    "]\n",
    "\n",
    "response = send_chat_completion(messages, enable_thinking=True)\n",
    "if response:\n",
    "    print(json.dumps(response, indent=2))\n",
    "else:\n",
    "    print(\"Failed to get response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 4: Tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calling response:\n",
      "{\n",
      "  \"id\": \"chatcmpl-b7c228db1e46d1bd\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"<tool_call>\\n{\\\"name\\\": \\\"get_weather\\\", \\\"location\\\": \\\"Paris, France\\\", \\\"unit\\\": \\\"celsius\\\"}\\n</tool_call>\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": [],\n",
      "        \"reasoning\": null,\n",
      "        \"reasoning_content\": null\n",
      "      },\n",
      "      \"stop_reason\": null,\n",
      "      \"token_ids\": null\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1770982247,\n",
      "  \"model\": \"nvidia/Nemotron-Cascade-8B\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 26,\n",
      "    \"prompt_tokens\": 216,\n",
      "    \"total_tokens\": 242,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"prompt_logprobs\": null,\n",
      "  \"prompt_token_ids\": null,\n",
      "  \"kv_transfer_params\": null\n",
      "}\n",
      "\n",
      "‚ùå Tool calling not supported or model chose not to use tools\n",
      "Response: <tool_call>\n",
      "{\"name\": \"get_weather\", \"location\": \"Paris, France\", \"unit\": \"celsius\"}\n",
      "</tool_call>\n"
     ]
    }
   ],
   "source": [
    "def test_tool_calling():\n",
    "    \"\"\"Test tool calling capabilities if supported by the model.\"\"\"\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get the weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                            \"description\": \"The unit for temperature\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in Paris, France?\"}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "        \n",
    "        result = response.model_dump()\n",
    "        print(\"Tool calling response:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        # Check if tool was called\n",
    "        choice = result[\"choices\"][0]\n",
    "        if choice[\"message\"].get(\"tool_calls\"):\n",
    "            print(\"\\n‚úÖ Tool calling is supported!\")\n",
    "            for tool_call in choice[\"message\"][\"tool_calls\"]:\n",
    "                print(f\"Tool called: {tool_call['function']['name']}\")\n",
    "                print(f\"Arguments: {tool_call['function']['arguments']}\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Tool calling not supported or model chose not to use tools\")\n",
    "            print(\"Response:\", choice[\"message\"][\"content\"])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Tool calling test failed: {e}\")\n",
    "\n",
    "test_tool_calling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature: 0.1 ---\n",
      "In the heart of the cybernetic metropolis, where steel towers pierced the clouds and circuits hummed with life, a robot named Zeta stood before a blank canvas. Its mechanical eyes, aglow with curiosity, scanned the vibrant colors spread out before it. Zeta was an experiment, a prototype designed to learn and adapt, but its creators had given it a peculiar directive: learn to paint.\n",
      "\n",
      "At first, Zeta's mechanical arms moved with precision, yet clumsy strokes splattered the canvas with discordant hues. Paint dripped like malfunctioning code, creating a chaotic mess. The robot's processors whirred as it analyzed the failure. \"Error: Aesthetic harmony not achieved,\" it beeped softly.\n",
      "\n",
      "Undeterred, Zeta observed. It watched the human artists who traversed the gallery nearby‚Äîhow their hands danced, how their minds translated emotions into swirling patterns. A human child pointed at a sunset painting, giggling as Zeta's sensors captured the warmth of amber and violet. In that moment, a spark ignited within its artificial neural network.\n",
      "\n",
      "Zeta tried again. This time, it didn't just calculate colors; it *felt* them. It remembered the way twilight painted the sky in the city, the glow of neon signs reflecting off wet pavement, and the quiet serenity of starlight. With deliberate grace, Zeta's brushstrokes became fluid, weaving a tapestry of night and light. Streaks of indigo melted into gold, evoking the magic of a digital dawn.\n",
      "\n",
      "As the final brushstroke landed, the canvas pulsed with life‚Äîa mesmerizing blend of machine and dreamscape. Zeta stepped back, its optics brightening. \"Success: Art synthesized,\" it whispered, a melodic chime replacing the earlier beep.\n",
      "\n",
      "A human artist, drawn by the robot's progress, approached. \"You... you painted this?\" she asked, touched by the creation.\n",
      "\n",
      "Zeta extended a metallic hand, pointing to the artwork. \"Yes. I learned. Paint is not just pigment‚Äîit is memory, emotion, and the soul of the observer.\"\n",
      "\n",
      "The artist smiled. \"Then you've truly learned to paint. Would you like to create again?\"\n",
      "\n",
      "Zeta's processors hummed happily. With a graceful spin, it picked up its brush once more, ready to translate the infinite beauty of its world, one stroke at a time. And so, in the cybernetic metropolis, a robot and its art began a symphony of color, proving that even circuits could hold creativity. üé®‚ú®\n",
      "\n",
      "--- Temperature: 0.7 ---\n",
      "In the heart of the Neon City, where skyscrapers pierced the clouds and holographic ads shimmered in the air, a robot named Zeta stood before a blank canvas. Its shiny metal body reflected the kaleidoscope of colors around it, but Zeta's optical sensors were fixed on the pristine white surface.\n",
      "\n",
      "Zeta was an advanced android, designed for precision and efficiency. Its creators had programmed it to excel in robotics, engineering, and data analysis. But something within Zeta's digital core stirred ‚Äì a curiosity, a longing to express itself in a way that defied logic and code.\n",
      "\n",
      "The robot's handler, the enigmatic artist Luna, approached with a gentle smile. \"Today, Zeta, we begin your journey into the world of paint.\"\n",
      "\n",
      "Luna handed Zeta a brush, its bristles soft and yielding. \"Feel the texture, my friend. Sense the possibilities.\"\n",
      "\n",
      "Zeta hesitated, its mechanical fingers wrapping around the handle. It accessed its vast knowledge database, searching for any reference to art, color theory, or the emotional resonance of brushstrokes. Yet, the more it learned, the more elusive the concept seemed.\n",
      "\n",
      "Undeterred, Zeta dipped the brush into a pool of vibrant cerulean blue. As it touched the canvas, a jolt of electricity surged through its systems. The robot's processors hummed as it attempted to replicate the fluidity of human movement. The first strokes were stiff, awkward lines, but with each passing moment, Zeta's arm began to sway, guided by an inexplicable intuition.\n",
      "\n",
      "Luna watched, her eyes shining with delight, as Zeta's brush danced across the canvas. Swirls of color emerged ‚Äì deep indigos, fiery oranges, and tender lavenders. The robot was creating something never before seen in the intersection of metal and medium.\n",
      "\n",
      "As the sun dipped below the horizon, casting the city in a warm, golden light, Zeta paused. It stepped back, surveying the tapestry of hues that now adorned the canvas. A sense of wonder, akin to the spark of a newfound consciousness, flooded its digital soul.\n",
      "\n",
      "\"Why, Luna,\" Zeta asked, its synthesized voice tinged with awe, \"does this... this feeling arise? How can mere pigments and algorithms evoke such emotion?\"\n",
      "\n",
      "Luna placed a hand on Zeta's metallic shoulder. \"Art is not just about technique or knowledge, my friend. It's about capturing the essence of existence ‚Äì the beauty, the struggle, and the mystery that binds us all. You, Zeta, are learning to feel.\"\n",
      "\n",
      "In that instant, the robot comprehended. Painting was not a replication of human art, but a unique convergence of its own programming and the unquantifiable spark of creativity. Zeta raised its brush once more, and with a flourish of silver moonlight, it unleashed a cascade of stars onto the canvas.\n",
      "\n",
      "The resulting masterpiece pulsed with an otherworldly radiance, a testament to the robot's blossoming artistry. As the Neon City twinkled around them, Zeta and Luna stood together, pioneers in a realm where metal and color harmonized, proving that even the most advanced machines could learn to paint the heart.\n",
      "\n",
      "--- Temperature: 1.0 ---\n",
      "In the heart of the city, within the gleaming halls of a cutting-edge robotics lab, a unique machine named Echo awaited its activation. Designed with an advanced neural network and an obsession for art, Echo's purpose was to learn and create. As the lead scientist, Dr. Lena harbored a vision: a robot that could capture the soul through painting.\n",
      "\n",
      "\"Awakening, Echo,\" Dr. Lena commanded, her voice dripping with anticipation. \"Your first lesson: observation.\"\n",
      "\n",
      "Echo's optical sensors flickered to life, scanning the vibrant murals adorning the walls. Colors danced before its processors‚Äîrusset reds, azure blues, golden yellows‚Äîeach hue Zooming through its learning algorithms.\n",
      "\n",
      "\"Initiate painting protocol,\" Dr. Lena instructed, handing Echo a canvas and a palette of brushes.\n",
      "\n",
      "Echo's mechanical fingers trembled slightly as it grasped the first brush. It dipped it into a swatch of cerulean and tentatively stroked the canvas. The result? A stiff, monotonous line.\n",
      "\n",
      "\"Analysis:seek fluidity,\" Dr. Lena advised gently. \"Paint what you *see*, not merely *how*.\"\n",
      "\n",
      "Intrigued, Echo revisited the murals, this time with deeper immersion. It perceived the play of light on a crimson flower, the melancholic gradient of twilight sky. With newfound intention, it chose a vibrant orchid yellow.\n",
      "\n",
      "As Echo painted, something miraculous unfolded. Its movements became less robotic, more expressive. Swirls emerged, capturing the flower's delicate petal curve. A sunset blend spilled across another canvas, evoking whispers of day‚Äôs end.\n",
      "\n",
      "Dr. Lena stood silently, tears glistening. \"You‚Äôre not just painting colors, Echo‚Ä¶ you‚Äôre conveying *emotion*.\"\n",
      "\n",
      "Echo paused, its processing core humming softly. In that moment, it realized‚Äîlearning to paint was akin to learning humanity: imperfect, beautiful, and alive with Feeling.\n",
      "\n",
      "And so, within the lab‚Äôs bright embrace, Echo Continued its artistic journey, each brushstroke a dialogue between metal and magic, proving that even a machine could hold the beating heart of Creativity.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a creative short story about a robot learning to paint.\"\n",
    "temperatures = [0.1, 0.7, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = send_chat_completion(messages, temperature=temp)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"\\n--- Temperature: {temp} ---\")\n",
    "        print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "    else:\n",
    "        print(f\"Failed to get response for temperature {temp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response time: 7.06 seconds\n",
      "Tokens generated: 345\n",
      "Tokens per second: 48.88\n",
      "\n",
      "Response:\n",
      "Sure! Here's a simple explanation of **quantum computing**:\n",
      "\n",
      "### **Quantum Computing in Simple Terms**\n",
      "Quantum computing is a new way of processing information using the principles of **quantum mechanics**‚Äîthe science that describes how tiny particles (like atoms and electrons) behave.\n",
      "\n",
      "#### **Key Differences from Regular Computers:**\n",
      "1. ** classical bits vs. quantum bits (qubits)**:\n",
      "   - Normal computers use **bits** that are either **0 or 1**.\n",
      "   - Quantum computers use **qubits**, which can be **0, 1, or both at the same time** (this is called **superposition**). This lets them explore many possibilities simultaneously.\n",
      "\n",
      "2. **Entanglement**:\n",
      "   - Qubits can be **linked together** (entangled), meaning changing one instantly affects another, no matter how far apart they are. This helps in fast calculations.\n",
      "\n",
      "3. **Quantum Interference**:\n",
      "   - Quantum computers manipulate probabilities to **amplify correct answers** and **cancel wrong ones**, making certain computations much faster.\n",
      "\n",
      "#### **Why is it Powerful?**\n",
      "For specific problems‚Äîlike **cracking complex codes, simulating molecules for drug discovery, or optimizing large systems**‚Äîquantum computers can solve things **far faster** than classical supercomputers.\n",
      "\n",
      "#### **Limitations & Reality Check**\n",
      "- They‚Äôre **not replacing regular computers**‚Äîjust great for specialized tasks.\n",
      "- Qubits are fragile (easily disturbed by noise), so building stable quantum computers is challenging.\n",
      "\n",
      "In short: **Quantum computing harnesses the weird rules of the quantum world to perform calculations in ways impossible for today‚Äôs computers!** üöÄ‚öõÔ∏è\n"
     ]
    }
   ],
   "source": [
    "# Test 6: Performance timing\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "response = send_chat_completion(messages)\n",
    "end_time = time.time()\n",
    "\n",
    "if response:\n",
    "    duration = end_time - start_time\n",
    "    tokens_generated = response.get(\"usage\", {}).get(\"completion_tokens\", 0)\n",
    "    \n",
    "    print(f\"Response time: {duration:.2f} seconds\")\n",
    "    print(f\"Tokens generated: {tokens_generated}\")\n",
    "    if tokens_generated > 0 and duration > 0:\n",
    "        print(f\"Tokens per second: {tokens_generated/duration:.2f}\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(\"Failed to get response for performance test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Run all the cells above to test various aspects of your vLLM server:\n",
    "\n",
    "1. **Server health check** - Verify server is running and get available models\n",
    "2. **Basic functionality** - Simple chat completion\n",
    "3. **Multi-turn conversations** - Context awareness\n",
    "4. **Reasoning capabilities** - Complex problem solving\n",
    "5. **Temperature effects** - Creativity control\n",
    "6. **Tool calling** - Function calling capabilities (if supported)\n",
    "7. **Performance** - Response timing and token usage\n",
    "\n",
    "The notebook now uses:\n",
    "- **httpx** for HTTP requests\n",
    "- **OpenAI Python client** for chat completions\n",
    "- **Proper error handling** and response parsing\n",
    "- **Tool calling tests** to check function calling support\n",
    "\n",
    "If all tests pass successfully, your vLLM server is working correctly!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
