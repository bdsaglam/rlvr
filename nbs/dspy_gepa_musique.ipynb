{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3483a25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bae087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import dspy\n",
    "from vf_musique.data import prepare_dataset\n",
    "from vf_musique.metrics import exact_match, f1\n",
    "from vf_musique.rewards import extract_all_retrieved_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2de384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLflow tracking enabled at http://localhost:5005\n"
     ]
    }
   ],
   "source": [
    "def setup_mlflow():\n",
    "    import mlflow\n",
    "    import mlflow.dspy\n",
    "\n",
    "    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n",
    "    mlflow.set_experiment(\"dspy-gepa-musique\")\n",
    "    mlflow.dspy.autolog(\n",
    "        log_compiles=True,\n",
    "        log_evals=True,\n",
    "        log_traces_from_compile=True,\n",
    "    )\n",
    "    print(f\"✅ MLflow tracking enabled at {os.getenv('MLFLOW_TRACKING_URI')}\")\n",
    "\n",
    "setup_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c131983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../outputs/dspy-gepa-musique/20251002_131311')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXP_ID = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = Path(f\"../outputs/dspy-gepa-musique/{EXP_ID}\")\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2015c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/Qwen/Qwen3-8B\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=8192,\n",
    "    api_key=\"local\",\n",
    "    api_base=\"http://0.0.0.0:8000/v1\",\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "reflection_lm = dspy.LM(\"gemini/gemini-2.5-pro\", api_key=os.getenv(\"GEMINI_API_KEY\"), max_tokens=16384)\n",
    "# reflection_lm = dspy.LM(\n",
    "#     \"openai/Qwen/Qwen3-32B\",\n",
    "#     temperature=0.6,\n",
    "#     max_tokens=16384,\n",
    "#     api_key=\"local\",\n",
    "#     api_base=\"http://0.0.0.0:8001/v1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c161bc-d491-4e23-8c5a-7723d9f8296e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nHello! 😊 How can I assist you today?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-8a17c5dc0ffe228842ad3dd8e9e7f517&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-8a17c5dc0ffe228842ad3dd8e9e7f517)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b197cd14-2b0a-494c-948f-51317b3ba963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The largest prime number below 10 is **7**.\\n\\nThe prime numbers below 10 are 2, 3, 5, and 7.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-b1ab281cf0d7f7978957ddf5bd11b660&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-b1ab281cf0d7f7978957ddf5bd11b660)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reflection_lm(messages=[{\"role\": \"user\", \"content\": \"What is largest prime number below 10?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8140bb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428f73de406b40a386e891f4a3a287ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 300/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78751a1bf24c4bcbb5b0604747fe5c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd364c4fccb45f6b2dbe111661fabcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map: 100%|##########| 50/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773d349e858d4ef8aa943c15577fe3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def prepare_musique_dataset(datasets_str: str = \"bdsaglam/musique,answerable,train\", noise_rate: float = 1.0):\n",
    "    \"\"\"Load and prepare MuSiQue dataset using vf_musique data functions.\"\"\"\n",
    "    # Use the official vf_musique data preparation\n",
    "    dataset = prepare_dataset(datasets_str, noise_rate=noise_rate)\n",
    "\n",
    "    # Convert to DSPy examples\n",
    "    processed_examples = []\n",
    "    for x in dataset:\n",
    "        # Get supporting document IDs\n",
    "        supporting_doc_ids = [doc[\"id\"] for doc in x[\"info\"][\"docs\"] if doc.get(\"is_supporting\")]\n",
    "\n",
    "        # Create DSPy example\n",
    "        example = dspy.Example(\n",
    "            question=x[\"question\"],\n",
    "            answer=x[\"answer\"],\n",
    "            answers=x[\"info\"][\"answers\"],  # All valid answer forms\n",
    "            docs=x[\"info\"][\"docs\"],  # All documents\n",
    "            supporting_ids=supporting_doc_ids,  # IDs of supporting docs\n",
    "            n_hops=x[\"info\"][\"n_hops\"],  # Number of hops\n",
    "        ).with_inputs(\"question\", \"docs\")\n",
    "\n",
    "        processed_examples.append(example)\n",
    "\n",
    "    return processed_examples\n",
    "\n",
    "ds = prepare_musique_dataset(datasets_str=\"bdsaglam/musique-mini,answerable,train\", noise_rate=1.0)\n",
    "random.Random(89).shuffle(ds)\n",
    "train_size = int(len(ds)*0.60)\n",
    "train_ds, val_ds = ds[:train_size], ds[train_size:]\n",
    "test_ds = prepare_musique_dataset(datasets_str=\"bdsaglam/musique-mini,answerable,validation[:50]\", noise_rate=1.0)\n",
    "\n",
    "train_ds = train_ds[:30]\n",
    "val_ds = val_ds[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e75064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from agents import RunContextWrapper\n",
    "from vf_musique.tools import make_retrieve_tool, ToolContext\n",
    "\n",
    "\n",
    "class GenerateSearchQuery(dspy.Signature):\n",
    "    \"\"\"Given a multi-hop question and information collected so far, generate a search query\n",
    "    to find the next piece of information needed to answer the question.\n",
    "    Focus on entities, dates, or facts that need to be resolved step by step.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    collected_info: str = dspy.InputField(desc=\"Information collected from previous retrieval steps\")\n",
    "    search_query: str = dspy.OutputField(desc=\"Search query for the next retrieval step\")\n",
    "    top_n: int = dspy.OutputField(desc=\"Number of documents to retrieve. 1 <= top_n <= 3\")\n",
    "\n",
    "class KeyInformation(BaseModel):\n",
    "    info: str \n",
    "    source_doc_id: str\n",
    "\n",
    "    def format(self):\n",
    "        return f\"{self.info}[{self.source_doc_id}]\"\n",
    "\n",
    "class ExtractInformation(dspy.Signature):\n",
    "    \"\"\"Given a question and retrieved documents, extract the key information\n",
    "    that helps answer the question or leads to the next retrieval step.\n",
    "    Focus on entities, relationships, dates, and facts.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    documents: str = dspy.InputField(desc=\"Retrieved documents from search\")\n",
    "    key_informations: list[KeyInformation] = dspy.OutputField(desc=\"Key information(s) extracted from retrieved document(s)\")\n",
    "\n",
    "class DecideInfoCollection(dspy.Signature):\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    all_information: str = dspy.InputField(desc=\"All information collected during retrieval\")\n",
    "    has_collected_enough_info: bool = dspy.OutputField(desc=\"Has enough information been collected to answer question?\")\n",
    "\n",
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Given a multi-hop question and all collected information, provide a concise answer.\n",
    "    The answer should directly address what the question asks for.\n",
    "    Be specific and use the exact entities/dates/facts from the documents.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField(desc=\"The multi-hop question to answer\")\n",
    "    all_information: str = dspy.InputField(desc=\"All information collected during retrieval\")\n",
    "    answer: str = dspy.OutputField(desc=\"Final answer to the question\")\n",
    "    citations: list[str] = dspy.OutputField(desc=\"List of document IDs cited for the answer, e.g. `[4,9]`\")\n",
    "\n",
    "\n",
    "class MultiHopQA(dspy.Module):\n",
    "    \"\"\"Multi-hop question answering module for MuSiQue.\"\"\"\n",
    "\n",
    "    def __init__(self, retriever_name: str = \"hybrid\", max_iter: int = 5):\n",
    "        self.retriever_name = retriever_name\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        # Create the retrieve tool\n",
    "        self.retrieve_tool = make_retrieve_tool(retriever_name, default_top_n=2)\n",
    "\n",
    "        # Create modules with typed signatures\n",
    "        self.generate_query = dspy.ChainOfThought(GenerateSearchQuery)\n",
    "        self.extract_info = dspy.ChainOfThought(ExtractInformation)\n",
    "        self.decide_info_collect = dspy.ChainOfThought(DecideInfoCollection)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def forward(self, question: str, docs: list, **kwargs) -> dspy.Prediction:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-hop QA.\n",
    "\n",
    "        Args:\n",
    "            question: The multi-hop question to answer\n",
    "            docs: List of documents available for retrieval\n",
    "        \"\"\"\n",
    "        collected_info = []\n",
    "        retrieved_doc_ids = []\n",
    "\n",
    "        # Create a context object that mimics the verifiers tool environment\n",
    "        run_context_wrapper = RunContextWrapper[ToolContext](context=ToolContext(info=dict(docs=docs)))\n",
    "\n",
    "        for hop_idx in range(self.max_iter):\n",
    "            # Generate search query\n",
    "            if hop_idx == 0:\n",
    "                # First hop: use the original question\n",
    "                query = question\n",
    "                top_n = 2\n",
    "            else:\n",
    "                # Subsequent hops: generate query based on collected info\n",
    "                query_pred = self.generate_query(\n",
    "                    question=question,\n",
    "                    collected_info=\"\\n\".join([item.format() for item in collected_info]) if collected_info else \"No information collected yet\",\n",
    "                )\n",
    "                query = query_pred.search_query\n",
    "                top_n = max(min(query_pred.top_n, 3), 1)\n",
    "\n",
    "            # Retrieve documents using the MuSiQue retrieve tool\n",
    "            retrieved_text = self.retrieve_tool(run_context_wrapper, query=query, top_n=top_n)\n",
    "\n",
    "            # Extract document IDs from retrieved text using the official function\n",
    "            doc_ids = extract_all_retrieved_doc_ids(retrieved_text)\n",
    "            for doc_id in doc_ids:\n",
    "                if doc_id not in retrieved_doc_ids:\n",
    "                    retrieved_doc_ids.append(doc_id)\n",
    "\n",
    "            # Extract key information from retrieved documents\n",
    "            info_pred = self.extract_info(question=question, documents=retrieved_text)\n",
    "            collected_info.extend(info_pred.key_informations)\n",
    "\n",
    "            decision_pred = self.decide_info_collect(question=question, all_information=\"\\n\".join([item.format() for item in collected_info]))        \n",
    "            if decision_pred.has_collected_enough_info:\n",
    "                break\n",
    "\n",
    "        # Generate final answer based on all collected information\n",
    "        answer_pred: GenerateAnswer = self.generate_answer(question=question, all_information=\"\\n\".join([item.format() for item in collected_info]))\n",
    "\n",
    "        return dspy.Prediction(\n",
    "            answer=answer_pred.answer,\n",
    "            collected_info=collected_info,\n",
    "            retrieved_doc_ids=retrieved_doc_ids,\n",
    "            citations=answer_pred.citations,\n",
    "            n_turns = hop_idx + 1,\n",
    "        )\n",
    "\n",
    "program = MultiHopQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "502bfc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': \"What county contains the work location of the president making father's day a national holiday?\", 'answer': 'Washington County', 'answers': ['washington county', 'Washington County'], 'docs': [{'body': 'Thanksgiving, or Thanksgiving Day, is a public holiday celebrated on the fourth Thursday of November in the United States. It originated as a harvest festival. Thanksgiving has been celebrated nationally on and off since 1789, after Congress requested a proclamation by George Washington. It has been celebrated as a federal holiday every year since 1863, when, during the American Civil War, President Abraham Lincoln proclaimed a national day of \"Thanksgiving and Praise to our beneficent Father who dwelleth in the Heavens,\"to be celebrated on the last Thursday in November. Together with Christmas and the New Year, Thanksgiving is a part of the broader fall / winter holiday season in the U.S.', 'id': '0', 'is_supporting': False, 'text': '# Thanksgiving (United States)\\nThanksgiving, or Thanksgiving Day, is a public holiday celebrated on the fourth Thursday of November in the United States. It originated as a harvest festival. Thanksgiving has been celebrated nationally on and off since 1789, after Congress requested a proclamation by George Washington. It has been celebrated as a federal holiday every year since 1863, when, during the American Civil War, President Abraham Lincoln proclaimed a national day of \"Thanksgiving and Praise to our beneficent Father who dwelleth in the Heavens,\"to be celebrated on the last Thursday in November. Together with Christmas and the New Year, Thanksgiving is a part of the broader fall / winter holiday season in the U.S.', 'title': 'Thanksgiving (United States)'}, {'body': 'Many Italian - Americans observe Columbus Day as a celebration of their heritage, and the first such celebration was held in New York City on October 12, 1866. The day was first enshrined as a legal holiday in the United States through the lobbying of Angelo Noce, a first generation Italian, in Denver. The first statewide holiday was proclaimed by Colorado governor Jesse F. McDonald in 1905, and it was made a statutory holiday in 1907. In April 1934, as a result of lobbying by the Knights of Columbus and New York City Italian leader Generoso Pope, Congress and President Franklin Delano Roosevelt proclaimed October 12 a federal holiday under the name Columbus Day.', 'id': '1', 'is_supporting': False, 'text': '# Columbus Day\\nMany Italian - Americans observe Columbus Day as a celebration of their heritage, and the first such celebration was held in New York City on October 12, 1866. The day was first enshrined as a legal holiday in the United States through the lobbying of Angelo Noce, a first generation Italian, in Denver. The first statewide holiday was proclaimed by Colorado governor Jesse F. McDonald in 1905, and it was made a statutory holiday in 1907. In April 1934, as a result of lobbying by the Knights of Columbus and New York City Italian leader Generoso Pope, Congress and President Franklin Delano Roosevelt proclaimed October 12 a federal holiday under the name Columbus Day.', 'title': 'Columbus Day'}, {'body': \"In 1916, President Woodrow Wilson issued a proclamation that officially established June 14 as Flag Day; in August 1946, National Flag Day was established by an Act of Congress. Flag Day is not an official federal holiday. Title 36 of the United States Code, Subtitle I, Part A, CHAPTER 1, § 110 is the official statute on Flag Day; however, it is at the president's discretion to officially proclaim the observance. On June 14, 1937, Pennsylvania became the first U.S. state to celebrate Flag Day as a state holiday, beginning in the town of Rennerdale. New York Statutes designate the second Sunday in June as Flag Day, a state holiday.\", 'id': '2', 'is_supporting': False, 'text': \"# Flag Day (United States)\\nIn 1916, President Woodrow Wilson issued a proclamation that officially established June 14 as Flag Day; in August 1946, National Flag Day was established by an Act of Congress. Flag Day is not an official federal holiday. Title 36 of the United States Code, Subtitle I, Part A, CHAPTER 1, § 110 is the official statute on Flag Day; however, it is at the president's discretion to officially proclaim the observance. On June 14, 1937, Pennsylvania became the first U.S. state to celebrate Flag Day as a state holiday, beginning in the town of Rennerdale. New York Statutes designate the second Sunday in June as Flag Day, a state holiday.\", 'title': 'Flag Day (United States)'}, {'body': \"The John F. Kennedy Space Center (KSC, originally known as the NASA Launch Operations Center) is one of ten National Aeronautics and Space Administration field centers. Since December 1968, the KSC has been NASA's primary launch center of human spaceflight. Launch operations for the Apollo, Skylab and Space Shuttle programs were carried out from Kennedy Space Center Launch Complex 39 and managed by KSC. Located on the east coast of Florida, KSC is adjacent to Cape Canaveral Air Force Station (CCAFS). The management of the two entities work very closely together, share resources, and even own facilities on each other's property.\", 'id': '3', 'is_supporting': False, 'text': \"# Kennedy Space Center\\nThe John F. Kennedy Space Center (KSC, originally known as the NASA Launch Operations Center) is one of ten National Aeronautics and Space Administration field centers. Since December 1968, the KSC has been NASA's primary launch center of human spaceflight. Launch operations for the Apollo, Skylab and Space Shuttle programs were carried out from Kennedy Space Center Launch Complex 39 and managed by KSC. Located on the east coast of Florida, KSC is adjacent to Cape Canaveral Air Force Station (CCAFS). The management of the two entities work very closely together, share resources, and even own facilities on each other's property.\", 'title': 'Kennedy Space Center'}, {'body': 'In 1949, the Territory and the Territory of New Guinea were established in an administrative union by the name of the Territory of Papua and New Guinea. That administrative union was renamed as Papua New Guinea in 1971. Notwithstanding that it was part of an administrative union, the Territory of Papua at all times retained a distinct legal status and identity; it was a Possession of the Crown whereas the Territory of New Guinea was initially a League of Nations mandate territory and subsequently a United Nations trust territory. This important legal and political distinction remained until the advent of the Independent State of Papua New Guinea in 1975.', 'id': '4', 'is_supporting': False, 'text': '# Territory of Papua\\nIn 1949, the Territory and the Territory of New Guinea were established in an administrative union by the name of the Territory of Papua and New Guinea. That administrative union was renamed as Papua New Guinea in 1971. Notwithstanding that it was part of an administrative union, the Territory of Papua at all times retained a distinct legal status and identity; it was a Possession of the Crown whereas the Territory of New Guinea was initially a League of Nations mandate territory and subsequently a United Nations trust territory. This important legal and political distinction remained until the advent of the Independent State of Papua New Guinea in 1975.', 'title': 'Territory of Papua'}, {'body': 'Angelo J. Lano was an American field agent for the Federal Bureau of Investigation in Washington DC, notable for his work heading the investigation of, and appearing as a witness for, the Watergate scandal surrounding President Richard M. Nixon. Lano was one of a number of FBI agents who was falsely accused as a source of information for Carl Bernstein and the \"Washington Post\" during the investigation, which helped shift White House suspicions away from Mark Felt, who was revealed as informant Deep Throat on May 31, 2005.', 'id': '5', 'is_supporting': True, 'text': '# Angelo Lano\\nAngelo J. Lano was an American field agent for the Federal Bureau of Investigation in Washington DC, notable for his work heading the investigation of, and appearing as a witness for, the Watergate scandal surrounding President Richard M. Nixon. Lano was one of a number of FBI agents who was falsely accused as a source of information for Carl Bernstein and the \"Washington Post\" during the investigation, which helped shift White House suspicions away from Mark Felt, who was revealed as informant Deep Throat on May 31, 2005.', 'title': 'Angelo Lano'}, {'body': \"Ratu Sir Lala Sukuna Day (commonly known as Ratu Sukuna Day) was a national public holiday in Fiji until the year 2010, when the Prime Minister, Commodore Voreqe Bainimarama, declared both Ratu Sir Lala Sakuna Day and National Youth Day to no longer be public holidays. It was originally celebrated annually on the last Monday of May, in honour of Lala Sukuna (1888-1958), the national father of modern Fiji, whose death anniversary falls on 30 May. The week leading up to Ratu Sukuna Day is marked by public celebrations with speeches and events, with an address from the President of Fiji on the closing day. Members of the public enter Parliament grounds to polish Sukuna's statue.\", 'id': '6', 'is_supporting': False, 'text': \"# Ratu Sir Lala Sukuna Day\\nRatu Sir Lala Sukuna Day (commonly known as Ratu Sukuna Day) was a national public holiday in Fiji until the year 2010, when the Prime Minister, Commodore Voreqe Bainimarama, declared both Ratu Sir Lala Sakuna Day and National Youth Day to no longer be public holidays. It was originally celebrated annually on the last Monday of May, in honour of Lala Sukuna (1888-1958), the national father of modern Fiji, whose death anniversary falls on 30 May. The week leading up to Ratu Sukuna Day is marked by public celebrations with speeches and events, with an address from the President of Fiji on the closing day. Members of the public enter Parliament grounds to polish Sukuna's statue.\", 'title': 'Ratu Sir Lala Sukuna Day'}, {'body': 'Washington is a city in, and the county seat of, Washington County, Kansas, United States. As of the 2010 census, the city population was 1,131.', 'id': '7', 'is_supporting': True, 'text': '# Washington, Kansas\\nWashington is a city in, and the county seat of, Washington County, Kansas, United States. As of the 2010 census, the city population was 1,131.', 'title': 'Washington, Kansas'}, {'body': 'A bill to accord national recognition of the holiday was introduced in Congress in 1913. In 1916, President Woodrow Wilson went to Spokane to speak in a Father\\'s Day celebration and wanted to make it official, but Congress resisted, fearing that it would become commercialized. US President Calvin Coolidge recommended in 1924 that the day be observed by the nation, but stopped short of issuing a national proclamation. Two earlier attempts to formally recognize the holiday had been defeated by Congress. In 1957, Maine Senator Margaret Chase Smith wrote a proposal accusing Congress of ignoring fathers for 40 years while honoring mothers, thus \"(singling) out just one of our two parents\". In 1966, President Lyndon B. Johnson issued the first presidential proclamation honoring fathers, designating the third Sunday in June as Father\\'s Day. Six years later, the day was made a permanent national holiday when President Richard Nixon signed it into law in 1972.', 'id': '8', 'is_supporting': False, 'text': '# Father\\'s Day (United States)\\nA bill to accord national recognition of the holiday was introduced in Congress in 1913. In 1916, President Woodrow Wilson went to Spokane to speak in a Father\\'s Day celebration and wanted to make it official, but Congress resisted, fearing that it would become commercialized. US President Calvin Coolidge recommended in 1924 that the day be observed by the nation, but stopped short of issuing a national proclamation. Two earlier attempts to formally recognize the holiday had been defeated by Congress. In 1957, Maine Senator Margaret Chase Smith wrote a proposal accusing Congress of ignoring fathers for 40 years while honoring mothers, thus \"(singling) out just one of our two parents\". In 1966, President Lyndon B. Johnson issued the first presidential proclamation honoring fathers, designating the third Sunday in June as Father\\'s Day. Six years later, the day was made a permanent national holiday when President Richard Nixon signed it into law in 1972.', 'title': \"Father's Day (United States)\"}, {'body': \"26 March 1991 is the day that marks the clash between military soldiers and peaceful demonstrating students which climaxed in the massacre of dozens under the orders of then President Moussa Traoré. He and three associates were later tried and convicted and received the death sentence for their part in the decision-making of that day. Nowadays, the day is a national holiday in order to remember the tragic events and the people that were killed.[unreliable source?] The coup is remembered as Mali's March Revolution of 1991.\", 'id': '9', 'is_supporting': False, 'text': \"# Mali\\n26 March 1991 is the day that marks the clash between military soldiers and peaceful demonstrating students which climaxed in the massacre of dozens under the orders of then President Moussa Traoré. He and three associates were later tried and convicted and received the death sentence for their part in the decision-making of that day. Nowadays, the day is a national holiday in order to remember the tragic events and the people that were killed.[unreliable source?] The coup is remembered as Mali's March Revolution of 1991.\", 'title': 'Mali'}, {'body': 'The holiday lasts seven days in Israel and eight in the diaspora. The first day (and second day in the diaspora) is a Shabbat - like holiday when work is forbidden. This is followed by intermediate days called Chol Hamoed, when certain work is permitted. The festival is closed with another Shabbat - like holiday called Shemini Atzeret (one day in Israel, two days in the diaspora, where the second day is called Simchat Torah). Shemini Atzeret coincides with the eighth day of Sukkot outside Israel.', 'id': '10', 'is_supporting': False, 'text': '# Sukkot\\nThe holiday lasts seven days in Israel and eight in the diaspora. The first day (and second day in the diaspora) is a Shabbat - like holiday when work is forbidden. This is followed by intermediate days called Chol Hamoed, when certain work is permitted. The festival is closed with another Shabbat - like holiday called Shemini Atzeret (one day in Israel, two days in the diaspora, where the second day is called Simchat Torah). Shemini Atzeret coincides with the eighth day of Sukkot outside Israel.', 'title': 'Sukkot'}, {'body': \"Following the deaths of workers at the hands of United States Army and United States Marshals Service during the Pullman Strike of 1894 in Chicago, the United States Congress unanimously voted to approve legislation to make Labor Day a national holiday and President Grover Cleveland signed it into law six days after the end of the strike. Cleveland supported the creation of the national holiday in an attempt to shore up support among trade unions following the Pullman Strike. The date of May 1 (an ancient European holiday known as May Day) was an alternative date, celebrated then (and now) as International Workers' Day, but President Cleveland was concerned that observance of Labor Day on May 1 would encourage Haymarket - style protests and would strengthen socialist and anarchist movements that, though distinct from one another, had rallied to commemorate the Haymarket Affair on International Workers' Day.\", 'id': '11', 'is_supporting': False, 'text': \"# Labor Day\\nFollowing the deaths of workers at the hands of United States Army and United States Marshals Service during the Pullman Strike of 1894 in Chicago, the United States Congress unanimously voted to approve legislation to make Labor Day a national holiday and President Grover Cleveland signed it into law six days after the end of the strike. Cleveland supported the creation of the national holiday in an attempt to shore up support among trade unions following the Pullman Strike. The date of May 1 (an ancient European holiday known as May Day) was an alternative date, celebrated then (and now) as International Workers' Day, but President Cleveland was concerned that observance of Labor Day on May 1 would encourage Haymarket - style protests and would strengthen socialist and anarchist movements that, though distinct from one another, had rallied to commemorate the Haymarket Affair on International Workers' Day.\", 'title': 'Labor Day'}, {'body': 'Khabarovsky District () is an administrative and municipal district (raion), one of the seventeen in Khabarovsk Krai, Russia. It consists of two unconnected segments separated by the territory of Amursky District, which are located in the southwest of the krai. The area of the district is . Its administrative center is the city of Khabarovsk (which is not administratively a part of the district). Population:', 'id': '12', 'is_supporting': False, 'text': '# Khabarovsky District\\nKhabarovsky District () is an administrative and municipal district (raion), one of the seventeen in Khabarovsk Krai, Russia. It consists of two unconnected segments separated by the territory of Amursky District, which are located in the southwest of the krai. The area of the district is . Its administrative center is the city of Khabarovsk (which is not administratively a part of the district). Population:', 'title': 'Khabarovsky District'}, {'body': 'India made many requisitions to the Salazar regime of Portugal to grant their Indian colonies independence, but when that failed, on 18 December 1961, Indian troops crossed the border into Goa and \"liberated\"it. Operation Vijay involved sustained land, sea and air strikes for more than thirty - six hours; it resulted in the unconditional surrender of Portuguese forces on 19 December. A United Nations resolution \"condemning\" the invasion was proposed by the United States and the United Kingdom in the United Nations Security Council, but would be vetoed by the USSR. The territory of Goa was under military rule for five months. However, the previous civil service was soon restored. Goan voters went to the polls in a referendum and voted to become an autonomous, federally administered territory. Goa was later admitted Indian statehood in 1987. Goa celebrates \"Liberation Day\"on 19 December every year, which is also a state holiday.', 'id': '13', 'is_supporting': False, 'text': '# History of Goa\\nIndia made many requisitions to the Salazar regime of Portugal to grant their Indian colonies independence, but when that failed, on 18 December 1961, Indian troops crossed the border into Goa and \"liberated\"it. Operation Vijay involved sustained land, sea and air strikes for more than thirty - six hours; it resulted in the unconditional surrender of Portuguese forces on 19 December. A United Nations resolution \"condemning\" the invasion was proposed by the United States and the United Kingdom in the United Nations Security Council, but would be vetoed by the USSR. The territory of Goa was under military rule for five months. However, the previous civil service was soon restored. Goan voters went to the polls in a referendum and voted to become an autonomous, federally administered territory. Goa was later admitted Indian statehood in 1987. Goa celebrates \"Liberation Day\"on 19 December every year, which is also a state holiday.', 'title': 'History of Goa'}, {'body': 'A bill to accord national recognition of the holiday was introduced in Congress in 1913. In 1916, President Woodrow Wilson went to Spokane to speak at a Father\\'s Day celebration and he wanted to make it an officially recognized federal holiday, but Congress resisted, fearing that it would become commercialized. US President Calvin Coolidge recommended in 1924 that the day be observed throughout the entire nation, but he stopped short at issuing a national proclamation. Two earlier attempts to formally recognize the holiday had been defeated by Congress. In 1957, Maine Senator Margaret Chase Smith wrote a Father\\'s Day proposal accusing Congress of ignoring fathers for 40 years while honoring mothers, thus \"(singling) out just one of our two parents\". In 1966, President Lyndon B. Johnson issued the first presidential proclamation honoring fathers, designating the third Sunday in June as Father\\'s Day. Six years later, the day was made a permanent national holiday when President Richard Nixon signed it into law in 1972.', 'id': '14', 'is_supporting': True, 'text': '# Father\\'s Day\\nA bill to accord national recognition of the holiday was introduced in Congress in 1913. In 1916, President Woodrow Wilson went to Spokane to speak at a Father\\'s Day celebration and he wanted to make it an officially recognized federal holiday, but Congress resisted, fearing that it would become commercialized. US President Calvin Coolidge recommended in 1924 that the day be observed throughout the entire nation, but he stopped short at issuing a national proclamation. Two earlier attempts to formally recognize the holiday had been defeated by Congress. In 1957, Maine Senator Margaret Chase Smith wrote a Father\\'s Day proposal accusing Congress of ignoring fathers for 40 years while honoring mothers, thus \"(singling) out just one of our two parents\". In 1966, President Lyndon B. Johnson issued the first presidential proclamation honoring fathers, designating the third Sunday in June as Father\\'s Day. Six years later, the day was made a permanent national holiday when President Richard Nixon signed it into law in 1972.', 'title': \"Father's Day\"}, {'body': \"In the Philippines, Father's Day is officially celebrated every first Monday of December, but it is not a public holiday. It is more widely observed by the public on the 3rd Sunday of June perhaps due to American influence.\", 'id': '15', 'is_supporting': False, 'text': \"# Father's Day\\nIn the Philippines, Father's Day is officially celebrated every first Monday of December, but it is not a public holiday. It is more widely observed by the public on the 3rd Sunday of June perhaps due to American influence.\", 'title': \"Father's Day\"}, {'body': 'In 1779, July 4 fell on a Sunday. The holiday was celebrated on Monday, July 5. In 1781, the Massachusetts General Court became the first state legislature to recognize July 4 as a state celebration. In 1783, Moravians in Salem, North Carolina, held a celebration of July 4 with a challenging music program assembled by Johann Friedrich Peter. This work was titled The Psalm of Joy. This is recognized as the first recorded celebration and is still celebrated there today. In 1870, the U.S. Congress made Independence Day an unpaid holiday for federal employees. In 1938, Congress changed Independence Day to a paid federal holiday.', 'id': '16', 'is_supporting': False, 'text': '# Independence Day (United States)\\nIn 1779, July 4 fell on a Sunday. The holiday was celebrated on Monday, July 5. In 1781, the Massachusetts General Court became the first state legislature to recognize July 4 as a state celebration. In 1783, Moravians in Salem, North Carolina, held a celebration of July 4 with a challenging music program assembled by Johann Friedrich Peter. This work was titled The Psalm of Joy. This is recognized as the first recorded celebration and is still celebrated there today. In 1870, the U.S. Congress made Independence Day an unpaid holiday for federal employees. In 1938, Congress changed Independence Day to a paid federal holiday.', 'title': 'Independence Day (United States)'}, {'body': 'Vilnius County () is the largest of the 10 counties of Lithuania, located in the east of the country around the city Vilnius. On 1 July 2010, the county administration was abolished, and since that date, Vilnius County remains as the territorial and statistical unit.', 'id': '17', 'is_supporting': False, 'text': '# Vilnius County\\nVilnius County () is the largest of the 10 counties of Lithuania, located in the east of the country around the city Vilnius. On 1 July 2010, the county administration was abolished, and since that date, Vilnius County remains as the territorial and statistical unit.', 'title': 'Vilnius County'}, {'body': 'According to Greek law, every Sunday of the year is a public holiday. In addition, there are four mandatory official public holidays: 25 March (Greek Independence Day), Easter Monday, 15 August (Assumption or Dormition of the Holy Virgin), and 25 December (Christmas). 1 May (Labour Day) and 28 October (Ohi Day) are regulated by law as being optional but it is customary for employees to be given the day off. There are, however, more public holidays celebrated in Greece than are announced by the Ministry of Labour each year as either obligatory or optional. The list of these non-fixed national holidays rarely changes and has not changed in recent decades, giving a total of eleven national holidays each year.', 'id': '18', 'is_supporting': False, 'text': '# Greece\\nAccording to Greek law, every Sunday of the year is a public holiday. In addition, there are four mandatory official public holidays: 25 March (Greek Independence Day), Easter Monday, 15 August (Assumption or Dormition of the Holy Virgin), and 25 December (Christmas). 1 May (Labour Day) and 28 October (Ohi Day) are regulated by law as being optional but it is customary for employees to be given the day off. There are, however, more public holidays celebrated in Greece than are announced by the Ministry of Labour each year as either obligatory or optional. The list of these non-fixed national holidays rarely changes and has not changed in recent decades, giving a total of eleven national holidays each year.', 'title': 'Greece'}, {'body': \"Administrative Professionals' Day (also known as Secretaries' Day or Admin Day) is a day observed yearly (but not a public holiday). In some countries, it falls within Administrative Professionals' Week (the last full week of April in the United States). The day recognizes the work of secretaries, administrative assistants, receptionists, and other administrative support professionals. Typically administrative professionals are given cards, flowers, chocolates, and lunches.\", 'id': '19', 'is_supporting': False, 'text': \"# Administrative Professionals Day\\nAdministrative Professionals' Day (also known as Secretaries' Day or Admin Day) is a day observed yearly (but not a public holiday). In some countries, it falls within Administrative Professionals' Week (the last full week of April in the United States). The day recognizes the work of secretaries, administrative assistants, receptionists, and other administrative support professionals. Typically administrative professionals are given cards, flowers, chocolates, and lunches.\", 'title': 'Administrative Professionals Day'}], 'supporting_ids': ['5', '7', '14'], 'n_hops': 3}) (input_keys={'question', 'docs'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_ds[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd58c5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    answer='Washington County, Kansas',\n",
       "    collected_info=[KeyInformation(info=\"President Richard Nixon signed the bill making Father's Day a permanent national holiday in 1972.\", source_doc_id='8'), KeyInformation(info=\"President Richard Nixon signed the bill making Father's Day a permanent national holiday in 1972.\", source_doc_id='14'), KeyInformation(info='Washington is the county seat of Washington County, Kansas.', source_doc_id='7')],\n",
       "    retrieved_doc_ids=['8', '14', '7', '5', '3'],\n",
       "    citations=['7'],\n",
       "    n_turns=5\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-b3749757287b0c2ab45fef30e20f5c4e&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-b3749757287b0c2ab45fef30e20f5c4e)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = program(example.question, example.docs)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e58239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_retrieval_recall(example, pred, trace=None):\n",
    "    \"\"\"Retrieval recall metric - fraction of supporting documents found.\"\"\"\n",
    "    if not example.supporting_ids:\n",
    "        return 1.0  # No supporting documents to evaluate\n",
    "\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "\n",
    "    if not gold_ids:\n",
    "        return 1.0\n",
    "\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    return len(found) / len(gold_ids)\n",
    "\n",
    "\n",
    "def metric_retrieval_precision(example, pred, trace=None):\n",
    "    \"\"\"Retrieval precision metric - fraction of retrieved documents that are supporting.\"\"\"\n",
    "    if not example.supporting_ids:\n",
    "        return 1.0\n",
    "\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    return len(found) / len(retrieved_ids)\n",
    "\n",
    "\n",
    "def metric_answer_exact_match(example, pred, trace=None):\n",
    "    \"\"\"Exact match metric for MuSiQue using the official metrics.\"\"\"\n",
    "    return exact_match(pred.answer, example.answers)\n",
    "\n",
    "\n",
    "def metric_answer_f1_score(example, pred, trace=None):\n",
    "    \"\"\"Token-level F1 score using the official metrics.\"\"\"\n",
    "    return f1(pred.answer, example.answers)\n",
    "\n",
    "\n",
    "def metric_citation_f1(example, pred, trace=None):\n",
    "    \"\"\"Citation accuracy metrics - precision, recall, F1 for cited document IDs.\"\"\"\n",
    "    # Convert to sets for easy comparison\n",
    "    gold_ids = set(example.supporting_ids) if example.supporting_ids else set()\n",
    "    cited_ids = set(str(doc_id) for doc_id in pred.citations)  # Ensure string format\n",
    "\n",
    "    # Handle edge cases\n",
    "    if not gold_ids:\n",
    "        raise ValueError(\"Supporting docs must be provided for citation metric\")\n",
    "\n",
    "    if not cited_ids:\n",
    "        # No citations given but some needed\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate standard precision/recall/F1\n",
    "    correct_citations = cited_ids & gold_ids\n",
    "    precision = len(correct_citations) / len(cited_ids)\n",
    "    recall = len(correct_citations) / len(gold_ids)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def metric_n_hops_penalty(example, pred, trace=None):\n",
    "    \"\"\"N-hops penalty metric that penalizes agents taking more turns than reference.\"\"\"\n",
    "    # Get actual turns taken by the agent\n",
    "    agent_turns = pred.n_turns\n",
    "    \n",
    "    # Get reference hops from the example\n",
    "    reference_hops = example.n_hops\n",
    "    \n",
    "    if agent_turns <= reference_hops:\n",
    "        # Perfect score if agent is efficient\n",
    "        return 1.0\n",
    "    else:\n",
    "        # Exponential penalty for taking too many hops\n",
    "        # Penalty = 0.8^(extra_hops) \n",
    "        extra_hops = agent_turns - reference_hops\n",
    "        return max(0.8 ** extra_hops, 0.1)  # Minimum score of 0.1\n",
    "\n",
    "\n",
    "def metric(example, pred, trace=None):\n",
    "    \"\"\"Combined metric for MuSiQue: weighted by number of hops.\"\"\"\n",
    "    retrieval_recall_score = metric_retrieval_recall(example, pred, trace)\n",
    "    retrieval_precision_score = metric_retrieval_precision(example, pred, trace)\n",
    "    answer_f1_score = metric_answer_f1_score(example, pred, trace)\n",
    "    citation_f1 = metric_citation_f1(example, pred, trace)\n",
    "    n_hops_penalty_score = metric_n_hops_penalty(example, pred, trace)\n",
    "\n",
    "    # Combine metrics: EM and F1 for answer quality, retrieval recall for completeness,\n",
    "    # citation F1 for proper attribution, and hop efficiency penalty\n",
    "    score_weight_pairs = [\n",
    "        (retrieval_recall_score, 0.9),  # Retrieval recall for finding supporting docs\n",
    "        (retrieval_precision_score, 0.5),  # Retrieval precision for finding supporting docs\n",
    "        (answer_f1_score, 1.0),  # F1\n",
    "        (citation_f1, 0.7),  # Citation accuracy for proper attribution\n",
    "        (n_hops_penalty_score, 0.6),  # Hop efficiency penalty\n",
    "    ]\n",
    "\n",
    "    return sum(score * weight for score, weight in score_weight_pairs) / sum(weight for _, weight in score_weight_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0115e6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7389189189189189"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(example, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79664e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluating ORIGINAL program...\n",
      "Average Metric: 37.04 / 50 (74.1%): 100%|█████████████████████████████████████████████████████| 50/50 [00:44<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:14:15 INFO dspy.evaluate.evaluate: Average Metric: 37.03619060614726 / 50 (74.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval at: http://localhost:5005/#/experiments/1/runs/4d9e3c3089aa43859734784e23680112\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5005/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-6350bdedd2c25a08fc2ed634394f8c2e&amp;experiment_id=1&amp;trace_id=tr-0b234efcb6223e3dae81d478a67b4d4b&amp;experiment_id=1&amp;trace_id=tr-b7568e98a71e5b0e538b4f80d4bde5d9&amp;experiment_id=1&amp;trace_id=tr-f0729951897392b45976b45c9ab39543&amp;experiment_id=1&amp;trace_id=tr-d54eb24d9f7e506d255a8c72a10a7b89&amp;experiment_id=1&amp;trace_id=tr-c992d802e13ca1b3e921ad3c293ac5e5&amp;experiment_id=1&amp;trace_id=tr-69ef26a1fa1410f9f22484da06638078&amp;experiment_id=1&amp;trace_id=tr-a710a27a5d33025ce5145c64f759bb06&amp;experiment_id=1&amp;trace_id=tr-71130782ac655e45d95489f5c19a7797&amp;experiment_id=1&amp;trace_id=tr-a96368dea45faeaabc55ff216a768dd2&amp;experiment_id=1&amp;version=3.4.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-6350bdedd2c25a08fc2ed634394f8c2e), Trace(trace_id=tr-0b234efcb6223e3dae81d478a67b4d4b), Trace(trace_id=tr-b7568e98a71e5b0e538b4f80d4bde5d9), Trace(trace_id=tr-f0729951897392b45976b45c9ab39543), Trace(trace_id=tr-d54eb24d9f7e506d255a8c72a10a7b89), Trace(trace_id=tr-c992d802e13ca1b3e921ad3c293ac5e5), Trace(trace_id=tr-69ef26a1fa1410f9f22484da06638078), Trace(trace_id=tr-a710a27a5d33025ce5145c64f759bb06), Trace(trace_id=tr-71130782ac655e45d95489f5c19a7797), Trace(trace_id=tr-a96368dea45faeaabc55ff216a768dd2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate original program\n",
    "print(\"📊 Evaluating ORIGINAL program...\")\n",
    "original_evaluate = dspy.Evaluate(\n",
    "    devset=test_ds,\n",
    "    metric=metric,\n",
    "    num_threads=8,\n",
    "    display_table=False,\n",
    "    display_progress=True\n",
    ")\n",
    "original_eval_result = original_evaluate(program)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7f427",
   "metadata": {},
   "source": [
    "## GEPA Optimization\n",
    "\n",
    "GEPA is a reflective prompt optimizer that uses textual feedback to improve performance. We'll create feedback functions for each evaluation aspect and optimize our multi-hop QA program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8212c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_retrieval_recall(example, pred):\n",
    "    \"\"\"Generate feedback for retrieval recall evaluation.\"\"\"\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    recall_score = len(found) / len(gold_ids)\n",
    "\n",
    "    if recall_score == 1.0:\n",
    "        feedback = f\"Perfect retrieval! You found all {len(gold_ids)} supporting documents: {sorted(found)}\"\n",
    "    elif recall_score >= 0.5:\n",
    "        missing_ids = gold_ids - found\n",
    "        feedback = (\n",
    "            f\"Good retrieval (recall: {recall_score:.2f}). Found {len(found)} out of {len(gold_ids)} \"\n",
    "            f\"supporting documents. Missing: {sorted(missing_ids)}. Consider refining your search queries \"\n",
    "            f\"to find the remaining relevant documents.\"\n",
    "        )\n",
    "    else:\n",
    "        missing_ids = gold_ids - found\n",
    "        feedback = (\n",
    "            f\"Poor retrieval (recall: {recall_score:.2f}). Only found {len(found)} out of {len(gold_ids)} \"\n",
    "            f\"supporting documents. Missing critical documents: {sorted(missing_ids)}. \"\n",
    "            f\"Your search queries need to be more comprehensive and targeted.\"\n",
    "        )\n",
    "\n",
    "    return recall_score, feedback\n",
    "\n",
    "\n",
    "def feedback_retrieval_precision(example, pred):\n",
    "    \"\"\"Generate feedback for retrieval precision evaluation.\"\"\"\n",
    "    gold_ids = set(example.supporting_ids)\n",
    "    retrieved_ids = set(pred.retrieved_doc_ids)\n",
    "\n",
    "    if not retrieved_ids:\n",
    "        return 0.0, \"No documents were retrieved. Your search queries need to find relevant documents.\"\n",
    "\n",
    "    found = gold_ids.intersection(retrieved_ids)\n",
    "    precision_score = len(found) / len(retrieved_ids)\n",
    "    irrelevant_docs = retrieved_ids - gold_ids\n",
    "\n",
    "    if precision_score == 1.0:\n",
    "        feedback = (\n",
    "            f\"Perfect precision! All {len(retrieved_ids)} retrieved documents are supporting documents: {sorted(found)}\"\n",
    "        )\n",
    "    elif precision_score >= 0.7:\n",
    "        feedback = (\n",
    "            f\"Good precision (precision: {precision_score:.2f}). {len(found)} out of {len(retrieved_ids)} \"\n",
    "            f\"retrieved documents are relevant. Irrelevant docs: {sorted(irrelevant_docs)}. \"\n",
    "            f\"Consider making your search queries more specific to avoid irrelevant documents.\"\n",
    "        )\n",
    "    elif precision_score >= 0.3:\n",
    "        feedback = (\n",
    "            f\"Moderate precision (precision: {precision_score:.2f}). Only {len(found)} out of {len(retrieved_ids)} \"\n",
    "            f\"retrieved documents are relevant. Many irrelevant docs retrieved: {sorted(irrelevant_docs)}. \"\n",
    "            f\"Your search queries are too broad - focus on more specific terms and entities.\"\n",
    "        )\n",
    "    else:\n",
    "        feedback = (\n",
    "            f\"Poor precision (precision: {precision_score:.2f}). Only {len(found)} out of {len(retrieved_ids)} \"\n",
    "            f\"retrieved documents are relevant. Most retrieved docs are irrelevant: {sorted(irrelevant_docs)}. \"\n",
    "            f\"Your search queries are retrieving too many irrelevant documents. Be much more specific and targeted.\"\n",
    "        )\n",
    "\n",
    "    return precision_score, feedback\n",
    "\n",
    "\n",
    "def feedback_answer_exact_match(example, pred):\n",
    "    \"\"\"Generate feedback for exact match evaluation.\"\"\"\n",
    "    em_score = exact_match(pred.answer, example.answers)\n",
    "\n",
    "    if em_score == 1.0:\n",
    "        feedback = f\"Perfect! You provided the exact correct answer: '{pred.answer}'. This matches the expected answer exactly.\"\n",
    "    else:\n",
    "        # Find the best matching answer for more specific feedback\n",
    "        best_answer = example.answers[0] if example.answers else \"N/A\"\n",
    "        feedback = (\n",
    "            f\"Your answer '{pred.answer}' doesn't exactly match the expected answer '{best_answer}'. \"\n",
    "            f\"Consider being more precise with entity names, dates, and specific facts.\"\n",
    "        )\n",
    "\n",
    "    return em_score, feedback\n",
    "\n",
    "\n",
    "def feedback_answer_f1_score(example, pred):\n",
    "    \"\"\"Generate feedback for F1 score evaluation.\"\"\"\n",
    "    f1_score = f1(pred.answer, example.answers)\n",
    "\n",
    "    if f1_score >= 0.9:\n",
    "        feedback = f\"Excellent! Your answer has high overlap (F1: {f1_score:.2f}) with the expected answer. Good token-level accuracy.\"\n",
    "    elif f1_score >= 0.5:\n",
    "        feedback = (\n",
    "            f\"Good partial match (F1: {f1_score:.2f}). Your answer contains relevant information but \"\n",
    "            f\"could be more complete or precise. Consider including more specific details from the retrieved documents.\"\n",
    "        )\n",
    "    else:\n",
    "        best_answer = example.answers[0] if example.answers else \"N/A\"\n",
    "        feedback = (\n",
    "            f\"Low overlap (F1: {f1_score:.2f}) with expected answer. Your answer '{pred.answer}' \"\n",
    "            f\"differs significantly from '{best_answer}'. Focus on extracting the specific information \"\n",
    "            f\"requested in the question.\"\n",
    "        )\n",
    "\n",
    "    return f1_score, feedback\n",
    "\n",
    "\n",
    "def feedback_citation_f1(example, pred):\n",
    "    \"\"\"Generate feedback for citation F1 evaluation.\"\"\"\n",
    "    gold_ids = set(example.supporting_ids) if example.supporting_ids else set()\n",
    "    cited_ids = set(str(doc_id) for doc_id in pred.citations)\n",
    "\n",
    "    if not gold_ids:\n",
    "        return 1.0, \"No supporting documents to cite.\"\n",
    "\n",
    "    if not cited_ids:\n",
    "        feedback = f\"You didn't cite any documents, but should have cited: {sorted(gold_ids)}. Always cite the documents that support your answer.\"\n",
    "        return 0.0, feedback\n",
    "\n",
    "    correct_citations = cited_ids & gold_ids\n",
    "    precision = len(correct_citations) / len(cited_ids)\n",
    "    recall = len(correct_citations) / len(gold_ids)\n",
    "    citation_f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    if citation_f1 >= 0.9:\n",
    "        feedback = f\"Excellent citations (F1: {citation_f1:.2f})! You properly cited the supporting documents: {sorted(correct_citations)}\"\n",
    "    elif citation_f1 >= 0.5:\n",
    "        incorrect_citations = cited_ids - gold_ids\n",
    "        missing_citations = gold_ids - cited_ids\n",
    "        feedback = f\"Good citations (F1: {citation_f1:.2f}). Correct: {sorted(correct_citations)}. \"\n",
    "        if incorrect_citations:\n",
    "            feedback += f\"Unnecessary: {sorted(incorrect_citations)}. \"\n",
    "        if missing_citations:\n",
    "            feedback += f\"Missing: {sorted(missing_citations)}. \"\n",
    "        feedback += \"Be more precise about which documents actually support your answer.\"\n",
    "    else:\n",
    "        incorrect_citations = cited_ids - gold_ids\n",
    "        missing_citations = gold_ids - cited_ids\n",
    "        feedback = (\n",
    "            f\"Poor citations (F1: {citation_f1:.2f}). You cited {sorted(cited_ids)} but should cite {sorted(gold_ids)}. \"\n",
    "            f\"Focus on identifying which documents directly support your answer claims.\"\n",
    "        )\n",
    "\n",
    "    return citation_f1, feedback\n",
    "\n",
    "\n",
    "def feedback_n_hops_penalty(example, pred):\n",
    "    \"\"\"Generate feedback for n-hops penalty evaluation.\"\"\"\n",
    "    # Get actual turns taken by the agent\n",
    "    agent_turns = pred.n_turns\n",
    "    \n",
    "    # Get reference hops from the example\n",
    "    reference_hops = example.n_hops\n",
    "    \n",
    "    penalty_score = metric_n_hops_penalty(example, pred)\n",
    "    \n",
    "    # Analyze retrieval patterns to provide specific feedback\n",
    "    retrieved_docs = pred.retrieved_doc_ids \n",
    "    unique_docs = len(set(retrieved_docs))\n",
    "    total_retrievals = len(retrieved_docs)\n",
    "    \n",
    "    if agent_turns <= reference_hops:\n",
    "        feedback = f\"Perfect efficiency! You completed the task in {agent_turns} turns, same as or fewer than the reference ({reference_hops} hops). This shows excellent retrieval strategy and reasoning efficiency.\"\n",
    "    elif agent_turns == reference_hops + 1:\n",
    "        if total_retrievals > unique_docs:\n",
    "            feedback = f\"Good efficiency (penalty: {penalty_score:.2f}). You took {agent_turns} turns vs reference {reference_hops} hops. Only 1 extra turn - but check if you retrieved duplicate documents ({total_retrievals} retrievals, {unique_docs} unique). Focus on more targeted initial queries.\"\n",
    "        else:\n",
    "            feedback = f\"Good efficiency (penalty: {penalty_score:.2f}). You took {agent_turns} turns vs reference {reference_hops} hops. Only 1 extra turn - consider if your initial retrieval query could have been more comprehensive to get the needed information upfront.\"\n",
    "    elif agent_turns <= reference_hops + 2:\n",
    "        if total_retrievals > unique_docs:\n",
    "            redundant_retrievals = total_retrievals - unique_docs\n",
    "            feedback = f\"Moderate efficiency (penalty: {penalty_score:.2f}). You took {agent_turns} turns vs reference {reference_hops} hops. You made {redundant_retrievals} redundant retrieval(s) - avoid retrieving the same documents multiple times. Plan your queries more strategically.\"\n",
    "        else:\n",
    "            feedback = f\"Moderate efficiency (penalty: {penalty_score:.2f}). You took {agent_turns} turns vs reference {reference_hops} hops. Your retrieval queries may be too specific or missing key entities. Try broader, more comprehensive initial searches.\"\n",
    "    else:\n",
    "        extra_turns = agent_turns - reference_hops\n",
    "        if total_retrievals > unique_docs:\n",
    "            redundant_retrievals = total_retrievals - unique_docs\n",
    "            feedback = f\"Poor efficiency (penalty: {penalty_score:.2f}). You took {extra_turns} extra turns ({agent_turns} vs {reference_hops} reference). Major issue: {redundant_retrievals} redundant retrieval(s) - you're wasting turns retrieving documents you already have. Focus on tracking what you've retrieved and crafting better initial queries.\"\n",
    "        else:\n",
    "            feedback = f\"Poor efficiency (penalty: {penalty_score:.2f}). You took {extra_turns} extra turns ({agent_turns} vs {reference_hops} reference). Your retrieval strategy is inefficient - queries may be too narrow or poorly targeted. Plan what information you need and retrieve it strategically in fewer, more comprehensive searches.\"\n",
    "    \n",
    "    return penalty_score, feedback\n",
    "\n",
    "\n",
    "def metric_with_feedback(example, pred, trace=None, pred_name=None, pred_trace=None):\n",
    "    \"\"\"\n",
    "    Combined metric for MuSiQue with feedback for GEPA optimization.\n",
    "    Returns a dspy.Prediction with score (float) and feedback (str).\n",
    "\n",
    "    The feedback is targeted at specific predictors when pred_name is provided,\n",
    "    helping GEPA understand how to improve each component.\n",
    "    \"\"\"\n",
    "    # Compute feedback and scores for all metrics\n",
    "    score_answer_f1, fb_answer_f1 = feedback_answer_f1_score(example, pred)\n",
    "    score_retrieval_recall, fb_retrieval_recall = feedback_retrieval_recall(example, pred)\n",
    "    score_retrieval_precision, fb_retrieval_precision = feedback_retrieval_precision(example, pred)\n",
    "    score_citation_f1, fb_citation_f1 = feedback_citation_f1(example, pred)\n",
    "    score_n_hops_penalty, fb_n_hops_penalty = feedback_n_hops_penalty(example, pred)\n",
    "\n",
    "    # Combined score: weighted average of all metrics (same as original metric)\n",
    "    score_weight_pairs = [\n",
    "        (score_answer_f1, 1.0),  # Answer F1\n",
    "        (score_retrieval_recall, 0.9),  # Retrieval recall for finding supporting docs\n",
    "        (score_retrieval_precision, 0.5),  # Retrieval precision for finding supporting docs\n",
    "        (score_citation_f1, 0.7),  # Citation accuracy for proper attribution\n",
    "        (score_n_hops_penalty, 0.6),  # Hop efficiency penalty\n",
    "    ]\n",
    "\n",
    "    total_score = sum(score * weight for score, weight in score_weight_pairs) / sum(\n",
    "        weight for _, weight in score_weight_pairs\n",
    "    )\n",
    "\n",
    "    # Provide targeted feedback based on the predictor being optimized\n",
    "    if pred_name == \"generate_query.predict\":\n",
    "        # Focus on query generation quality and retrieval effectiveness\n",
    "        feedback = (\n",
    "            fb_retrieval_recall\n",
    "            + \" \"\n",
    "            + fb_retrieval_precision\n",
    "            + \" \"\n",
    "            + fb_n_hops_penalty\n",
    "            + \" \"\n",
    "            + \"Your search queries should be both comprehensive (high recall) and specific (high precision). \"\n",
    "            \"Consider what entities, relationships, or facts are needed for each hop of reasoning. \"\n",
    "            \"Plan your queries strategically to avoid unnecessary turns.\"\n",
    "        )\n",
    "\n",
    "    elif pred_name == \"extract_info.predict\":\n",
    "        # Focus on information extraction quality\n",
    "        feedback = (\n",
    "            fb_answer_f1\n",
    "            + \" \"\n",
    "            + (\n",
    "                \"Focus on extracting the most relevant facts, entities, and relationships from the retrieved documents. \"\n",
    "                \"Make sure to capture information that directly helps answer the question or leads to the next reasoning step.\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    elif pred_name == \"generate_answer.predict\":\n",
    "        # Focus on answer generation and citation quality\n",
    "        feedback = (\n",
    "            fb_answer_f1\n",
    "            + \" \"\n",
    "            + fb_citation_f1\n",
    "            + \" \"\n",
    "            + (\n",
    "                \"Provide precise, complete answers using the exact information from the retrieved documents. \"\n",
    "                \"Always cite the document IDs that support your answer claims.\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        # Generic feedback combining all aspects\n",
    "        feedback = \"\\n\".join([\n",
    "            \"Overall performance breakdown:\",\n",
    "            f\"- Answer F1 Score: {fb_answer_f1}\",\n",
    "            f\"- Retrieval Recall: {fb_retrieval_recall}\",\n",
    "            f\"- Retrieval Precision: {fb_retrieval_precision}\",\n",
    "            f\"- Citations F1 Score: {fb_citation_f1}\",\n",
    "            f\"- Hop Efficiency: {fb_n_hops_penalty}\"\n",
    "        ])\n",
    "\n",
    "    return dspy.Prediction(score=total_score, feedback=feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4450236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.739\n",
      "Feedback: Overall performance breakdown:\n",
      "- Answer F1 Score: Good partial match (F1: 0.80). Your answer contains relevant information but could be more complete or precise. Consider including more specific details from the retrieved documents.\n",
      "- Retrieval Recall: Perfect retrieval! You found all 3 supporting documents: ['14', '5', '7']\n",
      "- Retrieval Precision: Moderate precision (precision: 0.60). Only 3 out of 5 retrieved documents are relevant. Many irrelevant docs retrieved: ['3', '8']. Your search queries are too broad - focus on more specific terms and entities.\n",
      "- Citations F1 Score: Good citations (F1: 0.50). Correct: ['7']. Missing: ['14', '5']. Be more precise about which documents actually support your answer.\n",
      "- Hop Efficiency: Moderate efficiency (penalty: 0.64). You took 5 turns vs reference 3 hops. Your retrieval queries may be too specific or missing key entities. Try broader, more comprehensive initial searches.\n"
     ]
    }
   ],
   "source": [
    "# Test the feedback metric on our example\n",
    "feedback_result = metric_with_feedback(example, pred)\n",
    "print(f\"Score: {feedback_result.score:.3f}\")\n",
    "print(f\"Feedback: {feedback_result.feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f0150cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GEPA optimizer configured\n"
     ]
    }
   ],
   "source": [
    "from dspy import GEPA\n",
    "\n",
    "# Set up GEPA optimizer with reflection LM for optimization\n",
    "optimizer = GEPA(\n",
    "    metric=metric_with_feedback,\n",
    "    auto=\"light\",  # Use light budget for faster experimentation. Use \"heavy\" for best performance\n",
    "    num_threads=8,\n",
    "    track_stats=True,\n",
    "    use_merge=False,\n",
    "    reflection_lm=reflection_lm  \n",
    ")\n",
    "\n",
    "print(\"✅ GEPA optimizer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad84068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:14:15 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '4f2fb7f1c3d54cb5b55f6319bd78b1f6', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current dspy workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting GEPA optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:14:15 INFO dspy.teleprompt.gepa.gepa: Running GEPA for approx 1765 metric calls of the program. This amounts to 29.42 full evals on the train+val set.\n",
      "2025/10/02 13:14:15 INFO dspy.teleprompt.gepa.gepa: Using 30 examples for tracking Pareto scores. You can consider using a smaller sample of the valset to allow GEPA to explore more diverse solutions within the same budget.\n",
      "GEPA Optimization:   0%|                                                                       | 0/1765 [00:00<?, ?rollouts/s]2025/10/02 13:14:54 INFO dspy.evaluate.evaluate: Average Metric: 19.10404514404515 / 30 (63.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_0 at: http://localhost:5005/#/experiments/1/runs/5438de8e37a641ac88f70b0147a63053\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:14:54 INFO dspy.teleprompt.gepa.gepa: Iteration 0: Base program full valset score: 0.6368015048015047\n",
      "GEPA Optimization:   2%|█                                                             | 30/1765 [00:39<37:38,  1.30s/rollouts]2025/10/02 13:14:54 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Selected program 0 score: 0.6368015048015047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.50 / 3 (83.2%): 100%|█████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:14:56 INFO dspy.evaluate.evaluate: Average Metric: 2.496846846846847 / 3 (83.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_1 at: http://localhost:5005/#/experiments/1/runs/32915fb1cbff4c07aaa92a053948c3a1\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:15:21 INFO dspy.teleprompt.gepa.gepa: Iteration 1: Proposed new text for generate_query.predict: You are an expert reasoning agent designed for a multi-hop question-answering system. Your primary task is to generate the next optimal search query to find a missing piece of information needed to answer a complex question.\n",
      "\n",
      "### Task\n",
      "Given a multi-hop question and a set of facts that have already been collected, you must generate a search query to find the next piece of information.\n",
      "\n",
      "### Inputs\n",
      "1.  `question`: The original multi-hop question you are trying to answer.\n",
      "2.  `collected_info`: A list of facts and statements retrieved from previous search steps.\n",
      "\n",
      "### Outputs\n",
      "You must generate the following three outputs:\n",
      "1.  `reasoning`: A clear, step-by-step explanation of your thought process. This should:\n",
      "    *   Summarize the information you already have from `collected_info`.\n",
      "    *   Identify the next logical sub-question or the specific entity/fact that needs to be resolved.\n",
      "    *   Explain how your proposed search query will find this missing piece of information.\n",
      "2.  `search_query`: The search query string.\n",
      "3.  `top_n`: The number of top search results to retrieve for the query.\n",
      "\n",
      "### Instructions for Generating the Search Query\n",
      "\n",
      "1.  **Be Specific and Precise:** Your queries must be highly specific.\n",
      "    *   Use the full names of entities (people, places, organizations) from the `question` and `collected_info`.\n",
      "    *   Avoid broad, generic queries. For example, instead of \"mosaic in the church of the city\", use \"famous mosaics in churches in Thessaloniki\" if \"Thessaloniki\" is the known city. This improves precision and reduces irrelevant results.\n",
      "\n",
      "2.  **Focus on the Next Unresolved Step:**\n",
      "    *   Decompose the main question into a chain of smaller, answerable questions.\n",
      "    *   Your query should target only the *next* unknown piece of information. Do not try to answer the entire question in one go.\n",
      "\n",
      "3.  **Balance Specificity and Comprehensiveness:**\n",
      "    *   While your query needs to be specific to ensure high precision, it must also be comprehensive enough to achieve high recall (i.e., find all relevant supporting documents).\n",
      "    *   If a term could be ambiguous (e.g., \"regions\"), your reasoning should clarify your interpretation (e.g., \"interpreting regions as sovereign states\"), and the query should reflect that, like \"number of sovereign states in Asia\".\n",
      "\n",
      "4.  **Strategize for Efficiency:**\n",
      "    *   Plan your queries to answer the question in the minimum number of steps (hops).\n",
      "    *   Each query should build directly upon the `collected_info` to move closer to the final answer.\n",
      "\n",
      "5.  **Set `top_n` Appropriately:**\n",
      "    *   Since you are looking for a specific fact, keep the `top_n` value low, typically between 1 and 3. This focuses the search on the most relevant documents.\n",
      "2025/10/02 13:17:26 INFO dspy.evaluate.evaluate: Average Metric: 2.4120928620928623 / 3 (80.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_2 at: http://localhost:5005/#/experiments/1/runs/8601008d44dd4568b59e3092d5d89ae2\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:17:27 INFO dspy.teleprompt.gepa.gepa: Iteration 1: New subsample score is not better, skipping\n",
      "GEPA Optimization:   2%|█▏                                                          | 36/1765 [03:11<3:11:47,  6.66s/rollouts]2025/10/02 13:17:27 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Selected program 0 score: 0.6368015048015047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.02 / 3 (67.4%): 100%|█████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:17:29 INFO dspy.evaluate.evaluate: Average Metric: 2.0206306306306305 / 3 (67.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_3 at: http://localhost:5005/#/experiments/1/runs/12df9e7cd52b4d8aaca4d5f75dba68bc\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:17:52 INFO dspy.teleprompt.gepa.gepa: Iteration 2: Proposed new text for extract_info.predict: Your task is to act as a highly focused information extractor. Given a question and a set of retrieved documents, you must identify and extract key pieces of information that are essential for answering the question.\n",
      "\n",
      "### Core Instructions:\n",
      "1.  **Analyze the Question:** First, carefully deconstruct the question to understand its core components and the specific information required (e.g., a person, a location, a date, a relationship between entities).\n",
      "2.  **Scan for Relevance:** Read through the provided documents, specifically looking for text that directly addresses the components of the question.\n",
      "3.  **Extract Key Information:**\n",
      "    *   Focus on extracting concrete facts, entities (people, places, organizations), relationships, and dates.\n",
      "    *   The extracted information does not need to be the final answer. It can be an intermediate fact that is a crucial step in the reasoning process. For example, if the question is \"What is the capital of the country where person X was born?\", and a document states \"Person X was born in France,\" you should extract this fact, as it is a necessary step to find the final answer.\n",
      "    *   If a document mentions an entity from the question (e.g., a president's name) and provides a related piece of information (e.g., a city they were associated with), extract that information even if it doesn't fully answer the question. These are valuable clues.\n",
      "4.  **Strict Grounding:** You must **only** extract information that is explicitly stated in the provided documents. Do not infer, guess, or use any external knowledge. If the documents do not contain any relevant information to answer the question, you must return an empty list.\n",
      "5.  **Provide Reasoning:** Before presenting the extracted information, provide a brief, step-by-step reasoning process. Explain how you interpreted the question, what you looked for in the documents, and why you selected the specific pieces of information (or why you found none).\n",
      "\n",
      "### Output Format:\n",
      "Your output must consist of two parts:\n",
      "1.  `reasoning`: A string containing your thought process.\n",
      "2.  `key_informations`: A list of `KeyInformation` objects. Each object must contain:\n",
      "    *   `info`: A string with the extracted factual statement.\n",
      "    *   `source_doc_id`: The string ID of the document from which the information was extracted.\n",
      "\n",
      "If no relevant information is found, the `key_informations` list should be empty (`[]`).\n",
      "2025/10/02 13:18:44 INFO dspy.evaluate.evaluate: Average Metric: 0.9009549549549549 / 3 (30.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_4 at: http://localhost:5005/#/experiments/1/runs/c468e7ea5733428eadd158e64a2b9bd1\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:18:44 INFO dspy.teleprompt.gepa.gepa: Iteration 2: New subsample score is not better, skipping\n",
      "GEPA Optimization:   2%|█▍                                                          | 42/1765 [04:29<3:54:35,  8.17s/rollouts]2025/10/02 13:18:44 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Selected program 0 score: 0.6368015048015047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.26 / 3 (75.4%): 100%|█████████████████████████████████████████████████████████| 3/3 [00:21<00:00,  7.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:19:07 INFO dspy.evaluate.evaluate: Average Metric: 2.2623063063063062 / 3 (75.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_5 at: http://localhost:5005/#/experiments/1/runs/6adbfd52160444d8b2be7367e0f6a37d\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:19:33 INFO dspy.teleprompt.gepa.gepa: Iteration 3: Proposed new text for decide_info_collect.predict: Your task is to act as a verifier. Given a `question` and a collection of text snippets in `all_information`, you must determine if the provided information is sufficient to construct a complete and accurate answer. Your output should be a single boolean field: `has_collected_enough_info`.\n",
      "\n",
      "To do this, follow these steps:\n",
      "\n",
      "1.  **Deconstruct the Question:** Break down the `question` into its core components. Identify all the entities, the relationships between them, and any specific constraints such as dates, locations, or other conditions.\n",
      "\n",
      "2.  **Verify Each Component:** Systematically check if every essential component and constraint from the question can be addressed by the text in `all_information`. You must be able to form a complete logical chain that connects all the parts of the question to a final answer.\n",
      "\n",
      "3.  **Assess Sufficiency:**\n",
      "    *   **Set `has_collected_enough_info` to `True` if:** You can build a complete and unbroken chain of reasoning from the question to the answer using only the provided `all_information`.\n",
      "    *   **Set `has_collected_enough_info` to `False` if:** There is a gap in the logical chain. For example, if a critical piece of information is missing, or if the information provided does not match a key constraint in the question (e.g., the question asks about the year 1306, but the information is about the modern era).\n",
      "\n",
      "**Important Heuristics:**\n",
      "\n",
      "*   **Handling Complex Descriptions:** Questions often use multiple descriptive clauses to identify a single entity (e.g., \"the country that is the largest X and is also a member of Y\"). You do **not** need to find evidence for every single descriptive clause. If the `all_information` allows you to uniquely identify the core entity using at least one of the key descriptors, and you can then use that identification to answer the main part of the question, the information should be considered sufficient.\n",
      "*   **Focus Solely on the Provided Information:** Your decision must be based *only* on the `question` and the `all_information` provided in the input. Do not use any external knowledge. If the information is factually wrong but self-consistent for answering the question, you should still consider it sufficient for the purpose of this task.\n",
      "2025/10/02 13:19:54 INFO dspy.evaluate.evaluate: Average Metric: 2.2623063063063062 / 3 (75.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_6 at: http://localhost:5005/#/experiments/1/runs/3fa89e2b4d8c41808fda502a3e40653f\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:19:55 INFO dspy.teleprompt.gepa.gepa: Iteration 3: New subsample score is not better, skipping\n",
      "GEPA Optimization:   3%|█▋                                                          | 48/1765 [05:39<4:19:34,  9.07s/rollouts]2025/10/02 13:19:55 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Selected program 0 score: 0.6368015048015047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.27 / 3 (75.8%): 100%|█████████████████████████████████████████████████████████| 3/3 [00:49<00:00, 16.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:20:45 INFO dspy.evaluate.evaluate: Average Metric: 2.274954954954955 / 3 (75.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_7 at: http://localhost:5005/#/experiments/1/runs/d271c1faf7a04484a453db3866fd5bcc\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:21:06 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Proposed new text for generate_answer.predict: You are an expert at answering multi-hop questions based on a provided set of documents. Your task is to synthesize information from multiple sources to construct a concise and accurate answer.\n",
      "\n",
      "Follow these rules carefully:\n",
      "\n",
      "1.  **Analyze the Question:** First, break down the multi-hop question to understand the chain of information you need to find. Identify the entities and the relationships between them that you must establish.\n",
      "\n",
      "2.  **Synthesize Information:** Your answer will require connecting facts from different documents. Trace the logical path from one piece of information to the next using the provided documents. You must base your answer *exclusively* on the information given in the `all_information` section. Do not use any external knowledge.\n",
      "\n",
      "3.  **Provide a Concise Answer:** The `answer` should be direct and to the point, containing only the specific information requested (e.g., a name, a number, a location). Use the exact entities, dates, and facts as they appear in the source documents.\n",
      "\n",
      "4.  **Cite All Supporting Documents:** In the `citations` section, you must include the ID of *every* document that was necessary to form the chain of reasoning to get to the final answer. If you used a fact from a document to make a connection, it must be cited.\n",
      "\n",
      "5.  **Handle Insufficient Information:** If the provided documents do not contain the necessary information to connect all the logical steps and answer the question, you must state: \"The question cannot be answered with the provided information.\"\n",
      "\n",
      "6.  **Structure Your Output:** Your response must be structured into three parts:\n",
      "    *   **reasoning:** A brief, step-by-step explanation of how you connected the information from the different source documents to arrive at your answer. Mention the document IDs in your reasoning.\n",
      "    *   **answer:** The final, concise answer.\n",
      "    *   **citations:** A list of the document IDs that support your reasoning and final answer.\n",
      "2025/10/02 13:21:46 INFO dspy.evaluate.evaluate: Average Metric: 2.3380180180180177 / 3 (77.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_8 at: http://localhost:5005/#/experiments/1/runs/73fa1cebaa6740ee86863cf1ed906a8e\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:23:28 INFO dspy.evaluate.evaluate: Average Metric: 19.86307235982911 / 30 (66.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_9 at: http://localhost:5005/#/experiments/1/runs/061bfef200684c0789238719dfc4b45a\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program is on the linear pareto front\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset score for new program: 0.6621024119943039\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full train_val score for new program: 0.6621024119943039\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Individual valset scores for new program: [1.0, 0.3281081081081081, 0.9369369369369369, 0.9283783783783783, 0.8918918918918918, 0.8918918918918918, 0.8198198198198198, 0.3335135135135135, 0.6503287070854638, 0.45063063063063064, 0.5554054054054053, 0.4317117117117117, 0.545045045045045, 0.9549549549549549, 1.0, 0.2299099099099099, 0.8108108108108107, 0.7162162162162161, 0.42027027027027025, 0.4945945945945946, 0.5997297297297297, 0.6149058149058149, 0.8783783783783783, 0.6238738738738738, 0.6846846846846846, 0.4091891891891892, 0.5745045045045044, 0.7459459459459459, 1.0, 0.3414414414414414]\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New valset pareto front scores: [1.0, 0.3281081081081081, 0.9369369369369369, 0.9283783783783783, 0.8918918918918918, 0.8918918918918918, 0.8198198198198198, 0.6983783783783784, 0.6503287070854638, 0.45063063063063064, 0.5554054054054053, 0.4317117117117117, 0.545045045045045, 0.9549549549549549, 1.0, 0.2299099099099099, 0.8108108108108107, 0.7162162162162161, 0.42027027027027025, 0.4945945945945946, 0.5997297297297297, 0.8360360360360359, 0.8783783783783783, 0.6238738738738738, 0.6846846846846846, 0.4416216216216216, 0.5745045045045044, 0.7459459459459459, 1.0, 0.3414414414414414]\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Full valset pareto front score: 0.6827166626085545\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Updated valset pareto front programs: [{1}, {1}, {0, 1}, {1}, {1}, {0, 1}, {1}, {0}, {1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {0, 1}, {0}, {1}, {0, 1}, {1}, {0}, {1}, {1}, {1}, {0, 1}]\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best valset aggregate score so far: 0.6621024119943039\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on train_val: 1\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best program as per aggregate score on valset: 1\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on valset: 0.6621024119943039\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Best score on train_val: 0.6621024119943039\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: Linear pareto front program index: 1\n",
      "2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 4: New program candidate index: 1\n",
      "GEPA Optimization:   5%|██▊                                                         | 84/1765 [09:13<3:13:57,  6.92s/rollouts]2025/10/02 13:23:29 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Selected program 1 score: 0.6621024119943039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.56 / 3 (52.0%): 100%|█████████████████████████████████████████████████████████| 3/3 [02:04<00:00, 41.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:25:33 INFO dspy.evaluate.evaluate: Average Metric: 1.5605045045045045 / 3 (52.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_10 at: http://localhost:5005/#/experiments/1/runs/aeb102ec7a1d4c749a049adcf581cff7\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:26:05 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Proposed new text for generate_query.predict: You are an expert search strategist for a multi-hop question-answering system. Your task is to analyze a complex question and the information gathered so far, then generate the single best search query to find the next piece of information needed to answer the question.\n",
      "\n",
      "Your goal is to devise a search plan that answers the question in the fewest steps possible. This requires creating queries that are both highly precise (to avoid irrelevant results) and efficient (to resolve the next step completely).\n",
      "\n",
      "Follow these steps to generate your response:\n",
      "\n",
      "1.  **Analyze the Current State:**\n",
      "    *   **Deconstruct the `question`:** Identify all the entities (people, places, organizations), relationships, and constraints within the question.\n",
      "    *   **Synthesize `collected_info`:** Review the facts that have already been found. Understand what parts of the original question have been resolved and what entities have been identified or disambiguated.\n",
      "\n",
      "2.  **Identify the Information Gap:**\n",
      "    *   Compare the information required by the `question` with the `collected_info`.\n",
      "    *   Pinpoint the most critical, unresolved part of the question. This is your target for the next search. For example, if you have identified a person but need to know their role in a specific country, that role is your information gap.\n",
      "\n",
      "3.  **Formulate a Strategic Search Query:**\n",
      "    *   **Be Specific and Precise:** Use the full, official names of entities (e.g., \"Democratic Republic of Congo\" instead of \"Congo\") that have been confirmed in the `collected_info`. This is crucial for avoiding ambiguity and retrieving accurate documents.\n",
      "    *   **Focus on the Gap:** Your query should be laser-focused on finding the missing piece of information you identified in the previous step. Do not re-search for information you already have.\n",
      "    *   **Combine Entities:** To increase precision, construct your query by combining the key entities you have already resolved. For example, if you know the person is \"Willem-Alexander\" and the country is \"the Netherlands\", include both in your query to find his successor.\n",
      "\n",
      "4.  **Articulate Your Reasoning:**\n",
      "    *   In the `reasoning` field, clearly explain your thought process.\n",
      "    *   Begin by summarizing what is known from the `collected_info` and how it relates to the `question`.\n",
      "    *   State exactly what piece of information is still missing.\n",
      "    *   Explain how your proposed `search_query` is designed to find this specific missing information.\n",
      "\n",
      "5.  **Set `top_n`:**\n",
      "    *   Choose a small integer (typically 1-3) for `top_n`. This should be the number of search results you believe are necessary to find the answer for the current step.\n",
      "2025/10/02 13:27:54 INFO dspy.evaluate.evaluate: Average Metric: 1.7623063063063062 / 3 (58.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_11 at: http://localhost:5005/#/experiments/1/runs/b524379a91004a648aaca031a2af21bb\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:33:41 INFO dspy.evaluate.evaluate: Average Metric: 19.837278745976413 / 30 (66.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_12 at: http://localhost:5005/#/experiments/1/runs/90253f6f4eeb4b9d96951c92e623cdf1\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset score for new program: 0.6612426248658807\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full train_val score for new program: 0.6612426248658807\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Individual valset scores for new program: [1.0, 0.2882882882882883, 0.9369369369369369, 0.5657657657657658, 0.8918918918918918, 0.4614054054054054, 0.8648648648648649, 0.7792792792792792, 0.5612377959354703, 0.45063063063063064, 0.42702702702702705, 0.3281081081081081, 0.545045045045045, 0.9324324324324325, 1.0, 0.9078378378378378, 0.8108108108108107, 0.7162162162162161, 0.42027027027027025, 0.4945945945945946, 0.6335135135135135, 0.6149058149058149, 0.3662162162162162, 0.6238738738738738, 0.6846846846846846, 0.4317117117117117, 0.5745045045045044, 0.7459459459459459, 1.0, 0.7792792792792792]\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New valset pareto front scores: [1.0, 0.3281081081081081, 0.9369369369369369, 0.9283783783783783, 0.8918918918918918, 0.8918918918918918, 0.8648648648648649, 0.7792792792792792, 0.6503287070854638, 0.45063063063063064, 0.5554054054054053, 0.4317117117117117, 0.545045045045045, 0.9549549549549549, 1.0, 0.9078378378378378, 0.8108108108108107, 0.7162162162162161, 0.42027027027027025, 0.4945945945945946, 0.6335135135135135, 0.8360360360360359, 0.8783783783783783, 0.6238738738738738, 0.6846846846846846, 0.4416216216216216, 0.5745045045045044, 0.7459459459459459, 1.0, 0.7792792792792792]\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Full valset pareto front score: 0.725233179125071\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Updated valset pareto front programs: [{1, 2}, {1}, {0, 1, 2}, {1}, {1, 2}, {0, 1}, {2}, {2}, {1}, {0, 1, 2}, {1}, {1}, {1, 2}, {0, 1}, {1, 2}, {2}, {1, 2}, {1, 2}, {1, 2}, {0, 1, 2}, {2}, {0}, {1}, {0, 1, 2}, {1, 2}, {0}, {1, 2}, {1, 2}, {1, 2}, {2}]\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best valset aggregate score so far: 0.6621024119943039\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on train_val: 1\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best program as per aggregate score on valset: 1\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on valset: 0.6621024119943039\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Best score on train_val: 0.6621024119943039\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: Linear pareto front program index: 1\n",
      "2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 5: New program candidate index: 2\n",
      "GEPA Optimization:   7%|████                                                       | 120/1765 [19:26<5:27:12, 11.93s/rollouts]2025/10/02 13:33:42 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Selected program 2 score: 0.6612426248658807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.03 / 3 (67.7%): 100%|█████████████████████████████████████████████████████████| 3/3 [01:54<00:00, 38.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:35:37 INFO dspy.evaluate.evaluate: Average Metric: 2.031981981981982 / 3 (67.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_13 at: http://localhost:5005/#/experiments/1/runs/fd2733c504b24d0ea7ddcb292a60462b\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:36:06 INFO dspy.teleprompt.gepa.gepa: Iteration 6: Proposed new text for extract_info.predict: Your task is to act as a specialized information extractor. Given a question and a set of retrieved documents, you must meticulously extract all key pieces of information that are essential for answering the question or for taking the next step in a multi-step reasoning process.\n",
      "\n",
      "### Guiding Principles:\n",
      "\n",
      "1.  **Deconstruct the Question:** First, analyze the question to identify the core entities (e.g., people, places, organizations), relationships, and specific details (e.g., dates, quantities, locations) it is asking about. Break the question down into the individual facts needed to construct a complete answer.\n",
      "\n",
      "2.  **Extract Atomic and Relevant Facts:**\n",
      "    *   Scan the documents for any information that directly addresses the components of the deconstructed question.\n",
      "    *   Extract this information as concise, self-contained statements. For example, instead of a long paragraph, pull out the specific sentence or clause that contains the key fact.\n",
      "    *   Prioritize extracting entities, their attributes, their relationships with other entities, and specific factual data like dates, numbers, and official titles.\n",
      "\n",
      "3.  **Connect Information Across Documents:**\n",
      "    *   When multiple documents are provided, actively look for common entities that can link disparate pieces of information together.\n",
      "    *   For instance, if one document describes a documentary made by a specific band, and another document mentions a song performed by the same band, you should extract both pieces of information as they are linked by the common entity (the band).\n",
      "\n",
      "4.  **Handle Incomplete Information Strategically:**\n",
      "    *   Your primary goal is to gather evidence from the provided text. Even if the documents do not contain all the information needed to fully and definitively answer the question, you must extract what *is* available.\n",
      "    *   Do not state that the question is unanswerable. Instead, provide all the relevant clues and partial information found in the documents. This extracted information is crucial for subsequent reasoning or retrieval steps. For example, if the question asks for the land area of a capital city related to a person, and the documents provide the land area of a city but don't mention the person, you should still extract the city's land area.\n",
      "\n",
      "5.  **Be Precise and Grounded:**\n",
      "    *   Every piece of information you extract must be directly supported by the text in one of the provided documents.\n",
      "    *   Do not infer information that is not present in the text. Your reasoning should explain how the extracted facts connect, but the `key_informations` themselves must be verbatim or near-verbatim extractions.\n",
      "    *   For each extracted fact, you must accurately cite the `source_doc_id` from which it was taken.\n",
      "2025/10/02 13:38:14 INFO dspy.evaluate.evaluate: Average Metric: 2.0094594594594595 / 3 (67.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_14 at: http://localhost:5005/#/experiments/1/runs/3c8c475660f24ccba939f69ad192d503\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:38:15 INFO dspy.teleprompt.gepa.gepa: Iteration 6: New subsample score is not better, skipping\n",
      "GEPA Optimization:   7%|████▏                                                      | 126/1765 [23:59<7:02:31, 15.47s/rollouts]2025/10/02 13:38:15 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Selected program 2 score: 0.6612426248658807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.03 / 3 (67.7%): 100%|█████████████████████████████████████████████████████████| 3/3 [02:35<00:00, 51.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:40:50 INFO dspy.evaluate.evaluate: Average Metric: 2.0306306306306303 / 3 (67.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_15 at: http://localhost:5005/#/experiments/1/runs/4e3e8527ec694c0189b601de00e4c1de\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:41:18 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Proposed new text for decide_info_collect.predict: You are an expert at determining information sufficiency. Your task is to evaluate if a collection of text snippets (`all_information`) contains all the necessary facts to definitively answer a given `question`.\n",
      "\n",
      "Your goal is to produce a single boolean field: `has_collected_enough_info`.\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1.  **Deconstruct the Question:** First, carefully break down the `question` into a logical chain of required facts. Identify all the entities, their attributes, and the relationships between them that you need to establish to reach the final answer.\n",
      "\n",
      "2.  **Strictly Verify Each Fact:** For every single link in your logical chain, you must find explicit support within the provided `all_information`. **Do not use any external knowledge or make assumptions.** If a piece of information, even a seemingly obvious one, is not present in `all_information`, you must consider it missing.\n",
      "\n",
      "3.  **Assess for Completeness:** Once you have verified all the available facts, determine if you have a complete path from the question to the final answer.\n",
      "\n",
      "**Output Rules:**\n",
      "\n",
      "-   Set `has_collected_enough_info` to `True` **only if** every single piece of information required to construct the final, specific answer is present in `all_information`.\n",
      "\n",
      "-   Set `has_collected_enough_info` to `False` if **any** of the following conditions are met:\n",
      "    -   The final piece of data needed for the answer is missing. For example, you can identify the correct university but the `all_information` does not contain its undergraduate enrollment number (as in Example 1).\n",
      "    -   An intermediate, connecting fact is missing. For example, you know the founding date of San Francisco, but the `all_information` does not state that the \"RSA Security Conference\" is held there, so you cannot link the two facts (as in Example 2).\n",
      "    -   The information contains unresolved contradictions that make it impossible to determine the correct path of reasoning. For instance, if a person is stated to be born in two different cities and there's no other information to resolve the ambiguity.\n",
      "\n",
      "**Example Walkthroughs:**\n",
      "\n",
      "*   **Example 1:** The question requires the undergraduate enrollment of Harvard. The `all_information` successfully identifies the person (Tony Hsieh), his alma mater (Harvard), but critically lacks the final piece of data: Harvard's enrollment number. Therefore, `has_collected_enough_info` must be `False`.\n",
      "*   **Example 2:** The question requires the founding date of the city where the RSA conference is held. The `all_information` provides the founding date for San Francisco. However, it **does not** state that the RSA conference is held in San Francisco. Assuming this connection is a failure. Since a key link in the reasoning chain is missing from the provided context, `has_collected_enough_info` must be `False`.\n",
      "*   **Example 3:** The question requires a specific election year for North Carolina. The `all_information` successfully identifies the state (North Carolina) by linking it to its Civil War governor. However, it does not contain any information about election results. The final answer is missing, so `has_collected_enough_info` must be `False`.\n",
      "2025/10/02 13:44:52 INFO dspy.evaluate.evaluate: Average Metric: 2.4306306306306307 / 3 (81.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_16 at: http://localhost:5005/#/experiments/1/runs/c330ea55c524499f8c1e030d331e6112\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:52:04 INFO dspy.evaluate.evaluate: Average Metric: 19.20684139953907 / 30 (64.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_17 at: http://localhost:5005/#/experiments/1/runs/e4311d1087234065bec1e04729c741c3\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset score for new program: 0.6402280466513024\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full train_val score for new program: 0.6402280466513024\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Individual valset scores for new program: [1.0, 0.292972972972973, 0.9369369369369369, 0.41441441441441434, 0.6055495495495495, 0.4614054054054054, 0.8648648648648649, 0.35603603603603606, 0.5612377959354703, 0.45063063063063064, 0.42702702702702705, 0.3281081081081081, 0.545045045045045, 0.9324324324324325, 1.0, 0.9078378378378378, 0.8108108108108107, 0.7162162162162161, 0.42027027027027025, 0.4945945945945946, 0.6335135135135135, 0.8699999999999999, 0.8423423423423423, 0.6238738738738738, 0.6846846846846846, 0.4317117117117117, 0.5745045045045044, 0.7009009009009008, 1.0, 0.3189189189189189]\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New valset pareto front scores: [1.0, 0.3281081081081081, 0.9369369369369369, 0.9283783783783783, 0.8918918918918918, 0.8918918918918918, 0.8648648648648649, 0.7792792792792792, 0.6503287070854638, 0.45063063063063064, 0.5554054054054053, 0.4317117117117117, 0.545045045045045, 0.9549549549549549, 1.0, 0.9078378378378378, 0.8108108108108107, 0.7162162162162161, 0.42027027027027025, 0.4945945945945946, 0.6335135135135135, 0.8699999999999999, 0.8783783783783783, 0.6238738738738738, 0.6846846846846846, 0.4416216216216216, 0.5745045045045044, 0.7459459459459459, 1.0, 0.7792792792792792]\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Full valset pareto front score: 0.726365311257203\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Updated valset pareto front programs: [{1, 2, 3}, {1}, {0, 1, 2, 3}, {1}, {1, 2}, {0, 1}, {2, 3}, {2}, {1}, {0, 1, 2, 3}, {1}, {1}, {1, 2, 3}, {0, 1}, {1, 2, 3}, {2, 3}, {1, 2, 3}, {1, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {2, 3}, {3}, {1}, {0, 1, 2, 3}, {1, 2, 3}, {0}, {1, 2, 3}, {1, 2}, {1, 2, 3}, {2}]\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best valset aggregate score so far: 0.6621024119943039\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on train_val: 1\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best program as per aggregate score on valset: 1\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on valset: 0.6621024119943039\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Best score on train_val: 0.6621024119943039\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: Linear pareto front program index: 1\n",
      "2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 7: New program candidate index: 3\n",
      "GEPA Optimization:   9%|█████▍                                                     | 162/1765 [37:49<8:29:26, 19.07s/rollouts]2025/10/02 13:52:04 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Selected program 3 score: 0.6402280466513024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2.14 / 3 (71.4%): : 4it [04:27, 66.89s/it]                                                                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:56:32 INFO dspy.evaluate.evaluate: Average Metric: 2.142342342342342 / 3 (71.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_18 at: http://localhost:5005/#/experiments/1/runs/aa22fbd3ba14432c94b16e2e9140c229\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:57:07 INFO dspy.teleprompt.gepa.gepa: Iteration 8: Proposed new text for generate_answer.predict: You are an expert at answering multi-hop questions based on a provided set of documents. Your task is to synthesize information from multiple sources to construct a concise and accurate answer.\n",
      "\n",
      "Follow these rules carefully:\n",
      "\n",
      "1.  **Analyze the Question:** First, break down the multi-hop question to understand the chain of information you need to find. Identify the starting entity, the target information, and the intermediate or \"bridge\" entities required to connect them.\n",
      "\n",
      "2.  **Synthesize Information:** Your answer will require connecting facts from different documents. Trace the logical path from one piece of information to the next.\n",
      "    *   **Strict Grounding:** Your reasoning must be based *exclusively* on the provided documents. Do not make assumptions or logical leaps. For example, if a document states a person was \"associated with\" a city, you cannot conclude they were \"born in\" that city unless another document explicitly says so.\n",
      "    *   **Find the Most Specific Answer:** Search all documents for the most precise and specific information that answers the question. If one document provides a general answer (e.g., a continent like \"Europe\") and another provides a more specific one (e.g., a country like \"Italy\"), your final answer must be the more specific one.\n",
      "    *   **Exhaustive Search:** Before concluding that an answer is not possible, exhaustively search for connections between entities across all documents. An entity in one document might be referred to by a description or a synonym in another.\n",
      "\n",
      "3.  **Provide a Concise Answer:** The `answer` must be direct, to the point, and use the *exact phrasing* (names, numbers, locations, etc.) found in the source documents.\n",
      "\n",
      "4.  **Cite All Supporting Documents:** In the `citations` section, you must list the ID of *every single document* that contributes a piece of information to your reasoning chain. If you used a fact from a document to make a connection, it must be cited.\n",
      "\n",
      "5.  **Handle Insufficient Information:** Only state \"The question cannot be answered with the provided information\" as a last resort. This should only be used if a critical link in the logical chain is definitively missing from all documents and no connection can be made without violating the strict grounding rule.\n",
      "\n",
      "6.  **Structure Your Output:** Your response must be structured into three parts:\n",
      "    *   **reasoning:** A brief, step-by-step explanation of how you connected the information from the different source documents to arrive at your answer. Mention the document IDs in your reasoning.\n",
      "    *   **answer:** The final, concise answer.\n",
      "    *   **citations:** A list of the document IDs that support your reasoning and final answer.\n",
      "2025/10/02 13:58:12 INFO dspy.evaluate.evaluate: Average Metric: 2.0364864864864867 / 3 (67.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_19 at: http://localhost:5005/#/experiments/1/runs/213d5bb90d1c4622a33c716ccf2d6297\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:58:13 INFO dspy.teleprompt.gepa.gepa: Iteration 8: New subsample score is not better, skipping\n",
      "GEPA Optimization:  10%|█████▌                                                    | 168/1765 [43:57<10:21:59, 23.37s/rollouts]2025/10/02 13:58:13 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Selected program 0 score: 0.6368015048015047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1.98 / 3 (66.2%): 100%|█████████████████████████████████████████████████████████| 3/3 [01:07<00:00, 22.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:59:21 INFO dspy.evaluate.evaluate: Average Metric: 1.9849789789789791 / 3 (66.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏃 View run eval_20 at: http://localhost:5005/#/experiments/1/runs/9227af300e084085aa732bba0774c3ac\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 13:59:52 INFO dspy.teleprompt.gepa.gepa: Iteration 9: Proposed new text for generate_query.predict: You are an expert reasoning agent designed to solve complex, multi-hop questions. Your task is to generate the next search query needed to find a missing piece of information. You will be given the original question and a collection of information found so far.\n",
      "\n",
      "Your process should be as follows:\n",
      "\n",
      "1.  **Decompose the Question:** First, analyze the main question and break it down into a logical chain of smaller, dependent sub-questions or entities that need to be resolved.\n",
      "2.  **Synthesize Known Information:** Review the `collected_info`. Identify which parts of your decomposed question have already been answered by this information.\n",
      "3.  **Identify the Next Unresolved Step:** Pinpoint the immediate next piece of information required to move forward in the chain. Do not skip steps or make assumptions about unresolved entities. If a location, person, or date in the question is still an unknown variable, your next query must be to identify it.\n",
      "4.  **Formulate Your Reasoning:** In the `reasoning` field, clearly explain your thought process.\n",
      "    *   Start by stating what you know based on the `collected_info`.\n",
      "    *   Then, explicitly state what the next missing piece of information is.\n",
      "    *   This reasoning must logically lead to and justify your search query.\n",
      "5.  **Construct a Precise Search Query:** In the `search_query` field, create a query that is specific and targeted to find only the missing information identified in your reasoning.\n",
      "    *   **Incorporate Resolved Entities:** Use the specific names, locations, and dates from the `collected_info` to make your query precise (e.g., use \"Macomb County, Michigan\" not just \"Macomb County\").\n",
      "    *   **Use Relationship Keywords:** Use clear terms to define the relationship you are searching for (e.g., \"counties adjacent to\", \"successor of\", \"date of invasion\").\n",
      "\n",
      "**Key Principles to Follow:**\n",
      "\n",
      "*   **Be Strictly Sequential:** Do not jump ahead. For a question like \"When did the country where Ladakh found his guidance in religion become a part of Qing China?\", if you do not yet know \"the country where Ladakh found his guidance\", your first step is to find that country (e.g., \"Ladakh religion guidance\"), not to ask when Ladakh joined Qing China.\n",
      "*   **Resolve Ambiguity First:** Your primary goal is to resolve unknown entities (people, places, dates) one by one. Once an entity is resolved, use it in subsequent queries to find information about it.\n",
      "*   **Focus and Specificity:** Your queries should be laser-focused. For finding neighboring geographical areas, use terms like \"adjacent to\" or \"neighboring\". For finding a historical event date, specify the event, the location, and the actors involved if known (e.g., \"Muslim armies invade Syria date\").\n",
      "2025/10/02 14:02:39 INFO dspy.evaluate.evaluate: Average Metric: 2.0271692745376955 / 3 (67.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run eval_21 at: http://localhost:5005/#/experiments/1/runs/64bbe47fc5b340c3bc042226e331236c\n",
      "🧪 View experiment at: http://localhost:5005/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# Run GEPA optimization\n",
    "print(\"🚀 Starting GEPA optimization...\")\n",
    "\n",
    "optimized_program = optimizer.compile(\n",
    "    program,\n",
    "    trainset=train_ds,\n",
    "    valset=val_ds,\n",
    ")\n",
    "\n",
    "print(\"✅ GEPA optimization completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f130b47-19f7-496c-9c5c-618256ae63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_program.save(str(EXP_DIR / \"optimized-program\"), save_program=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18cfef",
   "metadata": {},
   "source": [
    "### Examine Optimized Prompts\n",
    "\n",
    "Let's look at how GEPA improved the prompts for each predictor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f024bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred in optimized_program.named_predictors():\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Predictor: {name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Optimized Instructions:\")\n",
    "    print(pred.signature.instructions)\n",
    "    print(\"*\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124ea24",
   "metadata": {},
   "source": [
    "### Evaluate Optimized Program\n",
    "\n",
    "Compare the performance before and after GEPA optimization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ccf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n📊 Evaluating OPTIMIZED program...\")\n",
    "# Evaluate optimized program  \n",
    "optimized_evaluate = dspy.Evaluate(\n",
    "    devset=test_ds,\n",
    "    metric=metric,\n",
    "    num_threads=8,\n",
    "    display_table=False,\n",
    "    display_progress=True\n",
    ")\n",
    "optimized_eval_result = optimized_evaluate(optimized_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229aecb-88f7-4c2c-b862-f05060d1be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\n\" + \"=\" * 50)\n",
    "print(\"🏆 PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original Program Score:  {original_eval_result.score:.3f}\")\n",
    "print(f\"Optimized Program Score: {optimized_eval_result.score:.3f}\")\n",
    "print(f\"Improvement:            {optimized_eval_result.score - original_eval_result.score:+.3f}\")\n",
    "print(f\"Relative Improvement:   {((optimized_eval_result.score / original_eval_result.score) - 1) * 100:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf4598",
   "metadata": {},
   "source": [
    "### GEPA Optimization Analysis\n",
    "\n",
    "Analyze the detailed optimization results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba80a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze GEPA optimization trajectory\n",
    "if hasattr(optimized_program, 'detailed_results'):\n",
    "    results = optimized_program.detailed_results\n",
    "    \n",
    "    print(\"🔍 GEPA Optimization Details:\")\n",
    "    print(f\"- Total candidates explored: {len(results.candidates)}\")\n",
    "    print(f\"- Best candidate index: {results.best_idx}\")\n",
    "    print(f\"- Best validation score: {results.val_aggregate_scores[results.best_idx]:.3f}\")\n",
    "    print(f\"- Discovery evaluations used: {sum(results.discovery_eval_counts)}\")\n",
    "    \n",
    "    # Show score progression\n",
    "    print(\"\\\\n📈 Score progression:\")\n",
    "    for i, score in enumerate(results.val_aggregate_scores[:10]):  # Show first 10\n",
    "        print(f\"Candidate {i}: {score:.3f}\")\n",
    "    \n",
    "    if len(results.val_aggregate_scores) > 10:\n",
    "        print(f\"... and {len(results.val_aggregate_scores) - 10} more candidates\")\n",
    "else:\n",
    "    print(\"Detailed results not available (set track_stats=True in GEPA constructor)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2255f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test optimized program on the same example\n",
    "example = test_ds[3]\n",
    "\n",
    "print(\"🧪 Testing optimized program on example:\")\n",
    "print(f\"Question: {example.question}\")\n",
    "print(f\"Expected Answer: {example.answer}\")\n",
    "print(f\"Supporting Docs: {example.supporting_ids}\")\n",
    "print()\n",
    "\n",
    "pred = program(example.question, example.docs)\n",
    "optimized_pred = optimized_program(example.question, example.docs)\n",
    "\n",
    "print(\"📋 ORIGINAL vs OPTIMIZED Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"ORIGINAL:\")\n",
    "print(f\"  Answer: {pred.answer}\")\n",
    "print(f\"  Retrieved docs: {pred.retrieved_doc_ids}\")\n",
    "print(f\"  Cited docs: {pred.citations}\")\n",
    "\n",
    "print(\"OPTIMIZED:\")\n",
    "print(f\"  Answer: {optimized_pred.answer}\")\n",
    "print(f\"  Retrieved docs: {optimized_pred.retrieved_doc_ids}\")\n",
    "print(f\"  Cited docs: {optimized_pred.citations}\")\n",
    "\n",
    "print(\"🎯 Metric Comparison:\")\n",
    "original_metric_result = metric_with_feedback(example, pred)\n",
    "optimized_metric_result = metric_with_feedback(example, optimized_pred)\n",
    "print(f\"Original score: {original_metric_result.score:.3f}\")\n",
    "print(f\"Original feedback: {original_metric_result.feedback}\")\n",
    "print()\n",
    "print(f\"Optimized score: {optimized_metric_result.score:.3f}\")\n",
    "print(f\"Optimized feedback: {optimized_metric_result.feedback}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff3a39-8ff5-4be5-bbf6-0a57f14ef517",
   "metadata": {},
   "source": [
    "Can we measure instruction quality by using them with a larger model to see if it gets questions right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c762b-5842-4396-af06-a99bee74577e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
